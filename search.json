[
  {
    "objectID": "slides/02-binomial-glm.html#example-shimizu2024-xl",
    "href": "slides/02-binomial-glm.html#example-shimizu2024-xl",
    "title": "Binomial GLM example",
    "section": "Example: Shimizu et al. (2024)",
    "text": "Example: Shimizu et al. (2024)\n\nShimizu et al. (2024) investigated the processing of emotional faces.\n\n6 basic emotions: anger, disgust, fear, happiness, sadness and surprise\nintensity in % (from 10% to 100% in steps of 10%)\n71 participants\n377 faces (males and females of different identities)\nforced-choice procedure with 7 options (6 emotions + neutral). Chance level at \\(1/7 = 0.14\\)."
  },
  {
    "objectID": "slides/02-binomial-glm.html#shimizu2024-xl-dataset",
    "href": "slides/02-binomial-glm.html#shimizu2024-xl-dataset",
    "title": "Binomial GLM example",
    "section": "Shimizu et al. (2024) dataset",
    "text": "Shimizu et al. (2024) dataset\nWe did some pre-processing for the purpose of this example. The original dataset can be found at  osf.io/zhtbj/.\nYou can download the dataset for this example at this link. It is a rds file, and you can open it using:\n\ndat &lt;- readRDS(\"shimizu.rds\")\n\nThen we can load some packages:\n\nlibrary(tidyverse) # for data manipulation\nlibrary(ggplot2)   # plotting"
  },
  {
    "objectID": "slides/02-binomial-glm.html#exploring",
    "href": "slides/02-binomial-glm.html#exploring",
    "title": "Binomial GLM example",
    "section": "Exploring",
    "text": "Exploring\nFor the purpose if this workshop, we will focus on a single subject (otherwise we should use a mixed-effects model). We also select only the relevant columns.\n\ndat &lt;- subset(dat, id == 22)\ndat &lt;- dat[, c(\"id\", \"age\", \"intensity\", \"emotion_lbl\", \"response_lbl\", \"acc\")]\ndat\n\n# A tibble: 377 × 6\n      id age   intensity emotion_lbl response_lbl   acc\n   &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;        &lt;int&gt;\n 1    22 53           60 fear        suprise          0\n 2    22 53           60 disgust     disgust          1\n 3    22 53           70 happiness   happiness        1\n 4    22 53          100 happiness   happiness        1\n 5    22 53           60 disgust     sadness          0\n 6    22 53           20 fear        neutral          0\n 7    22 53           10 fear        neutral          0\n 8    22 53          100 sadness     sadness          1\n 9    22 53           90 disgust     disgust          1\n10    22 53           40 happiness   happiness        1\n# ℹ 367 more rows"
  },
  {
    "objectID": "slides/02-binomial-glm.html#exploring-1",
    "href": "slides/02-binomial-glm.html#exploring-1",
    "title": "Binomial GLM example",
    "section": "Exploring",
    "text": "Exploring\n\nWe have 377 trials and 6 columns.\nThe intensity is the intensity (from 10% to 100%) of the facial expression. emotion_lbl is the emotion and response_lbl is the response.\nWhen emotion_lbl = response_lbl the acc = 1 namely a correct response."
  },
  {
    "objectID": "slides/02-binomial-glm.html#exploring-2",
    "href": "slides/02-binomial-glm.html#exploring-2",
    "title": "Binomial GLM example",
    "section": "Exploring",
    "text": "Exploring\nWe can calculate the average accuracy for each emotion. Clearly there is a big difference with fear being the hardest one and surprise the easiest. We remove neutral because we have no associated intensity\n\ndat |&gt; \n  group_by(emotion_lbl) |&gt; \n  summarise(p = mean(acc),\n            n = n()) |&gt; \n  arrange(desc(p))\n\n# A tibble: 7 × 3\n  emotion_lbl     p     n\n  &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;\n1 neutral     1         7\n2 surprise    0.8      70\n3 happiness   0.686    70\n4 sadness     0.517    60\n5 disgust     0.35    100\n6 anger       0.333    30\n7 fear        0.05     40\n\ndat &lt;- filter(dat, emotion_lbl != \"neutral\")"
  },
  {
    "objectID": "slides/02-binomial-glm.html#exploring-3",
    "href": "slides/02-binomial-glm.html#exploring-3",
    "title": "Binomial GLM example",
    "section": "Exploring",
    "text": "Exploring\nAlso for intensity, there is a clear pattern. An increasing intensity is associated with increased accuracy.\n\ndat |&gt; \n  group_by(intensity) |&gt; \n  summarise(p = mean(acc),\n            n = n()) |&gt; \n  arrange(desc(p))\n\n# A tibble: 10 × 3\n   intensity      p     n\n       &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n 1        70 0.703     37\n 2        90 0.676     37\n 3       100 0.676     37\n 4        80 0.649     37\n 5        50 0.622     37\n 6        60 0.622     37\n 7        40 0.486     37\n 8        30 0.324     37\n 9        10 0.0811    37\n10        20 0.0811    37"
  },
  {
    "objectID": "slides/02-binomial-glm.html#exploring-plots",
    "href": "slides/02-binomial-glm.html#exploring-plots",
    "title": "Binomial GLM example",
    "section": "Exploring, plots",
    "text": "Exploring, plots\n\ndat |&gt; \n  group_by(emotion_lbl) |&gt; \n  summarise(p = mean(acc),\n            n = n()) |&gt; \n  ggplot(aes(x = fct_reorder(emotion_lbl, p), y = p)) + \n  geom_point(size = 4) +\n  geom_line(group = 1) +\n  ylim(c(0, 1)) +\n  xlab(\"Emotion\") +\n  ylab(\"Accuracy\") +\n  geom_hline(yintercept = 1/7, lty = \"dotted\")"
  },
  {
    "objectID": "slides/02-binomial-glm.html#exploring-plots-output",
    "href": "slides/02-binomial-glm.html#exploring-plots-output",
    "title": "Binomial GLM example",
    "section": "Exploring, plots",
    "text": "Exploring, plots"
  },
  {
    "objectID": "slides/02-binomial-glm.html#exploring-plots-1",
    "href": "slides/02-binomial-glm.html#exploring-plots-1",
    "title": "Binomial GLM example",
    "section": "Exploring, plots",
    "text": "Exploring, plots\n\ndat |&gt; \n  group_by(intensity) |&gt; \n  summarise(p = mean(acc),\n            n = n()) |&gt; \n  arrange(desc(p)) |&gt; \n  ggplot(aes(x = intensity, y = p)) + \n  geom_point(size = 4) +\n  geom_line() +\n  ylim(c(0, 1)) +\n  xlab(\"Intensity (%)\") +\n  ylab(\"Accuracy\") +\n  geom_hline(yintercept = 1/7, lty = \"dotted\")"
  },
  {
    "objectID": "slides/02-binomial-glm.html#exploring-plots-1-output",
    "href": "slides/02-binomial-glm.html#exploring-plots-1-output",
    "title": "Binomial GLM example",
    "section": "Exploring, plots",
    "text": "Exploring, plots"
  },
  {
    "objectID": "slides/02-binomial-glm.html#exploring-plots-2",
    "href": "slides/02-binomial-glm.html#exploring-plots-2",
    "title": "Binomial GLM example",
    "section": "Exploring, plots",
    "text": "Exploring, plots\nWe have few trials but we can also explore the interaction between emotion and intensity. There are some emotions where the rate of increase in accuracy as a function of the emotion is faster compared to others.\n\ndat |&gt; \n  group_by(intensity, emotion_lbl) |&gt; \n  summarise(p = mean(acc)) |&gt; \n  ggplot(aes(x = intensity, y = p, color = emotion_lbl)) +\n  geom_point(size = 4) +\n  geom_line()"
  },
  {
    "objectID": "slides/02-binomial-glm.html#exploring-plots-2-output",
    "href": "slides/02-binomial-glm.html#exploring-plots-2-output",
    "title": "Binomial GLM example",
    "section": "Exploring, plots",
    "text": "Exploring, plots"
  },
  {
    "objectID": "slides/02-binomial-glm.html#exploring-odds-and-odds-ratios",
    "href": "slides/02-binomial-glm.html#exploring-odds-and-odds-ratios",
    "title": "Binomial GLM example",
    "section": "Exploring, odds and odds ratios",
    "text": "Exploring, odds and odds ratios\nWe can start exploring the effects calculating odds and odds ratios.\n\nodds &lt;- function(p) p / (1 - p)\nor &lt;- function(pn, pd) odds(pn) / odds(pd)\n\n\n(p_anger &lt;- mean(dat$acc[dat$emotion_lbl == \"anger\"]))\n## [1] 0.3333333\n(p_surprise &lt;- mean(dat$acc[dat$emotion_lbl == \"surprise\"]))\n## [1] 0.8\n\nodds(p_anger)\n## [1] 0.5\nodds(p_surprise)\n## [1] 4\n\nor(p_surprise, p_anger)\n## [1] 8"
  },
  {
    "objectID": "slides/02-binomial-glm.html#exploring-odds-and-odds-ratios-1",
    "href": "slides/02-binomial-glm.html#exploring-odds-and-odds-ratios-1",
    "title": "Binomial GLM example",
    "section": "Exploring, odds and odds ratios",
    "text": "Exploring, odds and odds ratios\nWe can also create a contingency table:\n\ntable(dat$acc, dat$emotion_lbl)\n\n   \n    anger disgust fear happiness sadness surprise\n  0    20      65   38        22      29       14\n  1    10      35    2        48      31       56\n\n(all_p &lt;- tapply(dat$acc, dat$emotion_lbl, FUN = mean))\n\n    anger   disgust      fear happiness   sadness  surprise \n0.3333333 0.3500000 0.0500000 0.6857143 0.5166667 0.8000000 \n\nodds(all_p)\n\n     anger    disgust       fear  happiness    sadness   surprise \n0.50000000 0.53846154 0.05263158 2.18181818 1.06896552 4.00000000 \n\n\nWhen the odds are lower than 1, the probability of success is lower than the probability of failure. When the odds are greater than 1 the probability of success is higher."
  },
  {
    "objectID": "slides/02-binomial-glm.html#exploring-odds-and-odds-ratios-2",
    "href": "slides/02-binomial-glm.html#exploring-odds-and-odds-ratios-2",
    "title": "Binomial GLM example",
    "section": "Exploring, odds and odds ratios",
    "text": "Exploring, odds and odds ratios\nWe can also calculate all the possible comparisons. Note that depending on the numerator/denominator the odds ratio is different, but we can simply take the inverse to switch numerator and numerator. Interpreting odds ratio &gt; 1 is usually more intuitive.\n\n\n     anger / disgust         anger / fear    anger / happiness \n               0.929                9.500                0.229 \n     anger / sadness     anger / surprise       disgust / fear \n               0.468                0.125               10.231 \n disgust / happiness    disgust / sadness   disgust / surprise \n               0.247                0.504                0.135 \n    fear / happiness       fear / sadness      fear / surprise \n               0.024                0.049                0.013 \n happiness / sadness happiness / surprise   sadness / surprise \n               2.041                0.545                0.267 \n\n\nFor example, happiness / sadness ~ 2.04 means that the odds (not the probability) of a correct response is 2 times higher for happy faces compared to sad faces."
  },
  {
    "objectID": "slides/02-binomial-glm.html#the-glm-function",
    "href": "slides/02-binomial-glm.html#the-glm-function",
    "title": "Binomial GLM example",
    "section": "The glm function",
    "text": "The glm function\nIn R we can fit a GLM with the glm function. The syntax is the same as the lm (for standard linear models). We only need to specify the random component and the link function.\n\nglm(y ~ x1 + x2 + x3 * x4, # systematic component (linear predictor)\n    data = data,\n    family = binomial(link = \"logit\")) # random component and link function\n\nClearly, the y in this example need to be consistent with the chosen family. In this case the model is expecting a 0-1 vector. If we provide labels (characters) or number &gt; 1 or &lt; 0 the function will fail.\n\n\n\n\n\n\nNote\n\n\nA glm with family = gaussian(link = \"identity\") is the same as running a lm. Internally glm calls lm in this case."
  },
  {
    "objectID": "slides/02-binomial-glm.html#the-null-model",
    "href": "slides/02-binomial-glm.html#the-null-model",
    "title": "Binomial GLM example",
    "section": "The null model",
    "text": "The null model\nWe can start with the easiest model that is a model without the systematic component (with no predictors).\n\nfit0 &lt;- glm(acc ~ 1, data = dat, family = binomial(link = \"logit\"))\nsummary(fit0)\n\n\nCall:\nglm(formula = acc ~ 1, family = binomial(link = \"logit\"), data = dat)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -0.03244    0.10399  -0.312    0.755\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 512.83  on 369  degrees of freedom\nResidual deviance: 512.83  on 369  degrees of freedom\nAIC: 514.83\n\nNumber of Fisher Scoring iterations: 3"
  },
  {
    "objectID": "slides/02-binomial-glm.html#the-null-model-1",
    "href": "slides/02-binomial-glm.html#the-null-model-1",
    "title": "Binomial GLM example",
    "section": "The null model",
    "text": "The null model"
  },
  {
    "objectID": "slides/02-binomial-glm.html#the-null-model-formally",
    "href": "slides/02-binomial-glm.html#the-null-model-formally",
    "title": "Binomial GLM example",
    "section": "The null model, formally",
    "text": "The null model, formally\n\\[\n\\eta_i = \\beta_0\n\\]\n\\[\np_i = g^{-1}(\\eta_i) = g^{-1}(\\beta_0)\n\\]\n\\(g^{-1}(\\cdot)\\) is the inverse-logit link:\n\\[\np_i = \\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}\n\\]\nIn other terms, the probability can be calculated inverting the logit link function evaluated on the linear predictor \\(\\eta\\). In this case \\(\\eta\\) only contains \\(\\beta_0\\)."
  },
  {
    "objectID": "slides/02-binomial-glm.html#the-null-model-interpretation",
    "href": "slides/02-binomial-glm.html#the-null-model-interpretation",
    "title": "Binomial GLM example",
    "section": "The null model, interpretation",
    "text": "The null model, interpretation\nIn this case, the intercept is -0.032. The intercept is the expected value (i.e., the mean) of y (accuracy here) when everything is zero. In this case \\(\\beta_0\\) is just the (logit transformed) overall accuracy.\n\nb0 &lt;- coef(fit0)[\"(Intercept)\"]\nexp(b0) / (1 + exp(b0)) # inverse logit\n## (Intercept) \n##   0.4918919\nplogis(b0)              # directly with the dedicated function\n## (Intercept) \n##   0.4918919\nmean(dat$acc)           # average accuracy\n## [1] 0.4918919\nlog(odds(mean(dat$acc))) # probability to logit\n## [1] -0.03243528\nqlogis(mean(dat$acc)) # probability to logit with the dedicated function\n## [1] -0.03243528"
  },
  {
    "objectID": "slides/02-binomial-glm.html#categorical-predictor-emotion",
    "href": "slides/02-binomial-glm.html#categorical-predictor-emotion",
    "title": "Binomial GLM example",
    "section": "Categorical predictor, emotion",
    "text": "Categorical predictor, emotion\nThen we can include emotion_lbl as predictor. Let’s see what happens:\n\nfit_em &lt;- glm(acc ~ emotion_lbl, data = dat, family = binomial(link = \"logit\"))\nsummary(fit_em)\n\n\nCall:\nglm(formula = acc ~ emotion_lbl, family = binomial(link = \"logit\"), \n    data = dat)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -0.69315    0.38730  -1.790  0.07350 .  \nemotion_lbldisgust    0.07411    0.44040   0.168  0.86637    \nemotion_lblfear      -2.25129    0.82235  -2.738  0.00619 ** \nemotion_lblhappiness  1.47331    0.46507   3.168  0.00154 ** \nemotion_lblsadness    0.75984    0.46555   1.632  0.10266    \nemotion_lblsurprise   2.07944    0.48917   4.251 2.13e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 512.83  on 369  degrees of freedom\nResidual deviance: 423.88  on 364  degrees of freedom\nAIC: 435.88\n\nNumber of Fisher Scoring iterations: 5"
  },
  {
    "objectID": "slides/02-binomial-glm.html#categorical-predictor-emotion-1",
    "href": "slides/02-binomial-glm.html#categorical-predictor-emotion-1",
    "title": "Binomial GLM example",
    "section": "Categorical predictor, emotion",
    "text": "Categorical predictor, emotion\nNow we have 6 coefficients. As in standard linear models, by default, categorical predictors are transformed into dummy variables:\n\nunique(model.matrix(~ emotion_lbl, data = dat))\n\n   (Intercept) emotion_lbldisgust emotion_lblfear emotion_lblhappiness\n1            1                  0               1                    0\n2            1                  1               0                    0\n3            1                  0               0                    1\n8            1                  0               0                    0\n15           1                  0               0                    0\n42           1                  0               0                    0\n   emotion_lblsadness emotion_lblsurprise\n1                   0                   0\n2                   0                   0\n3                   0                   0\n8                   1                   0\n15                  0                   1\n42                  0                   0\n\n\nThe intercept is the reference level (in this case anger) and the other 5 coefficients represent the comparison between all emotions vs anger."
  },
  {
    "objectID": "slides/02-binomial-glm.html#categorical-predictor-emotion-2",
    "href": "slides/02-binomial-glm.html#categorical-predictor-emotion-2",
    "title": "Binomial GLM example",
    "section": "Categorical predictor, emotion",
    "text": "Categorical predictor, emotion\nRemember that we are working on the link-function space where comparisons are expressed in logit. \\(\\beta_1\\) (emotion_lbldisgust) is the comparison between disgust and anger. Formally:\n\\[\n\\beta_1 = \\mbox{logit}(p(y = 1 | \\mbox{disgust})) - \\mbox{logit}(p(y = 1 | \\mbox{anger}))\n\\]\nBut the logit is the logarithm of the odds (let’s call \\(p_a\\) and \\(p_d\\) anger and disgust respectively)\n\\[\n\\log{\\frac{p_d}{1 - p_d}} - \\log{\\frac{p_a}{1 - p_a}}\n\\]\nA difference of logs can be expressed as the log of the ratio. We can take the exponential to remove the log.\n\\[\n\\log{\\frac{\\frac{p_d}{1 - p_d}}{\\frac{p_a}{1 - p_a}}} \\qquad e^{\\log{\\frac{\\frac{p_d}{1 - p_d}}{\\frac{p_a}{1 - p_a}}}} = \\frac{\\frac{p_d}{1 - p_d}}{\\frac{p_a}{1 - p_a}}\n\\]\nThis is exactly the odds ratio! This means that taking the exponential of \\(\\beta_1\\) returns the estimated odds ratio for that comparison."
  },
  {
    "objectID": "slides/02-binomial-glm.html#categorical-predictor",
    "href": "slides/02-binomial-glm.html#categorical-predictor",
    "title": "Binomial GLM example",
    "section": "Categorical predictor",
    "text": "Categorical predictor\n\ncoef(fit_em)\n\n         (Intercept)   emotion_lbldisgust      emotion_lblfear \n         -0.69314718           0.07410797          -2.25129179 \nemotion_lblhappiness   emotion_lblsadness  emotion_lblsurprise \n          1.47330574           0.75983856           2.07944154 \n\nexp(coef(fit_em))\n\n         (Intercept)   emotion_lbldisgust      emotion_lblfear \n           0.5000000            1.0769231            0.1052632 \nemotion_lblhappiness   emotion_lblsadness  emotion_lblsurprise \n           4.3636364            2.1379310            8.0000000 \n\n\nComparing with the manual calculation:\n\np_d &lt;- mean(dat$acc[dat$emotion_lbl == \"disgust\"])\np_a &lt;- mean(dat$acc[dat$emotion_lbl == \"anger\"])\n\nor(p_d, p_a)\n\n[1] 1.076923"
  },
  {
    "objectID": "slides/02-binomial-glm.html#main-effect-of-emotion",
    "href": "slides/02-binomial-glm.html#main-effect-of-emotion",
    "title": "Binomial GLM example",
    "section": "Main effect of emotion",
    "text": "Main effect of emotion\nWe can also assess the effect of emotion_lbl using a Likelihood Ratio Test (LRT). Basically we can compare the model with or without the emotion_lbl predictor. Using the LRT we are setting the effect of emotion_lbl to zero. This means that the null hypothesis is that all possible contrasts among emotions are zero.\n\nanova(fit0, fit_em) # comparing two models\n\nAnalysis of Deviance Table\n\nModel 1: acc ~ 1\nModel 2: acc ~ emotion_lbl\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       369     512.83                          \n2       364     423.88  5   88.955 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/02-binomial-glm.html#main-effect-of-emotion-1",
    "href": "slides/02-binomial-glm.html#main-effect-of-emotion-1",
    "title": "Binomial GLM example",
    "section": "Main effect of emotion",
    "text": "Main effect of emotion\n\ncar::Anova(fit_em)  # using the car::Anova\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: acc\n            LR Chisq Df Pr(&gt;Chisq)    \nemotion_lbl   88.955  5  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/02-binomial-glm.html#confidence-intervals",
    "href": "slides/02-binomial-glm.html#confidence-intervals",
    "title": "Binomial GLM example",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nThe confidence intervals for model parameters can be extracted with the confint.default() function. These are called Wald confidence intervals. They are computed as:\n\\[\n\\beta \\pm q_{\\alpha/2} \\mbox{SE}_\\beta\n\\]\nWhere \\(q\\) is the critical test statistics (\\(z\\) in this case) at \\(\\alpha\\) level and \\(\\mbox{SE}_\\beta\\) is the standard error.\n\nfits &lt;- summary(fit_em)$coefficients\nfits\n\n                        Estimate Std. Error    z value     Pr(&gt;|z|)\n(Intercept)          -0.69314718  0.3872983 -1.7896983 0.0735024219\nemotion_lbldisgust    0.07410797  0.4404044  0.1682725 0.8663688694\nemotion_lblfear      -2.25129179  0.8223512 -2.7376280 0.0061884033\nemotion_lblhappiness  1.47330574  0.4650676  3.1679388 0.0015352381\nemotion_lblsadness    0.75983856  0.4655543  1.6321158 0.1026550954\nemotion_lblsurprise   2.07944154  0.4891684  4.2509728 0.0000212844"
  },
  {
    "objectID": "slides/02-binomial-glm.html#confidence-intervals-1",
    "href": "slides/02-binomial-glm.html#confidence-intervals-1",
    "title": "Binomial GLM example",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\n(z &lt;- abs(qnorm(0.05/2))) # critical test statistics at alpha/2 (two tails)\n\n[1] 1.959964\n\ndata.frame(\n  lower = fits[, \"Estimate\"] - z * fits[, \"Std. Error\"],\n  upper = fits[, \"Estimate\"] + z * fits[, \"Std. Error\"]\n)\n\n                          lower       upper\n(Intercept)          -1.4522380  0.06594361\nemotion_lbldisgust   -0.7890688  0.93728475\nemotion_lblfear      -3.8630706 -0.63951297\nemotion_lblhappiness  0.5617900  2.38482150\nemotion_lblsadness   -0.1526311  1.67230825\nemotion_lblsurprise   1.1206891  3.03819397"
  },
  {
    "objectID": "slides/02-binomial-glm.html#confidence-intervals-2",
    "href": "slides/02-binomial-glm.html#confidence-intervals-2",
    "title": "Binomial GLM example",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nOr directly using the confint.default()\n\nconfint.default(fit_em)\n\n                          2.5 %      97.5 %\n(Intercept)          -1.4522380  0.06594361\nemotion_lbldisgust   -0.7890688  0.93728475\nemotion_lblfear      -3.8630706 -0.63951297\nemotion_lblhappiness  0.5617900  2.38482150\nemotion_lblsadness   -0.1526311  1.67230825\nemotion_lblsurprise   1.1206891  3.03819397"
  },
  {
    "objectID": "slides/02-binomial-glm.html#confidence-intervals-3",
    "href": "slides/02-binomial-glm.html#confidence-intervals-3",
    "title": "Binomial GLM example",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nBut these are confidence intervals of log odds ratios (differences in logit). To obtain confidence intervals of odds ratios we can just take the exponential of the upper and lower bound:\n\nexp(confint.default(fit_em))\n\n                          2.5 %     97.5 %\n(Intercept)          0.23404591  1.0681665\nemotion_lbldisgust   0.45426761  2.5530399\nemotion_lblfear      0.02100341  0.5275493\nemotion_lblhappiness 1.75380897 10.8571245\nemotion_lblsadness   0.85844631  5.3244438\nemotion_lblsurprise  3.06696696 20.8675218\n\n\nNotice that, for differences in logit the null value is zero. Taking \\(e^0 = 1\\) thus the null value of an odds ratio is 1 (numerator is the same as the denominator)."
  },
  {
    "objectID": "slides/02-binomial-glm.html#confidence-intervals-4",
    "href": "slides/02-binomial-glm.html#confidence-intervals-4",
    "title": "Binomial GLM example",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nThe real default for confidence intervals using just confint() (and not confint.default()) are the so-called profile likelihood confidence intervals. The main difference is that Wald confidence intervals are symmetric by definition while the profile likelihood not necessary.\n\nconfint(fit_em)\n\n                          2.5 %      97.5 %\n(Intercept)          -1.4942114  0.04295714\nemotion_lbldisgust   -0.7729310  0.96810000\nemotion_lblfear      -4.1872943 -0.80517570\nemotion_lblhappiness  0.5830921  2.41838129\nemotion_lblsadness   -0.1359529  1.70183598\nemotion_lblsurprise   1.1479829  3.07708494\n\n# you can also do exp(confint(fit_em))\n\nIn this case they are very similar to the Wald but is not always like this.\n\n\nSee this post for a nice overview."
  },
  {
    "objectID": "slides/02-binomial-glm.html#specific-contrasts-of-emotion-levels",
    "href": "slides/02-binomial-glm.html#specific-contrasts-of-emotion-levels",
    "title": "Binomial GLM example",
    "section": "Specific contrasts of emotion levels",
    "text": "Specific contrasts of emotion levels\nWe can also test some specific contrasts using the emmeans or the multcomp package. For example:\n\nlibrary(emmeans)\nmm &lt;- emmeans(fit_em, ~ emotion_lbl)\nmm\n\n emotion_lbl  emmean    SE  df asymp.LCL asymp.UCL\n anger       -0.6931 0.387 Inf    -1.452    0.0659\n disgust     -0.6190 0.210 Inf    -1.030   -0.2081\n fear        -2.9444 0.725 Inf    -4.366   -1.5226\n happiness    0.7802 0.257 Inf     0.276    1.2848\n sadness      0.0667 0.258 Inf    -0.440    0.5730\n surprise     1.3863 0.299 Inf     0.801    1.9719\n\nResults are given on the logit (not the response) scale. \nConfidence level used: 0.95 \n\n\nThese are called estimated mrginal means. Importantly, the emmeans package uses the model and not the data. Marginal means will depend on the fitted model."
  },
  {
    "objectID": "slides/02-binomial-glm.html#specific-contrasts-of-emotion-levels-1",
    "href": "slides/02-binomial-glm.html#specific-contrasts-of-emotion-levels-1",
    "title": "Binomial GLM example",
    "section": "Specific contrasts of emotion levels",
    "text": "Specific contrasts of emotion levels\nWe would expect estimated probabilities but we have values in logit (this is why we have negative values). We can also transform the logit into probabilities:\n\nemmeans(fit_em, ~ emotion_lbl, type = \"response\")\n\n\n emotion_lbl  prob     SE  df asymp.LCL asymp.UCL\n anger       0.333 0.0861 Inf    0.1897     0.516\n disgust     0.350 0.0477 Inf    0.2631     0.448\n fear        0.050 0.0345 Inf    0.0125     0.179\n happiness   0.686 0.0555 Inf    0.5685     0.783\n sadness     0.517 0.0645 Inf    0.3918     0.639\n surprise    0.800 0.0478 Inf    0.6901     0.878\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale"
  },
  {
    "objectID": "slides/02-binomial-glm.html#what-emmeans-is-doing",
    "href": "slides/02-binomial-glm.html#what-emmeans-is-doing",
    "title": "Binomial GLM example",
    "section": "What emmeans is doing?",
    "text": "What emmeans is doing?\nTo understand what emmeans is doing we need to introduce the term prediction. Given the predictors we ask the model the predicted logit or probability.\n\n# prediction for the first 5 trials. \n# The best prediction of the model is the (logit) mean\n\nhead(predict(fit0, type = \"link\"))\n\n          1           2           3           4           5           6 \n-0.03243528 -0.03243528 -0.03243528 -0.03243528 -0.03243528 -0.03243528"
  },
  {
    "objectID": "slides/02-binomial-glm.html#what-emmeans-is-doing-1",
    "href": "slides/02-binomial-glm.html#what-emmeans-is-doing-1",
    "title": "Binomial GLM example",
    "section": "What emmeans is doing?",
    "text": "What emmeans is doing?\n\n# prediction for the first 5 trials. \n# the prediction depend on the emotion\n\nhead(predict(fit_em, type = \"link\")) \n\n         1          2          3          4          5          6 \n-2.9444390 -0.6190392  0.7801586  0.7801586 -0.6190392 -2.9444390 \n\nhead(predict(fit_em, type = \"response\")) \n\n        1         2         3         4         5         6 \n0.0500000 0.3500000 0.6857143 0.6857143 0.3500000 0.0500000 \n\nhead(plogis(predict(fit_em, type = \"link\"))) # same\n\n        1         2         3         4         5         6 \n0.0500000 0.3500000 0.6857143 0.6857143 0.3500000 0.0500000"
  },
  {
    "objectID": "slides/02-binomial-glm.html#what-emmeans-is-doing-2",
    "href": "slides/02-binomial-glm.html#what-emmeans-is-doing-2",
    "title": "Binomial GLM example",
    "section": "What emmeans is doing?",
    "text": "What emmeans is doing?\nFor example, to know what is the predicted accuracy for anger and disgust we can do:\n\npredict(fit_em, newdata = data.frame(emotion_lbl = c(\"anger\", \"disgust\")), type = \"response\")\n\n        1         2 \n0.3333333 0.3500000 \n\n\nOr manually:\n\nB &lt;- coef(fit_em)\nc(plogis(B[\"(Intercept)\"]), # anger\n  plogis(B[\"(Intercept)\"] + B[\"emotion_lbldisgust\"])) # disgust\n\n(Intercept) (Intercept) \n  0.3333333   0.3500000 \n\n\nOn the logit scale, we can do linear combinations of coefficients. This is not valid on the probability scale, this is the reason why we need the link function."
  },
  {
    "objectID": "slides/02-binomial-glm.html#what-emmeans-is-doing-3",
    "href": "slides/02-binomial-glm.html#what-emmeans-is-doing-3",
    "title": "Binomial GLM example",
    "section": "What emmeans is doing?",
    "text": "What emmeans is doing?\nFor reproducing the entire emmeans output we just need to provide all emotions into newdata =\n\nnd &lt;- data.frame(emotion_lbl = unique(dat$emotion_lbl))\ndata.frame(predict(fit_em, newdata = nd, se = TRUE, type = \"response\"))\n\n        fit     se.fit residual.scale\n1 0.0500000 0.03445835              1\n2 0.3500000 0.04769696              1\n3 0.6857143 0.05548619              1\n4 0.5166667 0.06451385              1\n5 0.8000000 0.04780914              1\n6 0.3333333 0.08606630              1"
  },
  {
    "objectID": "slides/02-binomial-glm.html#contrasts-with-emmeans",
    "href": "slides/02-binomial-glm.html#contrasts-with-emmeans",
    "title": "Binomial GLM example",
    "section": "Contrasts with emmeans",
    "text": "Contrasts with emmeans\nWe can also compute all contrasts across emotions:\n\n# or emmeans(fit_em, pairwise ~ emotion_lbl)\npairs(mm, p.adjust = \"bonferroni\")\n\n contrast             estimate    SE  df z.ratio p.value\n anger - disgust       -0.0741 0.440 Inf  -0.168  1.0000\n anger - fear           2.2513 0.822 Inf   2.738  0.0680\n anger - happiness     -1.4733 0.465 Inf  -3.168  0.0192\n anger - sadness       -0.7598 0.466 Inf  -1.632  0.5771\n anger - surprise      -2.0794 0.489 Inf  -4.251  0.0003\n disgust - fear         2.3254 0.755 Inf   3.079  0.0253\n disgust - happiness   -1.3992 0.332 Inf  -4.214  0.0004\n disgust - sadness     -0.6857 0.333 Inf  -2.061  0.3080\n disgust - surprise    -2.0053 0.365 Inf  -5.494  &lt;.0001\n fear - happiness      -3.7246 0.770 Inf  -4.839  &lt;.0001\n fear - sadness        -3.0111 0.770 Inf  -3.910  0.0013\n fear - surprise       -4.3307 0.785 Inf  -5.520  &lt;.0001\n happiness - sadness    0.7135 0.365 Inf   1.956  0.3679\n happiness - surprise  -0.6061 0.394 Inf  -1.537  0.6404\n sadness - surprise    -1.3196 0.395 Inf  -3.341  0.0108\n\nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: tukey method for comparing a family of 6 estimates \n\n\nBe careful to the multiple comparison approach! You can use the p.adjust = argument and choose an appropriate method."
  },
  {
    "objectID": "slides/02-binomial-glm.html#contrasts-with-emmeans-1",
    "href": "slides/02-binomial-glm.html#contrasts-with-emmeans-1",
    "title": "Binomial GLM example",
    "section": "Contrasts with emmeans",
    "text": "Contrasts with emmeans\nSome of these contrasts are also the model parameters:\n\n contrast             estimate    SE  df z.ratio p.value\n anger - disgust       -0.0741 0.440 Inf  -0.168  1.0000\n anger - fear           2.2513 0.822 Inf   2.738  0.0680\n anger - happiness     -1.4733 0.465 Inf  -3.168  0.0192\n anger - sadness       -0.7598 0.466 Inf  -1.632  0.5771\n anger - surprise      -2.0794 0.489 Inf  -4.251  0.0003\n disgust - fear         2.3254 0.755 Inf   3.079  0.0253\n disgust - happiness   -1.3992 0.332 Inf  -4.214  0.0004\n disgust - sadness     -0.6857 0.333 Inf  -2.061  0.3080\n disgust - surprise    -2.0053 0.365 Inf  -5.494  &lt;.0001\n fear - happiness      -3.7246 0.770 Inf  -4.839  &lt;.0001\n fear - sadness        -3.0111 0.770 Inf  -3.910  0.0013\n fear - surprise       -4.3307 0.785 Inf  -5.520  &lt;.0001\n happiness - sadness    0.7135 0.365 Inf   1.956  0.3679\n happiness - surprise  -0.6061 0.394 Inf  -1.537  0.6404\n sadness - surprise    -1.3196 0.395 Inf  -3.341  0.0108\n\nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: tukey method for comparing a family of 6 estimates"
  },
  {
    "objectID": "slides/02-binomial-glm.html#contrasts-with-emmeans-2",
    "href": "slides/02-binomial-glm.html#contrasts-with-emmeans-2",
    "title": "Binomial GLM example",
    "section": "Contrasts with emmeans",
    "text": "Contrasts with emmeans\nYou can also express the contrasts into the probability space. We are just taking the exponential thus transforming differences of logit into odds ratios.\n\npairs(mm, type = \"response\")\n\n contrast             odds.ratio     SE  df null z.ratio p.value\n anger / disgust          0.9286 0.4090 Inf    1  -0.168  1.0000\n anger / fear             9.5000 7.8100 Inf    1   2.738  0.0680\n anger / happiness        0.2292 0.1070 Inf    1  -3.168  0.0192\n anger / sadness          0.4677 0.2180 Inf    1  -1.632  0.5771\n anger / surprise         0.1250 0.0611 Inf    1  -4.251  0.0003\n disgust / fear          10.2308 7.7300 Inf    1   3.079  0.0253\n disgust / happiness      0.2468 0.0819 Inf    1  -4.214  0.0004\n disgust / sadness        0.5037 0.1680 Inf    1  -2.061  0.3080\n disgust / surprise       0.1346 0.0491 Inf    1  -5.494  &lt;.0001\n fear / happiness         0.0241 0.0186 Inf    1  -4.839  &lt;.0001\n fear / sadness           0.0492 0.0379 Inf    1  -3.910  0.0013\n fear / surprise          0.0132 0.0103 Inf    1  -5.520  &lt;.0001\n happiness / sadness      2.0411 0.7440 Inf    1   1.956  0.3679\n happiness / surprise     0.5455 0.2150 Inf    1  -1.537  0.6404\n sadness / surprise       0.2672 0.1060 Inf    1  -3.341  0.0108\n\nP value adjustment: tukey method for comparing a family of 6 estimates \nTests are performed on the log odds ratio scale \n\n\nNotice that: Tests are performed on the log odds ratio scale"
  },
  {
    "objectID": "slides/02-binomial-glm.html#custom-contrasts",
    "href": "slides/02-binomial-glm.html#custom-contrasts",
    "title": "Binomial GLM example",
    "section": "Custom contrasts",
    "text": "Custom contrasts\nClearly you can also provide custom contrasts like contr.sum() or MASS::contr.sdiff() (for comparing the next level with the previous level). For an overview about contrasts coding see Granziol et al. (2025) and Schad et al. (2020).\n\ncontrast(mm, \"consec\") # consec ~ MASS::contr.sdif()\n\n contrast            estimate    SE  df z.ratio p.value\n disgust - anger       0.0741 0.440 Inf   0.168  0.9999\n fear - disgust       -2.3254 0.755 Inf  -3.079  0.0090\n happiness - fear      3.7246 0.770 Inf   4.839  &lt;.0001\n sadness - happiness  -0.7135 0.365 Inf  -1.956  0.1967\n surprise - sadness    1.3196 0.395 Inf   3.341  0.0034\n\nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: mvt method for 5 tests \n\n# see ?emmeans::contrast\n\nHere we are comparing 2 vs 1, 2 vs 3, 3 vs 4, etc."
  },
  {
    "objectID": "slides/02-binomial-glm.html#plotting",
    "href": "slides/02-binomial-glm.html#plotting",
    "title": "Binomial GLM example",
    "section": "Plotting",
    "text": "Plotting\nThere are several options to plot the model results. In this case we could plot the predicted probability for each emotion and the confidence intervals. You can use the effects package or ggeffects (that internally uses effects) to create ggplot2 objects.\n\nlibrary(ggeffects)\n# grid of prediction and CI (as we did with emmeans or predict())\nggeffect(fit_em, \"emotion_lbl\")\n\n# Predicted probabilities of acc\n\nemotion_lbl | Predicted |     95% CI\n------------------------------------\nanger       |      0.33 | 0.19, 0.52\ndisgust     |      0.35 | 0.26, 0.45\nfear        |      0.05 | 0.01, 0.18\nhappiness   |      0.69 | 0.57, 0.78\nsadness     |      0.52 | 0.39, 0.64\nsurprise    |      0.80 | 0.69, 0.88"
  },
  {
    "objectID": "slides/02-binomial-glm.html#plotting-1",
    "href": "slides/02-binomial-glm.html#plotting-1",
    "title": "Binomial GLM example",
    "section": "Plotting",
    "text": "Plotting\n\n# this return a ggplot2 object, you can add layers with +\nplot(ggeffect(fit_em, \"emotion_lbl\"))"
  },
  {
    "objectID": "slides/02-binomial-glm.html#numerical-predictor-effect-of-intensity",
    "href": "slides/02-binomial-glm.html#numerical-predictor-effect-of-intensity",
    "title": "Binomial GLM example",
    "section": "Numerical predictor, effect of intensity",
    "text": "Numerical predictor, effect of intensity\nNow let’s fit a model with only the effect of intensity.\n\nfit_int &lt;- glm(acc ~ intensity, data = dat, family = binomial(link = \"logit\"))\nsummary(fit_int)\n\n\nCall:\nglm(formula = acc ~ intensity, family = binomial(link = \"logit\"), \n    data = dat)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.797692   0.263646  -6.819 9.19e-12 ***\nintensity    0.031976   0.004291   7.452 9.19e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 512.83  on 369  degrees of freedom\nResidual deviance: 447.05  on 368  degrees of freedom\nAIC: 451.05\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "slides/02-binomial-glm.html#numerical-predictor-effect-of-intensity-1",
    "href": "slides/02-binomial-glm.html#numerical-predictor-effect-of-intensity-1",
    "title": "Binomial GLM example",
    "section": "Numerical predictor, effect of intensity",
    "text": "Numerical predictor, effect of intensity\nNow the intercept is the logit accuracy when intensity is zero (that is not really a meaningful value). \\(\\beta_1\\) here is the expected increase in logit for a unit increase in intensity. In other terms, moving from intensity e.g., 10 to 11 increase the logit of 0.03.\nAs for the emotion, we can take the exponential of \\(\\beta_1\\) obtaining the odds ratio of increasing 1 unit in intensity:\n\nexp(coef(fit_int))\n\n(Intercept)   intensity \n  0.1656809   1.0324926 \n\n\nWe have an odds ratio of 1.03 that is quite low (1 is the null value). But this is a scale problem, 1 point in the intensity scale is meaningless."
  },
  {
    "objectID": "slides/02-binomial-glm.html#numerical-predictor-effect-of-intensity-2",
    "href": "slides/02-binomial-glm.html#numerical-predictor-effect-of-intensity-2",
    "title": "Binomial GLM example",
    "section": "Numerical predictor, effect of intensity",
    "text": "Numerical predictor, effect of intensity\nBefore improving the model, notice that the story is the same regardless having categorical or numerical predictor. For categorical predictors the coefficients are odds ratios (or difference in logit) comparing levels (anger vs fear). For numerical predictor the coefficients are odds ratios (or difference in logit) comparing values separated by 1 unit (in the scale of the predictor).\n\n(pp &lt;- predict(fit_int, newdata = data.frame(intensity = c(15, 16))))\n\n        1         2 \n-1.318054 -1.286078 \n\ndiff(pp) # same as b1\n\n         2 \n0.03197586 \n\nexp(diff(pp))  # same as exp(b1)\n\n       2 \n1.032493"
  },
  {
    "objectID": "slides/02-binomial-glm.html#numerical-predictor-effect-of-intensity-3",
    "href": "slides/02-binomial-glm.html#numerical-predictor-effect-of-intensity-3",
    "title": "Binomial GLM example",
    "section": "Numerical predictor, effect of intensity",
    "text": "Numerical predictor, effect of intensity\nIn fact, we can clearly see that on the logit scale the effect is linear while on the probability scale is not linear.\n\n# pairs of unit differences in different positions of x\ndiffs &lt;- list(c(10, 11), c(50, 51), c(80, 81))\nsapply(diffs, function(d) diff(predict(fit_int, data.frame(intensity = d))))\n\n         2          2          2 \n0.03197586 0.03197586 0.03197586 \n\n\nBut is not the same when we take differences in probabilities\n\n# pairs of unit differences in different positions of x\ndiffs &lt;- list(c(10, 11), c(50, 51), c(80, 81))\nsapply(diffs, function(d) diff(predict(fit_int, data.frame(intensity = d), type = \"response\")))\n\n          2           2           2 \n0.004884715 0.007927308 0.006900733"
  },
  {
    "objectID": "slides/02-binomial-glm.html#numerical-predictor-effect-of-intensity-4",
    "href": "slides/02-binomial-glm.html#numerical-predictor-effect-of-intensity-4",
    "title": "Binomial GLM example",
    "section": "Numerical predictor, effect of intensity",
    "text": "Numerical predictor, effect of intensity\nSame increase in intensity produces a different increase on the probability scale but not on the logit scale."
  },
  {
    "objectID": "slides/02-binomial-glm.html#numerical-predictor-marginal-effects",
    "href": "slides/02-binomial-glm.html#numerical-predictor-marginal-effects",
    "title": "Binomial GLM example",
    "section": "Numerical predictor, marginal effects",
    "text": "Numerical predictor, marginal effects\nThis means that for interpreting results in the probability scale (what we actually want) we cannot think in linear terms (as in standard linear regression). For each value of intensity we have a different slope (i.e., derivative)."
  },
  {
    "objectID": "slides/02-binomial-glm.html#marginal-effects",
    "href": "slides/02-binomial-glm.html#marginal-effects",
    "title": "Binomial GLM example",
    "section": "Marginal effects",
    "text": "Marginal effects\nNow let’s plot all the red slopes and see what we can learn:"
  },
  {
    "objectID": "slides/02-binomial-glm.html#marginal-effects-a-really-comprehensive-framework",
    "href": "slides/02-binomial-glm.html#marginal-effects-a-really-comprehensive-framework",
    "title": "Binomial GLM example",
    "section": "Marginal effects: a really comprehensive framework",
    "text": "Marginal effects: a really comprehensive framework\nThe marginaleffects package provide a very complete and comprehensive framework to compute marginal effects for several models. You can have a very detailed overview of the theory and the functions reading:\n\nThe main paper by Arel-Bundock et al. (2024)\nhttps://www.youtube.com/watch?v=ANDC_kkAjeM"
  },
  {
    "objectID": "slides/02-binomial-glm.html#marginal-effects-of-intensity",
    "href": "slides/02-binomial-glm.html#marginal-effects-of-intensity",
    "title": "Binomial GLM example",
    "section": "Marginal effects of intensity",
    "text": "Marginal effects of intensity\n\nlibrary(marginaleffects)\nsl &lt;- slopes(fit_int, variables = \"intensity\", by = \"intensity\")\nsl\n\n\n intensity Estimate Std. Error     z Pr(&gt;|z|)     S   2.5 %  97.5 %\n        10  0.00484   0.000351 13.77   &lt;0.001 140.8 0.00415 0.00552\n        20  0.00582   0.000471 12.35   &lt;0.001 113.9 0.00489 0.00674\n        30  0.00674   0.000685  9.84   &lt;0.001  73.5 0.00540 0.00808\n        40  0.00748   0.000905  8.27   &lt;0.001  52.7 0.00571 0.00925\n        50  0.00792   0.001048  7.55   &lt;0.001  44.4 0.00586 0.00997\n        60  0.00796   0.001061  7.51   &lt;0.001  43.9 0.00589 0.01004\n        70  0.00762   0.000938  8.12   &lt;0.001  50.9 0.00578 0.00946\n        80  0.00694   0.000725  9.57   &lt;0.001  69.7 0.00552 0.00836\n        90  0.00605   0.000502 12.05   &lt;0.001 108.7 0.00507 0.00703\n       100  0.00507   0.000362 14.01   &lt;0.001 145.8 0.00436 0.00578\n\nTerm: intensity\nType: response\nComparison: dY/dX"
  },
  {
    "objectID": "slides/02-binomial-glm.html#marginal-effects-of-intensity-1",
    "href": "slides/02-binomial-glm.html#marginal-effects-of-intensity-1",
    "title": "Binomial GLM example",
    "section": "Marginal effects of intensity",
    "text": "Marginal effects of intensity\n\n# average marginal effect\navg_slopes(fit_int)\n\n\n Estimate Std. Error  z Pr(&gt;|z|)    S   2.5 %  97.5 %\n  0.00664   0.000602 11   &lt;0.001 91.7 0.00546 0.00782\n\nTerm: intensity\nType: response\nComparison: dY/dX"
  },
  {
    "objectID": "slides/02-binomial-glm.html#marginal-effects-of-intensity-2",
    "href": "slides/02-binomial-glm.html#marginal-effects-of-intensity-2",
    "title": "Binomial GLM example",
    "section": "Marginal effects of intensity",
    "text": "Marginal effects of intensity\n\n# max marginal effect\nfilter(sl, estimate == max(estimate))\n\n\n intensity Estimate Std. Error    z Pr(&gt;|z|)    S   2.5 % 97.5 %\n        60  0.00796    0.00106 7.51   &lt;0.001 43.9 0.00589   0.01\n\nTerm: intensity\nType: response\nComparison: dY/dX"
  },
  {
    "objectID": "slides/02-binomial-glm.html#marginal-effects-of-intensity-3",
    "href": "slides/02-binomial-glm.html#marginal-effects-of-intensity-3",
    "title": "Binomial GLM example",
    "section": "Marginal effects of intensity",
    "text": "Marginal effects of intensity\n\nplot_slopes(fit_int, variables = \"intensity\", by = \"intensity\", vcov = FALSE)"
  },
  {
    "objectID": "slides/02-binomial-glm.html#plotting-2",
    "href": "slides/02-binomial-glm.html#plotting-2",
    "title": "Binomial GLM example",
    "section": "Plotting",
    "text": "Plotting\nThe pattern for intensity is almost linear, this is why we have more than one maximum.\n\nplot(ggeffect(fit_int, \"intensity\"))"
  },
  {
    "objectID": "slides/02-binomial-glm.html#inverse-estimation",
    "href": "slides/02-binomial-glm.html#inverse-estimation",
    "title": "Binomial GLM example",
    "section": "Inverse estimation",
    "text": "Inverse estimation\nWe can also do what is called inverse estimation (common in Psychophysics). We can ask the model what is the level of intensity required to achieve a certain accuracy.\n\nMASS::dose.p(fit_int, p = 0.75)\n\n              Dose       SE\np = 0.75: 90.57784 5.916972\n\n\nFurthermore, we need roughly 90% of intensity to have an accuracy of 75%."
  },
  {
    "objectID": "slides/02-binomial-glm.html#improving-the-model",
    "href": "slides/02-binomial-glm.html#improving-the-model",
    "title": "Binomial GLM example",
    "section": "Improving the model",
    "text": "Improving the model\nWe have two problems in terms of interpretability in this model:\n\nThe intercept is meaningless because 0% intensity is not a plausible value\nIntensity from 0% to 100% in steps of 1% is too granular\n\nWe can center the variable on the minimum (0 become 10%) and rescale the variable from 0 (10%) to 10 (100%) where the unit increase is 10%.\n\ndat$intensity10 &lt;- (dat$intensity - 10) / 10"
  },
  {
    "objectID": "slides/02-binomial-glm.html#additive-model-intensity-and-emotion",
    "href": "slides/02-binomial-glm.html#additive-model-intensity-and-emotion",
    "title": "Binomial GLM example",
    "section": "Additive model, intensity and emotion",
    "text": "Additive model, intensity and emotion\nWe can now fit a model with the additive effect of emotion and intensity. To simplify the pattern let’s keep only two emotions.\n\nfit_int_emo &lt;- glm(acc ~ intensity10 + emotion_lbl, data = dat, family = binomial(link = \"logit\"), subset = emotion_lbl %in% c(\"anger\", \"surprise\"))\n\nsummary(fit_int_emo)\n\n\nCall:\nglm(formula = acc ~ intensity10 + emotion_lbl, family = binomial(link = \"logit\"), \n    data = dat, subset = emotion_lbl %in% c(\"anger\", \"surprise\"))\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -5.2155     1.2367  -4.217 2.47e-05 ***\nintensity10           0.8363     0.1858   4.502 6.73e-06 ***\nemotion_lblsurprise   4.1623     1.0010   4.158 3.21e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 128.207  on 99  degrees of freedom\nResidual deviance:  63.942  on 97  degrees of freedom\nAIC: 69.942\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "slides/02-binomial-glm.html#additive-model-intensity-and-emotion-1",
    "href": "slides/02-binomial-glm.html#additive-model-intensity-and-emotion-1",
    "title": "Binomial GLM example",
    "section": "Additive model, intensity and emotion",
    "text": "Additive model, intensity and emotion\nThe interpretation is the same as before. The intercept is the expected logit when everything is zero (intensity = 10 and emotion = anger).\nintensity is the increase in logit accuracy for a unit (10%) increase in intensity controlling for emotion_lbl.\nemotion_lblsurprise is the logit difference between anger and surprise controlling for emotion_lbl."
  },
  {
    "objectID": "slides/02-binomial-glm.html#additive-model-intensity-and-emotion-2",
    "href": "slides/02-binomial-glm.html#additive-model-intensity-and-emotion-2",
    "title": "Binomial GLM example",
    "section": "Additive model, intensity and emotion",
    "text": "Additive model, intensity and emotion\nWe can have also the two main effects (not really useful with a factor with two levels):\n\ncar::Anova(fit_int_emo)\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: acc\n            LR Chisq Df Pr(&gt;Chisq)    \nintensity10   44.305  1  2.809e-11 ***\nemotion_lbl   32.834  1  1.004e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/02-binomial-glm.html#additive-model-intensity-and-emotion-3",
    "href": "slides/02-binomial-glm.html#additive-model-intensity-and-emotion-3",
    "title": "Binomial GLM example",
    "section": "Additive model, intensity and emotion",
    "text": "Additive model, intensity and emotion\n\nlibrary(patchwork) # composing plots\n\nplot(ggeffect(fit_int_emo, \"intensity10\")) + \n  plot(ggeffect(fit_int_emo, \"emotion_lbl\"))"
  },
  {
    "objectID": "slides/02-binomial-glm.html#additive-model-intensity-and-emotion-4",
    "href": "slides/02-binomial-glm.html#additive-model-intensity-and-emotion-4",
    "title": "Binomial GLM example",
    "section": "Additive model, intensity and emotion",
    "text": "Additive model, intensity and emotion\nThis means that we have an odds ratio of 64.22 in favor of anger and an odds ratio of 2.31 for a unit increase in intensity.\nWe can also compute again the marginal effects for intensity for each emotion:\n\navg_slopes(fit_int_emo, variables = \"intensity10\", by = \"emotion_lbl\")\n\n\n emotion_lbl Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n    anger      0.0937    0.00508 18.45   &lt;0.001 250.0 0.0837 0.1036\n    surprise   0.0815    0.00920  8.86   &lt;0.001  60.2 0.0635 0.0995\n\nTerm: intensity10\nType: response\nComparison: dY/dX\n\n\nThis means that we have on average an 8% and 9% of increase in accuracy."
  },
  {
    "objectID": "slides/02-binomial-glm.html#interaction-model-intensity-and-emotion",
    "href": "slides/02-binomial-glm.html#interaction-model-intensity-and-emotion",
    "title": "Binomial GLM example",
    "section": "Interaction model, intensity and emotion",
    "text": "Interaction model, intensity and emotion\nThe final model that we can try is including the interaction between intensity and emotion.\n\nfit_emo_x_int &lt;- glm(acc ~ intensity10 * emotion_lbl, \n                     data = dat, family = binomial(link = \"logit\"), \n                     subset = emotion_lbl %in% c(\"anger\", \"surprise\"))\n\nsummary(fit_emo_x_int)\n\n\nCall:\nglm(formula = acc ~ intensity10 * emotion_lbl, family = binomial(link = \"logit\"), \n    data = dat, subset = emotion_lbl %in% c(\"anger\", \"surprise\"))\n\nCoefficients:\n                                Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)                      -3.1654     1.1766  -2.690  0.00714 **\nintensity10                       0.4841     0.1933   2.504  0.01228 * \nemotion_lblsurprise               1.0226     1.4454   0.707  0.47929   \nintensity10:emotion_lblsurprise   0.9782     0.4738   2.065  0.03897 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 128.207  on 99  degrees of freedom\nResidual deviance:  58.274  on 96  degrees of freedom\nAIC: 66.274\n\nNumber of Fisher Scoring iterations: 7"
  },
  {
    "objectID": "slides/02-binomial-glm.html#interaction-model-intensity-and-emotion-1",
    "href": "slides/02-binomial-glm.html#interaction-model-intensity-and-emotion-1",
    "title": "Binomial GLM example",
    "section": "Interaction model, intensity and emotion",
    "text": "Interaction model, intensity and emotion\nWith interactions, always visualize first:\n\nplot(ggeffect(fit_emo_x_int, terms = c(\"intensity10\", \"emotion_lbl\")))"
  },
  {
    "objectID": "slides/02-binomial-glm.html#interaction-model-intensity-and-emotion-2",
    "href": "slides/02-binomial-glm.html#interaction-model-intensity-and-emotion-2",
    "title": "Binomial GLM example",
    "section": "Interaction model, intensity and emotion",
    "text": "Interaction model, intensity and emotion\nClearly the effect of intensity is not the same for anger and surprise. The participant reaches high accuracies faster for surprised faces compared to angry faces.\n\nintercept: is the expected logit for anger and intensity 0 (10%). Can be considered as the accuracy for the hardest angry face.\nintensity10: is the increase in logit accuracy for a unit increase (10%) in intensity for angry faces (the red slope in the previous plot)\nemotion_lblsurprise: is the logit difference (log odds ratio) between anger and surprise when intensity is 0 (10%). Is a conditional log odds ratio for a fixed value of intensity.\nintensity10:emotion_lblsurprise: this is the actual interaction. Is the logit difference of the two slopes (in logit). Is the red slope vs the blue slope."
  },
  {
    "objectID": "slides/02-binomial-glm.html#interaction-model-intensity-and-emotion-3",
    "href": "slides/02-binomial-glm.html#interaction-model-intensity-and-emotion-3",
    "title": "Binomial GLM example",
    "section": "Interaction model, intensity and emotion",
    "text": "Interaction model, intensity and emotion\nLet’s improve a little bit the interpretation. We can center the emotion applying not the dummy (or treatment) coding but the sum to zero coding.\n\ndatsub &lt;- filter(dat, emotion_lbl %in% c(\"anger\", \"surprise\"))\ndatsub$emotion_lbl &lt;- factor(datsub$emotion_lbl)\ncontrasts(datsub$emotion_lbl) &lt;- -contr.sum(2)/2\ncontrasts(datsub$emotion_lbl)\n\n         [,1]\nanger    -0.5\nsurprise  0.5"
  },
  {
    "objectID": "slides/02-binomial-glm.html#interaction-model-intensity-and-emotion-4",
    "href": "slides/02-binomial-glm.html#interaction-model-intensity-and-emotion-4",
    "title": "Binomial GLM example",
    "section": "Interaction model, intensity and emotion",
    "text": "Interaction model, intensity and emotion\n\nfit_emo_x_int2 &lt;- glm(acc ~ intensity10 * emotion_lbl, \n                      data = datsub, \n                      family = binomial(link = \"logit\"))\n\nsummary(fit_emo_x_int2)\n\n\nCall:\nglm(formula = acc ~ intensity10 * emotion_lbl, family = binomial(link = \"logit\"), \n    data = datsub)\n\nCoefficients:\n                         Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)               -2.6541     0.7227  -3.672  0.00024 ***\nintensity10                0.9732     0.2369   4.108 3.99e-05 ***\nemotion_lbl1               1.0226     1.4454   0.707  0.47929    \nintensity10:emotion_lbl1   0.9782     0.4738   2.065  0.03897 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 128.207  on 99  degrees of freedom\nResidual deviance:  58.274  on 96  degrees of freedom\nAIC: 66.274\n\nNumber of Fisher Scoring iterations: 7"
  },
  {
    "objectID": "slides/02-binomial-glm.html#interaction-model-intensity-and-emotion-5",
    "href": "slides/02-binomial-glm.html#interaction-model-intensity-and-emotion-5",
    "title": "Binomial GLM example",
    "section": "Interaction model, intensity and emotion",
    "text": "Interaction model, intensity and emotion\nThe parameters are interpreted in the same way, but now we have a different meaning of 0:\n\nThe intercept is the average logit accuracy when intensity is 10%\nintensity10: is the slope when emotion_lbl is 0 but 0 now is in the middle of anger and surprise. This means that intensity10 is the main effect of intensity controlling for emotion.\nThe interaction is the same as before as well as the other emotion_lbl1 parameter."
  },
  {
    "objectID": "slides/02-binomial-glm.html#interaction-model-intensity-and-emotion-6",
    "href": "slides/02-binomial-glm.html#interaction-model-intensity-and-emotion-6",
    "title": "Binomial GLM example",
    "section": "Interaction model, intensity and emotion",
    "text": "Interaction model, intensity and emotion\n\nnames(fit_emo_x_int$coefficients) &lt;- names(fit_emo_x_int2$coefficients) # just for a better output, dangerous otherwise\ncar::compareCoefs(fit_emo_x_int, fit_emo_x_int2)\n\nCalls:\n1: glm(formula = acc ~ intensity10 * emotion_lbl, family = binomial(link = \n  \"logit\"), data = dat, subset = emotion_lbl %in% c(\"anger\", \"surprise\"))\n2: glm(formula = acc ~ intensity10 * emotion_lbl, family = binomial(link = \n  \"logit\"), data = datsub)\n\n                         Model 1 Model 2\n(Intercept)               -3.165  -2.654\nSE                         1.177   0.723\n                                        \nintensity10                0.484   0.973\nSE                         0.193   0.237\n                                        \nemotion_lbl1                1.02    1.02\nSE                          1.45    1.45\n                                        \nintensity10:emotion_lbl1   0.978   0.978\nSE                         0.474   0.474"
  },
  {
    "objectID": "slides/02-binomial-glm.html#deviance",
    "href": "slides/02-binomial-glm.html#deviance",
    "title": "Binomial GLM example",
    "section": "Deviance",
    "text": "Deviance\nWhen using the summary() function we can see that there is as section about Deviance:\n\nsummary(fit_int)\n\n\n\nCall:\nglm(formula = acc ~ intensity, family = binomial(link = \"logit\"), \n    data = dat)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.797692   0.263646  -6.819 9.19e-12 ***\nintensity    0.031976   0.004291   7.452 9.19e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 512.83  on 369  degrees of freedom\nResidual deviance: 447.05  on 368  degrees of freedom\nAIC: 451.05\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nThis information can be used to assess the goodness of fit of the model and also to compute pseudo-\\(R^2\\) values (see later)."
  },
  {
    "objectID": "slides/02-binomial-glm.html#deviance-1",
    "href": "slides/02-binomial-glm.html#deviance-1",
    "title": "Binomial GLM example",
    "section": "Deviance",
    "text": "Deviance\nWe need to define three types of models:\n\nNull Model: a model without predictors (only the intercept)\nActual Model: the model we fitted with predictors of interest\nSaturated Model: a model fitting the data perfectly (no error)"
  },
  {
    "objectID": "slides/02-binomial-glm.html#deviance-and-likelihood",
    "href": "slides/02-binomial-glm.html#deviance-and-likelihood",
    "title": "Binomial GLM example",
    "section": "Deviance and likelihood",
    "text": "Deviance and likelihood\nThis is a visual representation of the three models. The current model should be always between the null and the saturated."
  },
  {
    "objectID": "slides/02-binomial-glm.html#deviance-and-likelihood-1",
    "href": "slides/02-binomial-glm.html#deviance-and-likelihood-1",
    "title": "Binomial GLM example",
    "section": "Deviance and likelihood",
    "text": "Deviance and likelihood\nWe can simplify the idea of likelihood and deviance thinking about the distance between the fitted line and the points. As the distance decreases, the likelihood of the model increases."
  },
  {
    "objectID": "slides/02-binomial-glm.html#deviance-2",
    "href": "slides/02-binomial-glm.html#deviance-2",
    "title": "Binomial GLM example",
    "section": "Deviance",
    "text": "Deviance\nThe null and residual deviance that we see in the model output can be calculated as:\n\\[\nD_{\\mbox{null}} = 2 -(log(\\mathcal{L}_{\\mbox{null}}) - log(\\mathcal{L}_{\\mbox{sat}}))\n\\] \\[\nD_{\\mbox{resid}} = 2 -(log(\\mathcal{L}_{\\mbox{current}}) - log(\\mathcal{L}_{\\mbox{sat}}))\n\\]"
  },
  {
    "objectID": "slides/02-binomial-glm.html#deviance-3",
    "href": "slides/02-binomial-glm.html#deviance-3",
    "title": "Binomial GLM example",
    "section": "Deviance",
    "text": "Deviance\n\ndat$id &lt;- factor(1:nrow(dat))\nfit_cur &lt;- fit_int # current model\nfit_sat &lt;- glm(acc ~ 0 + id, data = dat, family = binomial(link = \"logit\"))\nfit_null &lt;- glm(acc ~ 1, data = dat, family = binomial(link = \"logit\"))\n\n\n# residual\n2 * -(logLik(fit_cur) - logLik(fit_sat))\n\n'log Lik.' 447.0538 (df=2)\n\n# null\n2 * -(logLik(fit_null) - logLik(fit_sat))\n\n'log Lik.' 512.8316 (df=1)"
  },
  {
    "objectID": "slides/02-binomial-glm.html#deviance-lrt",
    "href": "slides/02-binomial-glm.html#deviance-lrt",
    "title": "Binomial GLM example",
    "section": "Deviance, LRT",
    "text": "Deviance, LRT"
  },
  {
    "objectID": "slides/02-binomial-glm.html#r2",
    "href": "slides/02-binomial-glm.html#r2",
    "title": "Binomial GLM example",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\nThe \\(R^2\\) cannot be computed as in standard linear regression. There are different types of pseudo-\\(R^2\\) for example:\n\nLikelihood ratio \\(R^2_L\\)\nCox and Snell \\(R^2_{CS}\\)\nNagelkerke \\(R^2_N\\)\nMcFadden \\(R^2_{McF}\\)\nTjur \\(R^2_T\\)\n\nAll these methods are based on the deviance and/or the likelihood of current/null/saturated models.\n\n\nSee https://en.wikipedia.org/wiki/Pseudo-R-squared for a nice overview."
  },
  {
    "objectID": "slides/02-binomial-glm.html#r2-1",
    "href": "slides/02-binomial-glm.html#r2-1",
    "title": "Binomial GLM example",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\nThe performance R package (used to plot diagnostics and other modelling-related metrics) implements most of the pseudo-\\(R^2\\) values. The default for binomial models is the method proposed by Tjur (2009).\n\n# performance::r2_tjur()\nperformance::r2(fit_emo_x_int2) \n## # R2 for Logistic Regression\n##   Tjur's R2: 0.578\nperformance::r2_coxsnell(fit_emo_x_int2)\n## Cox & Snell's R2 \n##        0.5030847\nperformance::r2_mcfadden(fit_emo_x_int2)\n## # R2 for Generalized Linear Regression\n##        R2: 0.545\n##   adj. R2: 0.530\nperformance::r2_nagelkerke(fit_emo_x_int2)\n## Nagelkerke's R2 \n##       0.6962745"
  },
  {
    "objectID": "slides/02-binomial-glm.html#residuals",
    "href": "slides/02-binomial-glm.html#residuals",
    "title": "Binomial GLM example",
    "section": "Residuals",
    "text": "Residuals\nDiagnostics in GLMs is more complex than in standard linear models. The main reason is that residuals are more complex due to the link function. For example these are the residuals of the last model we fitted."
  },
  {
    "objectID": "slides/02-binomial-glm.html#residuals-1",
    "href": "slides/02-binomial-glm.html#residuals-1",
    "title": "Binomial GLM example",
    "section": "Residuals",
    "text": "Residuals\nThere are few problems with residuals in GLM:\n\nMean and variance are linked. This means that as the mean increase also the variance increase violating the standard homoschedasticity assumption. This mainly happens with standard raw residuals and in GLM we need to use other residuals (e.g., Pearson, Deviance, etc.)\nResiduals (even the Pearson or Deviance) are problematic for discrete GLM (such as Binomial or Poisson), see the plot in the previous slide.\nResiduals for non-normal distributions are not expected to be normally distributed even when the model is well specified.\nThere are no standard and universal way to assess the residuals pattern."
  },
  {
    "objectID": "slides/02-binomial-glm.html#residuals-a-proposal",
    "href": "slides/02-binomial-glm.html#residuals-a-proposal",
    "title": "Binomial GLM example",
    "section": "Residuals, a proposal",
    "text": "Residuals, a proposal\n\nThe DHARMa package uses a simulation-based approach to create readily interpretable scaled (quantile) residuals for fitted generalized linear (mixed) models\n\nThese residuals seems quite promising as an unified framework but I haven’t systematically explored this possibility.\nThe DHARMa package has a nice documentation explaining the idea and how to use the quantile residuals (see also Dunn & Smyth, 1996, 2018)."
  },
  {
    "objectID": "slides/02-binomial-glm.html#residuals-a-proposal-1",
    "href": "slides/02-binomial-glm.html#residuals-a-proposal-1",
    "title": "Binomial GLM example",
    "section": "Residuals, a proposal",
    "text": "Residuals, a proposal\n\nThe resulting residuals are standardized to values between 0 and 1 and can be interpreted as intuitively as residuals from a linear regression.\n\n\nlibrary(DHARMa)\nplot(simulateResiduals(fit_emo_x_int2))"
  },
  {
    "objectID": "slides/02-binomial-glm.html#residuals-a-proposal-1-output",
    "href": "slides/02-binomial-glm.html#residuals-a-proposal-1-output",
    "title": "Binomial GLM example",
    "section": "Residuals, a proposal",
    "text": "Residuals, a proposal"
  },
  {
    "objectID": "slides/02-binomial-glm.html#why-simulating-data",
    "href": "slides/02-binomial-glm.html#why-simulating-data",
    "title": "Binomial GLM example",
    "section": "Why simulating data?",
    "text": "Why simulating data?\nIf you plan to use a GLM and you want to compute some inferential properties (statistical power, type-1 error, etc.) there are usually no analytical (i.e., formula-based) methods to solve the problem.\nThe only way to do a power analysis with a logistic regression is to simulate data and re-fit the model multiple times to see the long-run behaviour.\nMCS are also useful to understand more deeply a certain statistical model or procedure."
  },
  {
    "objectID": "slides/02-binomial-glm.html#general-mcs-workflow",
    "href": "slides/02-binomial-glm.html#general-mcs-workflow",
    "title": "Binomial GLM example",
    "section": "General MCS workflow",
    "text": "General MCS workflow\n\nDefine the Data Generation Process (DGP)\nDefine the sample size, number of trials, conditions, etc.\nSimulate data using random number generations and the DGP\nFit the target model\nRepeat 3-4 a large number of times maybe with different features (e.g., different sample size) defined in 2\nSummarise the simulation results. For example, counting the number of times the p value is significant (i.e., estimating the statistical power)\n\n\n\nWe have also a longer document about implementing a MCS in R."
  },
  {
    "objectID": "slides/02-binomial-glm.html#data-generation-process-dgp",
    "href": "slides/02-binomial-glm.html#data-generation-process-dgp",
    "title": "Binomial GLM example",
    "section": "1. Data Generation Process (DGP)",
    "text": "1. Data Generation Process (DGP)\nLet’s try estimating the statistical power for the intensity effect in our example.\nWe will simulate data according to a Binomial GLM with a logit link function.\n\nbinomial(link = \"logit\")\nqlogis() # link function\nplogis() # inverse link function"
  },
  {
    "objectID": "slides/02-binomial-glm.html#experiment-features",
    "href": "slides/02-binomial-glm.html#experiment-features",
    "title": "Binomial GLM example",
    "section": "2. Experiment features",
    "text": "2. Experiment features\nWe will simulate a single subject doing \\(n\\) trials. The main predictor is intensity ranging from 10% to 100% in steps of 10%.\n\n(intensity &lt;- seq(10, 100, 10))\n\n [1]  10  20  30  40  50  60  70  80  90 100"
  },
  {
    "objectID": "slides/02-binomial-glm.html#random-number-generation",
    "href": "slides/02-binomial-glm.html#random-number-generation",
    "title": "Binomial GLM example",
    "section": "3. Random number generation",
    "text": "3. Random number generation\nIn R, to simulate data you can use the r* function. Each implemented distribution in R has the associated r* function (as the p* and q* function we used before).\n\n# simulate 10 numbers from a gaussian distribution\n# with mu = 10 and sigma = 5\nrnorm(n = 10, mean = 10, sd = 5)\n\n [1]  4.109284  6.797811  4.816875 11.341073 13.263932  8.876881 12.433004\n [8]  2.652825 -4.010016  7.900317"
  },
  {
    "objectID": "slides/02-binomial-glm.html#random-number-generation-1",
    "href": "slides/02-binomial-glm.html#random-number-generation-1",
    "title": "Binomial GLM example",
    "section": "3. Random number generation",
    "text": "3. Random number generation\nFor the Binomial we have rbinom:\n\n# n = number of rows in our case\n# size = number of trials\n# p = probability of success in each trial\nrbinom(n = 1, size = 1, prob = 0.5)\n\n[1] 0\n\nrbinom(n = 5, size = 1, prob = 0.5)\n\n[1] 1 0 0 1 0\n\nrbinom(n = 5, size = 10, prob = 0.5)\n\n[1] 4 4 4 6 4"
  },
  {
    "objectID": "slides/02-binomial-glm.html#random-number-generation-2",
    "href": "slides/02-binomial-glm.html#random-number-generation-2",
    "title": "Binomial GLM example",
    "section": "3. Random number generation",
    "text": "3. Random number generation\nNotice that rbinom is not really intutive because it works both on the binary and the binomial form.\nThis means to generate \\(k\\) bernoulli trials with results 0 or 1\n\nrbinom(10, 1, 0.5)\n\n [1] 0 1 1 1 0 0 1 0 1 1\n\n\nThis is the same but aggregating:\n\n# the result is the number of successes over 10 \n# bernoulli trials\nrbinom(1, 10, 0.5)\n\n[1] 5\n\n\nDepending if you want to work with 0-1 values or number of successes/failures (aggregated) you should use one of the two strategies."
  },
  {
    "objectID": "slides/02-binomial-glm.html#random-number-generation-3",
    "href": "slides/02-binomial-glm.html#random-number-generation-3",
    "title": "Binomial GLM example",
    "section": "3. Random number generation",
    "text": "3. Random number generation\nThe crucial part of rbinom is that the prob = argument is vectorized. This means that if you simulate \\(n\\) trials you can provide \\(n\\) probabilities.\n\n# 20 different probabilities\n(p &lt;- seq(0.1, 0.9, length.out = 20))\n\n [1] 0.1000000 0.1421053 0.1842105 0.2263158 0.2684211 0.3105263 0.3526316\n [8] 0.3947368 0.4368421 0.4789474 0.5210526 0.5631579 0.6052632 0.6473684\n[15] 0.6894737 0.7315789 0.7736842 0.8157895 0.8578947 0.9000000\n\n\nThis means that for the last trials, the probability of success (1) is larger.\n\nrbinom(n = 20, size = 1, prob = p)\n\n [1] 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1"
  },
  {
    "objectID": "slides/02-binomial-glm.html#dataset",
    "href": "slides/02-binomial-glm.html#dataset",
    "title": "Binomial GLM example",
    "section": "Dataset",
    "text": "Dataset\nWe can start with the deterministic part of the simulation, the experiment.\nWe simulate an experiment with nt trials with random intensity values from 10 to 100. We could also create a balanced version.\n\nnt &lt;- 100 # number of trials\n\ndat &lt;- data.frame(\n  intensity = sample(intensity, nt, replace = TRUE)\n)\n\nhead(dat) # first 6 rows\n\n  intensity\n1        50\n2        20\n3        40\n4        80\n5        50\n6       100"
  },
  {
    "objectID": "slides/02-binomial-glm.html#systematic-component-and-random-component",
    "href": "slides/02-binomial-glm.html#systematic-component-and-random-component",
    "title": "Binomial GLM example",
    "section": "Systematic component and random component",
    "text": "Systematic component and random component\nThe model can be formalized as:\n\\[\np_i = g^{-1}(\\beta_0 + \\beta_1 \\times \\mbox{intensity}_i)\n\\]\n\\[\ny_i \\sim \\mathrm{Bernoulli}(p_i) \\qquad y_i \\in \\{0,1\\}\n\\]\nWhere \\(g^{-1}\\) is the inverse logit function (qlogis). This can be reas as, for each trial \\(i\\) the true probability of success is a function of the intensity-\\(i\\). The 0-1 outcome comes from a Bernoulli distribution with probability of success \\(p_i\\). This means that according to \\(\\beta_1\\) different intensity values will have a different probability of success.\nFinally \\(\\beta_0 + \\beta_1 \\times \\mbox{intensity}_i\\) (i.e., the linear predictor \\(\\eta_i\\)) need to be simulated in logit scale, then converted back into probabilities."
  },
  {
    "objectID": "slides/02-binomial-glm.html#systematic-component-and-random-component-1",
    "href": "slides/02-binomial-glm.html#systematic-component-and-random-component-1",
    "title": "Binomial GLM example",
    "section": "Systematic component and random component",
    "text": "Systematic component and random component\n\\(\\beta_0\\) is the (logit) probability of success for intensity 10%, centering the intensity on the minimum. \\(\\beta_1\\) is the increase in logit for a unit increase in intensity. Let’s rescale intensity to be between 0 (10%) and 10 (100%).\n\n\nCode\nb0 &lt;- qlogis(0.1) # logit of success when intensity = 10%\nb1 &lt;- 1\nintensity10 &lt;- (intensity - 10) / 10\neta &lt;- b0 + b1 * intensity10\n\ndata.frame(\n  intensity,\n  eta\n) |&gt; \n  ggplot(aes(x = intensity, y = plogis(eta))) +\n  geom_point(size = 5) +\n  geom_line() +\n  xlab(\"Intensity\") +\n  ylab(\"Accuracy\") +\n  ggtitle(latex2exp::TeX(\"$\\\\beta_1 = 1$\"))"
  },
  {
    "objectID": "slides/02-binomial-glm.html#systematic-component-and-random-component-1-output",
    "href": "slides/02-binomial-glm.html#systematic-component-and-random-component-1-output",
    "title": "Binomial GLM example",
    "section": "Systematic component and random component",
    "text": "Systematic component and random component"
  },
  {
    "objectID": "slides/02-binomial-glm.html#systematic-component-and-random-component-2",
    "href": "slides/02-binomial-glm.html#systematic-component-and-random-component-2",
    "title": "Binomial GLM example",
    "section": "Systematic component and random component",
    "text": "Systematic component and random component\nYou can choose different \\(\\beta_1\\) values according to your hypothesis."
  },
  {
    "objectID": "slides/02-binomial-glm.html#systematic-component-and-random-component-3",
    "href": "slides/02-binomial-glm.html#systematic-component-and-random-component-3",
    "title": "Binomial GLM example",
    "section": "Systematic component and random component",
    "text": "Systematic component and random component\nNow we can simply apply the same functions to the full dataset. Let’s stick with \\(\\beta_1 = 0.5\\) for the moment.\n\nb0 &lt;- qlogis(0.1)\nb1 &lt;- 1\ndat$intensity10 &lt;- with(dat, (intensity - 10)/10)\ndat$eta &lt;- with(dat, b0 + b1 * intensity10) # link function space\ndat$p &lt;- plogis(dat$eta) # inverse link (probability)\nhead(dat)\n\n  intensity intensity10        eta         p\n1        50           4  1.8027754 0.8584864\n2        20           1 -1.1972246 0.2319693\n3        40           3  0.8027754 0.6905679\n4        80           7  4.8027754 0.9918599\n5        50           4  1.8027754 0.8584864\n6       100           9  6.8027754 0.9988905"
  },
  {
    "objectID": "slides/02-binomial-glm.html#simulate-the-0-1-response",
    "href": "slides/02-binomial-glm.html#simulate-the-0-1-response",
    "title": "Binomial GLM example",
    "section": "Simulate the 0-1 response",
    "text": "Simulate the 0-1 response\nFinally we need to simulate the 0-1 response for each trial/observation:\n\ndat$acc &lt;- rbinom(nrow(dat), 1, dat$p)\nhead(dat)\n\n  intensity intensity10        eta         p acc\n1        50           4  1.8027754 0.8584864   1\n2        20           1 -1.1972246 0.2319693   0\n3        40           3  0.8027754 0.6905679   0\n4        80           7  4.8027754 0.9918599   1\n5        50           4  1.8027754 0.8584864   1\n6       100           9  6.8027754 0.9988905   1\n\n\nFor each row/trial, p is the true accuracy according to intensity and acc is the actual simulated response."
  },
  {
    "objectID": "slides/02-binomial-glm.html#simulate-the-0-1-response-1",
    "href": "slides/02-binomial-glm.html#simulate-the-0-1-response-1",
    "title": "Binomial GLM example",
    "section": "Simulate the 0-1 response",
    "text": "Simulate the 0-1 response\nAlways plot your simulated data to check the results:"
  },
  {
    "objectID": "slides/02-binomial-glm.html#fit-the-target-model",
    "href": "slides/02-binomial-glm.html#fit-the-target-model",
    "title": "Binomial GLM example",
    "section": "4. Fit the target model",
    "text": "4. Fit the target model\nNow we can fit the desired model:\n\nfit &lt;- glm(acc ~ intensity10, data = dat, family = binomial(link = \"logit\"))\nsummary(fit)\n\n\nCall:\nglm(formula = acc ~ intensity10, family = binomial(link = \"logit\"), \n    data = dat)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -2.2313     0.6614  -3.373 0.000743 ***\nintensity10   1.1605     0.2499   4.644 3.42e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 110.216  on 99  degrees of freedom\nResidual deviance:  55.583  on 98  degrees of freedom\nAIC: 59.583\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "slides/02-binomial-glm.html#repeat-the-process",
    "href": "slides/02-binomial-glm.html#repeat-the-process",
    "title": "Binomial GLM example",
    "section": "5. Repeat the process",
    "text": "5. Repeat the process\nNow the core of MCS. By repeating the data generation and model fitting we are simulating the randomness that could happen in real data collection. Better wrapping everything into a function:\n\nsim_data &lt;- function(nt, b0, b1){\n  dat &lt;- data.frame(\n    intensity = sample(seq(10, 100, 10), nt, replace = TRUE)\n  )\n  dat$intensity10 &lt;- with(dat, (intensity - 10)/10)\n  dat$eta &lt;- with(dat, b0 + b1 * intensity10)\n  dat$p &lt;- plogis(dat$eta)\n  dat$acc &lt;- rbinom(nrow(dat), 1, dat$p)\n  return(dat)\n}\n\nfit_model &lt;- function(data){\n  glm(acc ~ intensity10, data = data, family = binomial(link = \"logit\"))\n}\n\nsummary_model &lt;- function(fit){\n  # keep only intensity coefficients\n  data.frame(summary(fit)$coefficients)[2, ]\n}"
  },
  {
    "objectID": "slides/02-binomial-glm.html#repeat-the-process-1",
    "href": "slides/02-binomial-glm.html#repeat-the-process-1",
    "title": "Binomial GLM example",
    "section": "5. Repeat the process",
    "text": "5. Repeat the process\nAnd a overall simulation function:\n\ndo_sim &lt;- function(nsim, nt, b0, b1){\n  res &lt;- replicate(nsim, {\n    dat &lt;- sim_data(nt, b0, b1)\n    fit &lt;- fit_model(dat)\n    summary_model(fit)\n  }, simplify = FALSE)\n  res &lt;- do.call(rbind, res)\n  rownames(res) &lt;- NULL\n  names(res) &lt;- c(\"b\", \"se\", \"z\", \"p\")\n  return(res)\n}\n\ndo_sim(5, 100, b0, b1)\n\n          b        se        z            p\n1 1.2505538 0.2783072 4.493429 7.008524e-06\n2 0.9635769 0.2147389 4.487203 7.216420e-06\n3 0.8768182 0.1863558 4.705077 2.537703e-06\n4 1.1696200 0.2385154 4.903750 9.402401e-07\n5 1.4935695 0.3632573 4.111602 3.929229e-05"
  },
  {
    "objectID": "slides/02-binomial-glm.html#repeat-the-process-2",
    "href": "slides/02-binomial-glm.html#repeat-the-process-2",
    "title": "Binomial GLM example",
    "section": "5. Repeat the process",
    "text": "5. Repeat the process\nnsim is the number of simulations. Usually larger is better considering computational constraints. If feasible, 5000 or 10000 is usually more than enough. For complex models where 5000 or 10000 is not an option, try never go below 1000. See Burton et al. (2006) and Koehler et al. (2009) for discussion about the number of simulations.\n\ntictoc::tic()\nsim &lt;- do_sim(1000, nt = 50, b0 = qlogis(0.1), b1 = 0.5)\ntictoc::toc()\n\n1.012 sec elapsed\n\n\nIn this case we are lucky, running 1000 simulations only takes ~ 1.5 seconds. Sometimes can also takes hours, days or weeks."
  },
  {
    "objectID": "slides/02-binomial-glm.html#results",
    "href": "slides/02-binomial-glm.html#results",
    "title": "Binomial GLM example",
    "section": "6. Results",
    "text": "6. Results\nEach row is the parameter estimated from a simulated dataset. The statistical power is the % of p values lower than \\(\\alpha\\) of the total number of simulations:\n\nhead(sim)\n\n          b        se        z            p\n1 0.5169572 0.1516280 3.409379 0.0006511095\n2 0.5833948 0.1776386 3.284167 0.0010228412\n3 0.4248221 0.1329617 3.195072 0.0013979615\n4 0.3061667 0.1173110 2.609872 0.0090576127\n5 0.7326639 0.1904946 3.846114 0.0001200058\n6 0.4789775 0.1500721 3.191648 0.0014146338\n\nsum(sim$p &lt;= 0.05) / 1000\n\n[1] 0.991\n\n# or mean(sum(sim$p &lt;= 0.05))\n\nThe power is pretty high with 50 trials. Let’s try a lower effect with different number of trials."
  },
  {
    "objectID": "slides/02-binomial-glm.html#more-conditions",
    "href": "slides/02-binomial-glm.html#more-conditions",
    "title": "Binomial GLM example",
    "section": "More conditions",
    "text": "More conditions\nWe can try different number of trials. For each n we simulate 1000 datasets, fit the models, extract the p values and estimate the statistical power.\n\nn &lt;- c(10, 50, 80, 100, 150, 200)\npower &lt;- rep(NA, length(n))\n\nfor(i in 1:length(power)){\n  res &lt;- do_sim(1000, n[i], b0 = qlogis(0.1), b1 = 0.2)\n  power[i] &lt;- mean(res$p &lt;= 0.05)\n}\n\npower\n\n[1] 0.000 0.338 0.521 0.610 0.818 0.904"
  },
  {
    "objectID": "slides/02-binomial-glm.html#more-conditions-1",
    "href": "slides/02-binomial-glm.html#more-conditions-1",
    "title": "Binomial GLM example",
    "section": "More conditions",
    "text": "More conditions"
  },
  {
    "objectID": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-1",
    "href": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-1",
    "title": "Binomial GLM example",
    "section": "Latent formulation of the binomial model",
    "text": "Latent formulation of the binomial model\nTo clarify a little bit the terminology the logit function (link function \\(g(\\cdot)\\)) is:\n\\[\nq = \\log{\\frac{p}{1 - p}}\n\\]\nThe inverse of the logit is called logistic (inverse link function \\(g^{-1}(\\cdot)\\)) function:\n\\[\np = \\frac{e^p}{1 + e^p}\n\\]"
  },
  {
    "objectID": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-2",
    "href": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-2",
    "title": "Binomial GLM example",
    "section": "Latent formulation of the binomial model",
    "text": "Latent formulation of the binomial model"
  },
  {
    "objectID": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-3",
    "href": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-3",
    "title": "Binomial GLM example",
    "section": "Latent formulation of the binomial model",
    "text": "Latent formulation of the binomial model\n\nInstead of thinking about a binary variable, you can think about e.g. accuracy as a continous measure from \\(-\\infty\\) to \\(+\\infty\\).\nThen you can imagine to cut this continous measure using a threshold. Everthing above the threshold takes the value 1 otherwise 0.\nUsing the logit function we are assuming that this underlying latent variable is a standard logistic distribution.\nThe standard logistic distribution is a continous variable (similar to the Gaussian) with mean \\(\\mu = 0\\) and \\(\\sigma^2 = \\frac{s^2 \\pi^2}{3}\\). Given that \\(s^2 = 1\\) in the standard logistic distribution the variance is \\(\\frac{\\pi^2}{3}\\).\n\n\n\nhttps://en.wikipedia.org/wiki/Logistic_distribution"
  },
  {
    "objectID": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-4",
    "href": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-4",
    "title": "Binomial GLM example",
    "section": "Latent formulation of the binomial model",
    "text": "Latent formulation of the binomial model\nYou can draw the logistic distribution using dlogis(). The comulative distribution function plogis() is the logistic function and the quantile function qlogis() is the logit function."
  },
  {
    "objectID": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-5",
    "href": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-5",
    "title": "Binomial GLM example",
    "section": "Latent formulation of the binomial model",
    "text": "Latent formulation of the binomial model\nIn this framework, saying that the logit in one condition is 0 means that the probability is 0.5 thus 50% of the observations are expected to have a value of 1 and 50% a value of 0.\nA shift in the latent distribution represents also a shift in the proportions of 1 and 0. Regression coefficients (in logit) represent the shift in the latent distribution and odds ratios are the shift in the odds/proportions."
  },
  {
    "objectID": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-6",
    "href": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-6",
    "title": "Binomial GLM example",
    "section": "Latent formulation of the binomial model",
    "text": "Latent formulation of the binomial model"
  },
  {
    "objectID": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-7",
    "href": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-7",
    "title": "Binomial GLM example",
    "section": "Latent formulation of the binomial model",
    "text": "Latent formulation of the binomial model\nIn other terms, the logistic regression can be expressed as a linear model predicting shifts in the mean of the logistic distribution as a functions of predictors as in standard linear models assuming normality.\nThe parameters \\(\\beta\\) are shifts in the underlying latent distribution. \\(\\beta = 1\\) is like saying that the difference between the groups is 1 standard deviation on the latent scale."
  },
  {
    "objectID": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-8",
    "href": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-8",
    "title": "Binomial GLM example",
    "section": "Latent formulation of the binomial model",
    "text": "Latent formulation of the binomial model\nPractically this means that we can write the same model in the latent form:\n\\[\nz_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\]\n\\[\n\\epsilon_i \\sim \\mbox{Logistic}(0, 1)\n\\]\nWhere \\(z_i\\) is the latent value (e.g., logit). Errors are distributed according to a standard logistic distribution.\nThen the observed 0-1 values:\n\\[\n\\begin{cases}\n1 & \\text{if } z_i &gt; 0, \\\\\n0 & \\text{if } z_i \\le 0 .\n\\end{cases}\n\\]"
  },
  {
    "objectID": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-9",
    "href": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-9",
    "title": "Binomial GLM example",
    "section": "Latent formulation of the binomial model",
    "text": "Latent formulation of the binomial model\nIn R we can show this very easily. We can simulate a latent shift and the corresponding pattern in probabilities using the standard and latent form of the logistic regression.\n\np1 &lt;- 0.5 # probability of group 1\np2 &lt;- 0.8 # probability of group 2\n(d &lt;- qlogis(p2) - qlogis(p1)) # latent shift, log odds ratio\n\n[1] 1.386294\n\n(OR &lt;- exp(d)) # odds ratio\n\n[1] 4"
  },
  {
    "objectID": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-10",
    "href": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-10",
    "title": "Binomial GLM example",
    "section": "Latent formulation of the binomial model",
    "text": "Latent formulation of the binomial model\n\n# number of observations, large to show the pattern\nn &lt;- 10000\n\n# using rbinom\nx &lt;- rep(0:1, each = n/2) # condition 1 and condition 2\np &lt;- plogis(qlogis(p1) + d * x)\nyb &lt;- rbinom(n, 1, p)\n\n# latent form\nz &lt;- qlogis(p1) + x * d + rlogis(n, 0, 1)\nyl &lt;- ifelse(z &gt; 0, 1, 0)\n\ntapply(yb, x, mean)\n\n     0      1 \n0.5186 0.7990 \n\ntapply(yl, x, mean)\n\n     0      1 \n0.5046 0.7996"
  },
  {
    "objectID": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-11",
    "href": "slides/02-binomial-glm.html#latent-formulation-of-the-binomial-model-11",
    "title": "Binomial GLM example",
    "section": "Latent formulation of the binomial model",
    "text": "Latent formulation of the binomial model\nThe beauty of the latent intepretation is that if you change the error part from a logistic to a gaussian distribution, you obtain the probit model.\nIn the probit the idea is the same but shifts are in units of a standard normal distribution that can be intepreted as Cohen’s \\(d\\)."
  },
  {
    "objectID": "slides/02-binomial-glm.html#probit-link-function-1",
    "href": "slides/02-binomial-glm.html#probit-link-function-1",
    "title": "Binomial GLM example",
    "section": "Probit link function",
    "text": "Probit link function\nSometimes, the probit link function can be used. The main difference is that when using the logit link function the underlying distribution is called logistic. When using the probit link function the underlying distribution is Gaussian."
  },
  {
    "objectID": "slides/02-binomial-glm.html#probit-link-function-2",
    "href": "slides/02-binomial-glm.html#probit-link-function-2",
    "title": "Binomial GLM example",
    "section": "Probit link function",
    "text": "Probit link function\nWhen using the probit link the parameters are interpreted as difference in z-scores associated with a unit increase in the predictors. In fact probabilities are mapped into z-scores using the cumulative normal distribution.\n\np1 &lt;- 0.8\np2 &lt;- 0.6\n\nqlogis(c(p1, p2)) # log(odds(p1)), logit link\n\n[1] 1.3862944 0.4054651\n\nqnorm(c(p1, p2)) # probit link\n\n[1] 0.8416212 0.2533471\n\nlog(or(p1, p2)) # ~ beta1, logit link\n\n[1] 0.9808293\n\nqnorm(p1) - qnorm(p2) # ~beta1, probit link\n\n[1] 0.5882741"
  },
  {
    "objectID": "slides/02-binomial-glm.html#probit-link-function-3",
    "href": "slides/02-binomial-glm.html#probit-link-function-3",
    "title": "Binomial GLM example",
    "section": "Probit link function",
    "text": "Probit link function"
  },
  {
    "objectID": "slides/02-binomial-glm.html#why-using-the-probit-link-function",
    "href": "slides/02-binomial-glm.html#why-using-the-probit-link-function",
    "title": "Binomial GLM example",
    "section": "Why using the probit link function?",
    "text": "Why using the probit link function?\nThe probit can be a really useful choice:\n\nA binomial GLM with a probit link can be used to estimate Signal Detection Theory (\\(d\\)’ and criterion) parameters (see DeCarlo, 1998, 2010).\nThe parameters are intepreted as differences in standard deviations similar to Cohen’s \\(d\\). This is also very useful in simulations."
  },
  {
    "objectID": "slides/02-binomial-glm.html#odds-ratio-or-to-cohens-d-1",
    "href": "slides/02-binomial-glm.html#odds-ratio-or-to-cohens-d-1",
    "title": "Binomial GLM example",
    "section": "Odds Ratio (OR) to Cohen’s \\(d\\)",
    "text": "Odds Ratio (OR) to Cohen’s \\(d\\)\nThe Odds Ratio can be considered an effect size measure. We can transform the OR into other effect size measure (Borenstein et al., 2009; Sánchez-Meca et al., 2003).\n\\[\nd = \\log(OR) \\frac{\\sqrt{3}}{\\pi}\n\\]\n\n# in R with the effectsize package\nor &lt;- 1.5\neffectsize::logoddsratio_to_d(log(or))\n\n[1] 0.2235446\n\n# or \neffectsize::oddsratio_to_d(or)\n\n[1] 0.2235446"
  },
  {
    "objectID": "slides/02-binomial-glm.html#odds-ratio-or-to-cohens-d-2",
    "href": "slides/02-binomial-glm.html#odds-ratio-or-to-cohens-d-2",
    "title": "Binomial GLM example",
    "section": "Odds Ratio (OR) to Cohen’s \\(d\\)",
    "text": "Odds Ratio (OR) to Cohen’s \\(d\\)"
  },
  {
    "objectID": "slides/02-binomial-glm.html#binary-vs-binomial-data-structure-1",
    "href": "slides/02-binomial-glm.html#binary-vs-binomial-data-structure-1",
    "title": "Binomial GLM example",
    "section": "Binary vs binomial data structure",
    "text": "Binary vs binomial data structure\nIn our facial expression example we used the binary data structure. This means that the vector of the response variable contains 0 and 1.\n\nhead(dat)\n\n# A tibble: 6 × 6\n     id age   intensity emotion_lbl response_lbl   acc\n  &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;        &lt;int&gt;\n1    22 53           60 fear        suprise          0\n2    22 53           60 disgust     disgust          1\n3    22 53           70 happiness   happiness        1\n4    22 53          100 happiness   happiness        1\n5    22 53           60 disgust     sadness          0\n6    22 53           20 fear        neutral          0"
  },
  {
    "objectID": "slides/02-binomial-glm.html#binary-vs-binomial-data-structure-2",
    "href": "slides/02-binomial-glm.html#binary-vs-binomial-data-structure-2",
    "title": "Binomial GLM example",
    "section": "Binary vs binomial data structure",
    "text": "Binary vs binomial data structure\nThe same dataset can be also used in the so-called binomial format. In this case we can aggregate emotion_lbl and intensity counting the number of correct responses.\n\ndat_binomial &lt;- dat |&gt; \n  group_by(intensity) |&gt; \n  summarise(nt = n(),      # total number of trials\n            nc = sum(acc), # number of correct responses\n            nf = nt - nc,  # number of fails\n            acc = nc / nt) # proportion of correct response"
  },
  {
    "objectID": "slides/02-binomial-glm.html#binary-vs-binomial-data-structure-3",
    "href": "slides/02-binomial-glm.html#binary-vs-binomial-data-structure-3",
    "title": "Binomial GLM example",
    "section": "Binary vs binomial data structure",
    "text": "Binary vs binomial data structure\n\nhead(dat_binomial)\n\n# A tibble: 6 × 5\n  intensity    nt    nc    nf    acc\n      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n1        10    37     3    34 0.0811\n2        20    37     3    34 0.0811\n3        30    37    12    25 0.324 \n4        40    37    18    19 0.486 \n5        50    37    23    14 0.622 \n6        60    37    23    14 0.622"
  },
  {
    "objectID": "slides/02-binomial-glm.html#binary-vs-binomial-data-structure-4",
    "href": "slides/02-binomial-glm.html#binary-vs-binomial-data-structure-4",
    "title": "Binomial GLM example",
    "section": "Binary vs binomial data structure",
    "text": "Binary vs binomial data structure\nThe dataset contains exactly the same information, we are not losing anything by aggregating. We can check it by fitting two models. Notice the way of writing the model in the binomial way:\n\nfit_binary   &lt;- glm(acc ~ intensity, data = dat, family = binomial(link = \"logit\"))\nfit_binomial &lt;- glm(cbind(nc, nf) ~ intensity, data = dat_binomial, family = binomial(link = \"logit\"))"
  },
  {
    "objectID": "slides/02-binomial-glm.html#binary-vs-binomial-data-structure-5",
    "href": "slides/02-binomial-glm.html#binary-vs-binomial-data-structure-5",
    "title": "Binomial GLM example",
    "section": "Binary vs binomial data structure",
    "text": "Binary vs binomial data structure\n\ncar::compareCoefs(fit_binary, fit_binomial)\n\nCalls:\n1: glm(formula = acc ~ intensity, family = binomial(link = \"logit\"), data = \n  dat)\n2: glm(formula = cbind(nc, nf) ~ intensity, family = binomial(link = \n  \"logit\"), data = dat_binomial)\n\n            Model 1 Model 2\n(Intercept)  -1.798  -1.798\nSE            0.264   0.264\n                           \nintensity   0.03198 0.03198\nSE          0.00429 0.00429"
  },
  {
    "objectID": "slides/02-binomial-glm.html#binary-vs-binomial-data-structure-6",
    "href": "slides/02-binomial-glm.html#binary-vs-binomial-data-structure-6",
    "title": "Binomial GLM example",
    "section": "Binary vs binomial data structure",
    "text": "Binary vs binomial data structure\nA third option is also modelling directly the proportions providing the weigths = argument as the number of trials.\n\nfit_prop &lt;- glm(acc ~ intensity, data = dat_binomial, weights = nt, family = binomial(link = \"logit\")) \ncar::compareCoefs(fit_binary, fit_binomial, fit_prop)\n\nCalls:\n1: glm(formula = acc ~ intensity, family = binomial(link = \"logit\"), data = \n  dat)\n2: glm(formula = cbind(nc, nf) ~ intensity, family = binomial(link = \n  \"logit\"), data = dat_binomial)\n3: glm(formula = acc ~ intensity, family = binomial(link = \"logit\"), data = \n  dat_binomial, weights = nt)\n\n            Model 1 Model 2 Model 3\n(Intercept)  -1.798  -1.798  -1.798\nSE            0.264   0.264   0.264\n                                   \nintensity   0.03198 0.03198 0.03198\nSE          0.00429 0.00429 0.00429\n                                   \n\n\nAgain, exactly the same."
  },
  {
    "objectID": "slides/02-binomial-glm.html#binary-vs-binomial-data-structure-7",
    "href": "slides/02-binomial-glm.html#binary-vs-binomial-data-structure-7",
    "title": "Binomial GLM example",
    "section": "Binary vs binomial data structure",
    "text": "Binary vs binomial data structure\nWhy using the binary vs the binomial format? Depends on the type of predictors!\nIn our (and similar) case, “aggregating” with categorical predictor is possible without losing information. The advantage is that models in the binomial format are very fast (especially for complex models such as mixed-effects models). Also the diagnostics in terms of residuals are better (see Gelman et al., 2020, p. 253)\nIf you have predictors at the 0-1 level you cannot aggregate without losing information. In our case, imagine to have for each trial the 0-1 accuracy and the reaction time. You need to use the binary format otherwise you have to bin the reaction time variable (losing information)."
  },
  {
    "objectID": "slides/02-binomial-glm.html#references",
    "href": "slides/02-binomial-glm.html#references",
    "title": "Binomial GLM example",
    "section": "References",
    "text": "References\n\n\n\n\nArel-Bundock, V., Greifer, N., & Heiss, A. (2024). How to Interpret Statistical Models Using marginaleffects for R and Python. Journal of Statistical Software, 111, 1–32. https://doi.org/10.18637/jss.v111.i09\n\n\nBorenstein, M., Hedges, L. V., Higgins, J. P. T., & Rothstein, H. R. (2009). Introduction to Meta-Analysis. https://doi.org/10.1002/9780470743386\n\n\nBurton, A., Altman, D. G., Royston, P., & Holder, R. L. (2006). The design of simulation studies in medical statistics. Statistics in Medicine, 25, 4279–4292. https://doi.org/10.1002/sim.2673\n\n\nDeCarlo, L. T. (1998). Signal detection theory and generalized linear models. Psychological Methods, 3, 186–205. https://doi.org/10.1037/1082-989X.3.2.186\n\n\nDeCarlo, L. T. (2010). On the statistical and theoretical basis of signal detection theory and extensions: Unequal variance, random coefficient, and mixture models. Journal of Mathematical Psychology, 54, 304–313. https://doi.org/10.1016/j.jmp.2010.01.001\n\n\nDunn, P. K., & Smyth, G. K. (1996). Randomized Quantile Residuals. Journal of Computational and Graphical Statistics: A Joint Publication of American Statistical Association, Institute of Mathematical Statistics, Interface Foundation of North America, 5, 236–244. https://doi.org/10.1080/10618600.1996.10474708\n\n\nDunn, P. K., & Smyth, G. K. (2018). Generalized Linear Models With Examples in R. Springer.\n\n\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and Other Stories. Cambridge University Press. https://doi.org/10.1017/9781139161879\n\n\nGranziol, U., Rabe, M., Gallucci, M., Spoto, A., & Vidotto, G. (2025). Not another post hoc paper: A new look at contrast analysis and planned comparisons. Advances in Methods and Practices in Psychological Science, 8. https://doi.org/10.1177/25152459241293110\n\n\nKoehler, E., Brown, E., & Haneuse, S. J.-P. A. (2009). On the assessment of Monte Carlo error in simulation-based statistical analyses. The American Statistician, 63, 155–162. https://doi.org/10.1198/tast.2009.0030\n\n\nSánchez-Meca, J., Marín-Martínez, F., & Chacón-Moscoso, S. (2003). Effect-size indices for dichotomized outcomes in meta-analysis. Psychological Methods, 8, 448–467. https://doi.org/10.1037/1082-989X.8.4.448\n\n\nSchad, D. J., Vasishth, S., Hohenstein, S., & Kliegl, R. (2020). How to capitalize on a priori contrasts in linear (mixed) models: A tutorial. Journal of Memory and Language, 110, 104038. https://doi.org/10.1016/j.jml.2019.104038\n\n\nShimizu, Y., Ogawa, K., Kimura, M., Fujiwara, K., & Watanabe, N. (2024). The influence of emotional facial expression intensity on decoding accuracy: High intensity does not yield high accuracy. The Japanese Psychological Research, 66, 521–540. https://doi.org/10.1111/jpr.12529\n\n\nTjur, T. (2009). Coefficients of Determination in Logistic Regression Models—A New Proposal: The Coefficient of Discrimination. The American Statistician, 63, 366–372. https://doi.org/10.1198/tast.2009.08210"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Generalized Linear Models",
    "section": "",
    "text": "Introduction\nIntroductory workshop about Generalized Linear Models starting from the general theory, model fitting, parameters interpretation and data simulation.\n\n\nSlides\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nIntroduction to Generalized Linear Models for Psychology\n\n\nLast modified: 29-01-2026\n\n\n\n\n\n\nBinomial GLM example\n\n\nLast modified: 29-01-2026\n\n\n\n\n\n\nNo matching items\n\n\n\nMaterials\n\nGLMphd is a complete course on GLM part of the PhD Course in Psychological Sciences (University of Padova).\nGeneral workflow for Monte Carlo Simulations: a short blog post about a general framework to create MCS in R.\n\n\n\nBooks\nSome books about Generalized Linear Models:\n\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and Other Stories. Cambridge University Press. https://doi.org/10.1017/9781139161879\nAgresti, A. (2018). An Introduction to Categorical Data Analysis. John Wiley & Sons.\nAgresti, A. (2015). Foundations of Linear and Generalized Linear Models. John Wiley & Sons.\nDunn, P. K., & Smyth, G. K. (2018). Generalized Linear Models With Examples in R. Springer.\nFox, J. (2015). Applied Regression Analysis and Generalized Linear Models. SAGE Publications.\nFaraway, J. J. (2016). Extending the linear model with R: Generalized linear, mixed effects and nonparametric regression models, second edition. Chapman; Hall/CRC. https://doi.org/10.1201/9781315382722\nMcCullagh, P., & Nelder, J. A. (1989). Generalized Linear Models (2nd ed.). Chapman & Hall/CRC. https://doi.org/10.1007/978-1-4899-3242-6"
  },
  {
    "objectID": "slides/01-intro-glm.html#linear-regression-1",
    "href": "slides/01-intro-glm.html#linear-regression-1",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Linear Regression",
    "text": "Linear Regression\nEstimate the expected (average) outcome given predictors.\n\n\nThe expected value of a random variable with a finite number of outcomes is a weighted average of all possible outcomes."
  },
  {
    "objectID": "slides/01-intro-glm.html#linear-regression-2",
    "href": "slides/01-intro-glm.html#linear-regression-2",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Linear Regression",
    "text": "Linear Regression\nA constant change in x leads to a constant change in the expected outcome."
  },
  {
    "objectID": "slides/01-intro-glm.html#normal-linear-regression-lmyx",
    "href": "slides/01-intro-glm.html#normal-linear-regression-lmyx",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Normal linear regression: lm(y~x)",
    "text": "Normal linear regression: lm(y~x)\nFor a fixed xix_i, the model describes the distribution of possible outcomes around the mean μi\\mu_i.\n\n\nThe expected value of a random variable with a finite number of outcomes is a weighted average of all possible outcomes."
  },
  {
    "objectID": "slides/01-intro-glm.html#normal-distribution",
    "href": "slides/01-intro-glm.html#normal-distribution",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Normal distribution",
    "text": "Normal distribution\nA Normal distribution has parameters μ\\mu (mean) and σ2\\sigma^2 (variance):\nf(y∣μ,σ2)=12πσ2exp(−(y−μ)22σ2)f(y \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( - \\frac{(y - \\mu)^2}{2\\sigma^2} \\right)"
  },
  {
    "objectID": "slides/01-intro-glm.html#normal-distribution-1",
    "href": "slides/01-intro-glm.html#normal-distribution-1",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Normal distribution",
    "text": "Normal distribution\nA Normal distribution has parameters μ\\mu (mean) and σ2\\sigma^2 (variance):\nf(y∣μ,σ2)=12πσ2exp(−(y−μ)22σ2)f(y \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( - \\frac{(y - \\mu)^2}{2\\sigma^2} \\right)"
  },
  {
    "objectID": "slides/01-intro-glm.html#normal-distribution-2",
    "href": "slides/01-intro-glm.html#normal-distribution-2",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Normal distribution",
    "text": "Normal distribution\nA Normal distribution has parameters μ\\mu (mean) and σ2\\sigma^2 (variance):\nf(y∣μ,σ2)=12πσ2exp(−(y−μ)22σ2)f(y \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( - \\frac{(y - \\mu)^2}{2\\sigma^2} \\right)"
  },
  {
    "objectID": "slides/01-intro-glm.html#normal-linear-regression",
    "href": "slides/01-intro-glm.html#normal-linear-regression",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Normal linear regression",
    "text": "Normal linear regression\n\n\n\nyi∣xi∼𝒩(μi,σ2),μi=β0+β1xiy_i \\mid x_i \\sim \\mathcal{N}(\\mu_i,\\sigma^2), \\qquad \\mu_i = \\beta_0 + \\beta_1 x_i"
  },
  {
    "objectID": "slides/01-intro-glm.html#normal-distribution-3",
    "href": "slides/01-intro-glm.html#normal-distribution-3",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Normal distribution",
    "text": "Normal distribution\n\nSupport: −∞,+∞-\\infty, +\\infty\nMean (expected value): μ\\mu\nVariance: σ2\\sigma^2\nIndependence: changing the mean does not change the variance!"
  },
  {
    "objectID": "slides/01-intro-glm.html#why-do-these-matter-for-regression",
    "href": "slides/01-intro-glm.html#why-do-these-matter-for-regression",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Why do these matter for regression?",
    "text": "Why do these matter for regression?\nWhen building a model, we need to know:\n\nSupport: what range of values is possible\nMean (expected value): what value to predict on average\nVariance: how much variation around the mean\nMean–variance relationship: does variance change with the mean?"
  },
  {
    "objectID": "slides/01-intro-glm.html#where-the-normal-model-breaks-down",
    "href": "slides/01-intro-glm.html#where-the-normal-model-breaks-down",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Where the Normal model breaks down",
    "text": "Where the Normal model breaks down\nReaction Times"
  },
  {
    "objectID": "slides/01-intro-glm.html#where-the-normal-model-breaks-down-1",
    "href": "slides/01-intro-glm.html#where-the-normal-model-breaks-down-1",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Where the Normal model breaks down",
    "text": "Where the Normal model breaks down\nExam pass/fail"
  },
  {
    "objectID": "slides/01-intro-glm.html#where-the-normal-model-breaks-down-2",
    "href": "slides/01-intro-glm.html#where-the-normal-model-breaks-down-2",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Where the Normal model breaks down",
    "text": "Where the Normal model breaks down\nNumber of errors in a task"
  },
  {
    "objectID": "slides/01-intro-glm.html#understanding-distributions",
    "href": "slides/01-intro-glm.html#understanding-distributions",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Understanding distributions",
    "text": "Understanding distributions\nA probability distribution of a random variable YY describes the probabilities assigned to each possible value yy, given certain parameters values.\n\n\nf(y∣𝛉)f(y \\mid \\boldsymbol{\\theta})\n\n\n\nyy is a specific value\n𝛉\\boldsymbol{\\theta} is a vector of variables (parameters)\nf(⋅)f(\\cdot) is the function itself"
  },
  {
    "objectID": "slides/01-intro-glm.html#understanding-distributions-1",
    "href": "slides/01-intro-glm.html#understanding-distributions-1",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Understanding distributions",
    "text": "Understanding distributions\n\nDiscrete random variables: counts, binary, etc.\n\n→\\rightarrow f(Y∣𝛉)f(Y\\mid \\boldsymbol{\\theta}) becomes f(Y=y∣𝛉)f(Y = y\\mid \\boldsymbol{\\theta}) because we are assigning a certain probability to a discrete variables. The function is called probability mass function (PMF).\n\n\nContinuous random variables: time, weight, etc.\n\n→\\rightarrow f(Y∣𝛉)f(Y\\mid \\boldsymbol{\\theta}) and the function is called probability density function (PDF).\n\n\n\n\nThe PMF return the probability associated with a certain value of yy while the PDF returns the probability density of a certain value of yy"
  },
  {
    "objectID": "slides/01-intro-glm.html#moments-of-the-distributions",
    "href": "slides/01-intro-glm.html#moments-of-the-distributions",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Moments of the distributions",
    "text": "Moments of the distributions\nWhen we use the distributions, we are usually interested in some properties describing the distribution:\n\nFirst moment: is the expected value (i.e., the mean) of the distribution\nSecond moment: is the variance of the distribution\nThird moment: is the skewness\nFourth moment: is the kurtosis"
  },
  {
    "objectID": "slides/01-intro-glm.html#why-is-it-important",
    "href": "slides/01-intro-glm.html#why-is-it-important",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Why is it important?",
    "text": "Why is it important?\nMoments are always the same but the way we actual compute them could change according to the distribution.\nIn normal linear and generalized linear models we generally include predictors on the mean of the distribution.\nIn the case of Normal distribution the mean and the variance are the same as the parameters defining the distribution."
  },
  {
    "objectID": "slides/01-intro-glm.html#why-is-important",
    "href": "slides/01-intro-glm.html#why-is-important",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Why is important?",
    "text": "Why is important?\n\n\n\n\n\n\n\n\n\nDistribution\nSupport\nmean: E[Y]E[Y]\nvariance: Var(Y)\\mathrm{Var}(Y)\n\n\n\n\nNormal: f(y | μ,σ2)f(y \\text{ | } \\mu,\\sigma^2)\ny∈ℝy \\in \\mathbb{R}\nμ\\mu\nσ2\\sigma^2\n\n\nGamma: f(y | α,β)f(y \\text{ | } \\alpha,\\beta)\ny∈(0,∞)y \\in (0,\\infty)\nα/β\\alpha/\\beta\nα/β2\\alpha/\\beta^2\n\n\nBinomial: f(y | n,p)f(y \\text{ | } n,p)\ny∈{0,1,…,n}y \\in \\{0,1,\\dots,n\\}\nnpnp\nnp(1−p)np(1-p)\n\n\nPoisson: f(y | λ)f(y \\text{ | } \\lambda)\ny∈{0,1,2,…}y \\in \\{0,1,2,\\dots\\}\nλ\\lambda\nλ\\lambda"
  },
  {
    "objectID": "slides/01-intro-glm.html#example-passing-the-exam",
    "href": "slides/01-intro-glm.html#example-passing-the-exam",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Example: Passing the exam",
    "text": "Example: Passing the exam\nProbability of passing the exam as a function of how many hours students study:\n\n\n\n\n   id studyh passed\n1   1     29      0\n2   2     79      1\n3   3     41      1\n4   4     88      1\n5   5     94      1\n6   6      5      0\n7   7     53      0\n8   8     89      1\n9   9     55      1\n10 10     46      0\n11 11     96      1\n12 12     45      0\n13 13     68      1\n14 14     57      0\n15 15     10      0\n16 16     90      1\n17 17     25      0\n18 18      4      0\n19 19     33      1\n20 20     95      1\n21 21     89      1\n22 22     69      0\n23 23     64      0\n24 24     99      1\n25 25     66      1\n26 26     71      1\n27 27     54      0\n28 28     59      1\n29 29     29      0\n30 30     15      0\n31 31     96      1\n32 32     90      1\n33 33     69      1\n34 34     80      0\n35 35      2      0\n36 36     48      0\n37 37     76      0\n38 38     22      1\n39 39     32      1\n40 40     23      0\n41 41     14      0\n42 42     41      0\n43 43     41      0\n44 44     37      0\n45 45     15      0\n46 46     14      0\n47 47     23      0\n48 48     47      0\n49 49     27      0\n50 50     86      1\n\n\n\n\n# number of students that have passed the exam\nsum(dat_exam$passed) \n#&gt; [1] 22\n# proportion of students that have passed the exam\nmean(dat_exam$passed) \n#&gt; [1] 0.44\n\n# study hours and passing the exam\ntapply(dat_exam$studyh, dat_exam$passed, mean)\n#&gt;        0        1 \n#&gt; 35.46429 73.04545\n#&gt; \ntable(dat_exam$passed, cut(dat_exam$studyh, breaks = 4))\n#&gt;    (1.9,26.2] (26.2,50.5] (50.5,74.8] (74.8,99.1]\n#&gt;  0         11          10           5           2\n#&gt;  1          1           3           6          12\n#&gt;  \ntapply(dat_exam$passed, cut(dat_exam$studyh, breaks = 4), mean)\n#&gt;   (1.9,26.2] (26.2,50.5] (50.5,74.8] (74.8,99.1] \n#&gt;   0.08333333  0.23076923  0.54545455  0.85714286"
  },
  {
    "objectID": "slides/01-intro-glm.html#example-passing-the-exam-1",
    "href": "slides/01-intro-glm.html#example-passing-the-exam-1",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Example: Passing the exam",
    "text": "Example: Passing the exam"
  },
  {
    "objectID": "slides/01-intro-glm.html#fitting-a-normal-linear-model",
    "href": "slides/01-intro-glm.html#fitting-a-normal-linear-model",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Fitting a Normal Linear Model",
    "text": "Fitting a Normal Linear Model\n\nDo you see something strange?"
  },
  {
    "objectID": "slides/01-intro-glm.html#fitting-a-generalized-linear-model",
    "href": "slides/01-intro-glm.html#fitting-a-generalized-linear-model",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Fitting a Generalized Linear Model",
    "text": "Fitting a Generalized Linear Model\nThe model should consider both the support of the yy variable and the non-linear pattern!"
  },
  {
    "objectID": "slides/01-intro-glm.html#general-idea",
    "href": "slides/01-intro-glm.html#general-idea",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "General idea",
    "text": "General idea\n\nusing distributions beyond the Gaussian\nmodeling non linear functions on the response scale\ntaking into account mean-variance relationships"
  },
  {
    "objectID": "slides/01-intro-glm.html#the-three-ingredients-of-a-glm",
    "href": "slides/01-intro-glm.html#the-three-ingredients-of-a-glm",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "The three ingredients of a GLM",
    "text": "The three ingredients of a GLM\n\nRandom Component: Choose a distribution\nSystematic Component: Linear predictor ηi=β0+β1xi\\eta_i = \\beta_0 + \\beta_1 x_i\nLink Function: Transform the mean g(μi)=ηig(\\mu_i) = \\eta_i"
  },
  {
    "objectID": "slides/01-intro-glm.html#random-component",
    "href": "slides/01-intro-glm.html#random-component",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "1. Random component",
    "text": "1. Random component\nThe random component specifies a probability model for yiy_i:\n\nyi∣xi∼Distribution(parameters).\ny_i \\mid x_i \\sim \\text{Distribution}(\\text{parameters}).\n\n\n\nWhat support?\n\nBinary: {0,1}\\{0,1\\} →\\rightarrow Bernoulli(p)\\text{Bernoulli}(p) (or Binomial(1,p)\\text{Binomial}(1,p))\nCounts: {0,1,2,…}\\{0,1,2,\\ldots\\} →\\rightarrow Poisson(λ)\\text{Poisson}(\\lambda)\nAny real number: (−∞,∞)(-\\infty,\\infty) →\\rightarrow Normal(μ,σ2)\\text{Normal}(\\mu,\\sigma^2)"
  },
  {
    "objectID": "slides/01-intro-glm.html#random-component-meanvariance",
    "href": "slides/01-intro-glm.html#random-component-meanvariance",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "1. Random component (mean–variance)",
    "text": "1. Random component (mean–variance)\nChoosing a distribution specifies not only the mean, but also how the variance depends on the mean:\n\n\nVar(Y∣X)=ϕV(μ),μ=E(Y∣X).\n\\mathrm{Var}(Y \\mid X) = \\phi \\, V(\\mu), \\qquad \\mu = E(Y \\mid X).\n\n\n\nwhere V(μ)V(\\mu) is the variance function (determined by the family) and ϕ\\phi is a constant scale factor.\n\nNormal: V(μ)=1V(\\mu) = 1 (constant variance, ϕ=σ2\\phi = \\sigma^2)\nBernoulli: V(μ)=μ(1−μ)V(\\mu) = \\mu(1-\\mu) (typically ϕ=1\\phi = 1)\nPoisson: V(μ)=μV(\\mu) = \\mu (typically ϕ=1\\phi = 1)"
  },
  {
    "objectID": "slides/01-intro-glm.html#systematic-component",
    "href": "slides/01-intro-glm.html#systematic-component",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "2. Systematic Component",
    "text": "2. Systematic Component\nThe systematic component is exactly the same as in normal linear regression: we predict a linear combination of predictors.\n\nηi=β0+β1xi1+β2xi2+⋯+βkxik.\n\\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_k x_{ik}.\n \nBasically it describes how the expected value (i.e., the mean, the first moment) of the chosen distribution (the random component) varies according to the predictors."
  },
  {
    "objectID": "slides/01-intro-glm.html#link-function",
    "href": "slides/01-intro-glm.html#link-function",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "3. Link Function",
    "text": "3. Link Function\nThe link function g(⋅)g(\\cdot) connects the expected value (mean) μi\\mu_i of the distribution to the linear predictor ηi\\eta_i:\n\n\ng(μi)=ηi\ng(\\mu_i) = \\eta_i\n\n\n\n\nThe linear predictor ηi\\eta_i can be any real number: (−∞,+∞)(-\\infty, +\\infty)\nBut μi\\mu_i (the mean) is constrained by the distribution’s support\nThe link function transforms μi\\mu_i to be unbounded"
  },
  {
    "objectID": "slides/01-intro-glm.html#common-link-functions",
    "href": "slides/01-intro-glm.html#common-link-functions",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Common link functions",
    "text": "Common link functions\n\n\n\n\n\n\n\n\n\nDistribution\nSupport of yy\nLink\nPurpose\n\n\n\n\nNormal\n(−∞,∞)(-\\infty,\\infty)\nIdentity: g(μ)=μg(\\mu)=\\mu\nNo transformation\n\n\nBinomial\n{0,1,…,n}\\{0,1,\\ldots,n\\}\nLogit on pp: g(pi)=log(p1−p)g(p_i)=\\log\\!\\left(\\frac{p}{1-p}\\right), where p=μ/np=\\mu/n\nProbability →ℝ\\to \\mathbb{R}\n\n\nPoisson\n{0,1,2,…}\\{0,1,2,\\ldots\\}\nLog: g(μ)=log(μ)g(\\mu)=\\log(\\mu)\nPositive →ℝ\\to \\mathbb{R}\n\n\nGamma\n(0,∞)(0,\\infty)\nLog: g(μ)=log(μ)g(\\mu)=\\log(\\mu)\nPositive →ℝ\\to \\mathbb{R}"
  },
  {
    "objectID": "slides/01-intro-glm.html#normal-identity",
    "href": "slides/01-intro-glm.html#normal-identity",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Normal + identity",
    "text": "Normal + identity\nFor example, a Generalized Linear Model with the Normal family and identity link can be written as:\n\nyi∼𝒩(μi,σ2)Random componentμi=ηiLink (identity)ηi=β0+β1xi1+⋯+βkxikSystematic component\n\\begin{aligned}\ny_i &\\sim \\mathcal{N}(\\mu_i, \\sigma^2) && \\text{Random component} \\\\\n\\mu_i &= \\eta_i && \\text{Link (identity)} \\\\\n\\eta_i &= \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik} && \\text{Systematic component}\n\\end{aligned}"
  },
  {
    "objectID": "slides/01-intro-glm.html#binomial-logit",
    "href": "slides/01-intro-glm.html#binomial-logit",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Binomial + logit",
    "text": "Binomial + logit\nFor example, a Generalized Linear Model with the Binomial family and logit link can be written as:\n\nyi∣xi∼Binomial(ni,pi)Random componentlogit(pi)=log(pi1−pi)=ηiLink (logit)ηi=β0+β1xi1+⋯+βkxikSystematic component\n\\begin{aligned}\ny_i \\mid x_i &\\sim \\text{Binomial}(n_i, p_i) && \\text{Random component} \\\\\n\\text{logit}(p_i) &= \\log\\!\\left(\\frac{p_i}{1-p_i}\\right)=\\eta_i && \\text{Link (logit)} \\\\\n\\eta_i &= \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik} && \\text{Systematic component}\n\\end{aligned}"
  },
  {
    "objectID": "slides/01-intro-glm.html#binary-outcomes-and-counts",
    "href": "slides/01-intro-glm.html#binary-outcomes-and-counts",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Binary outcomes and counts",
    "text": "Binary outcomes and counts\nThe Bernoulli or the Binomial distributions can be used as random component when we have a binary dependent variable or the number of successes over the total number of trials.\n\n\n\nBinary (pass/fail), one trial per person; or\nCounts of successes out of nn trials (e.g., items correct out of nn)."
  },
  {
    "objectID": "slides/01-intro-glm.html#bernoulli-one-student",
    "href": "slides/01-intro-glm.html#bernoulli-one-student",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Bernoulli (one student)",
    "text": "Bernoulli (one student)\nLet k∈{0,1}k \\in \\{0,1\\} indicate whether a student passes the exam:\n\n\nf(k;p)={pif k=1,q=1−pif k=0.\nf(k; p) =\n\\begin{cases}\np       & \\text{if } k = 1, \\\\\nq = 1-p & \\text{if } k = 0.\n\\end{cases}\n\n\n\nThe probability mass function f{\\displaystyle f} of the Bernoulli distribution, over possible outcomes kk, is f(k,p)=pk(1−p)1−kf(k,p) = p^k(1-p)^{1-k}\n\n\nWhere pp is the probability of success and kk the two possible results 0 and 1. The mean is pp and the variance is p(1−p)p(1-p)"
  },
  {
    "objectID": "slides/01-intro-glm.html#one-student-takes-the-exam",
    "href": "slides/01-intro-glm.html#one-student-takes-the-exam",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "One student takes the exam",
    "text": "One student takes the exam\n\nrbinom(n = 1, size = 1, prob = 0.7)\n\n[1] 0\n\n\n\nOver 10,000 students?\n\nmany = rbinom(n = 100000, size = 1, prob = 0.7); head(many)\n\n[1] 1 1 1 0 1 1\n\n\n\n\n\n# Mean\n(p = mean(many))\n\n[1] 0.70085\n\n\n\n\n\n# Variance\n(p*(1-p))\n\n[1] 0.2096593\n\n(sd(many)^2)\n\n[1] 0.2096614"
  },
  {
    "objectID": "slides/01-intro-glm.html#binomial-a-class-of-students",
    "href": "slides/01-intro-glm.html#binomial-a-class-of-students",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Binomial (a class of students)",
    "text": "Binomial (a class of students)\nNow let kk be the number of students who pass out of nn students who take the same exam.\n\n\nThe probability of having kk success (e.g., 0, 1, 2, etc.), out of nn trials with a probability of success pp (and failing q=1−pq = 1 - p) is:\n\n\nf(n,k,p)=(nk)pk(1−p)n−k\nf(n, k, p) \\;=\\; \\binom{n}{k}\\, p^{k}\\,(1-p)^{\\,n-k}\n\n\n\nThe npnp is the mean of the binomial distribution and np(1−p)np(1-p) is the variance. The binomial distribution is just the repetition of nn independent Bernoulli trials."
  },
  {
    "objectID": "slides/01-intro-glm.html#ten-students-take-the-exam",
    "href": "slides/01-intro-glm.html#ten-students-take-the-exam",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Ten students take the exam",
    "text": "Ten students take the exam\nHow many pass?\n\nn = 10; p = 0.7\nrbinom(n = 1, size = n, prob = 0.7)\n\n[1] 7\n\n\n\nOver 10,000 repetition …\n\nmany = rbinom(n = 100000, size = n, prob = p); head(many)\n\n[1] 7 8 8 8 8 9\n\n\n\n\n\nn*p # Mean count: E(Y) = np \n\n[1] 7\n\n\n\n\n\nn*p*(1-p) # Variance count: Var(y) = np(1-p)\n\n[1] 2.1\n\n\n\n\n\nmean(many/n) # Mean proportion = p\n\n[1] 0.700083"
  },
  {
    "objectID": "slides/01-intro-glm.html#binomial-distribution-n-20-p-0.9",
    "href": "slides/01-intro-glm.html#binomial-distribution-n-20-p-0.9",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Binomial distribution: n=20n = 20, p=0.9p = 0.9",
    "text": "Binomial distribution: n=20n = 20, p=0.9p = 0.9"
  },
  {
    "objectID": "slides/01-intro-glm.html#binomial-distribution-n-20-p-0.5",
    "href": "slides/01-intro-glm.html#binomial-distribution-n-20-p-0.5",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Binomial distribution: n=20n = 20, p=0.5p = 0.5",
    "text": "Binomial distribution: n=20n = 20, p=0.5p = 0.5"
  },
  {
    "objectID": "slides/01-intro-glm.html#meanvariance-relationship",
    "href": "slides/01-intro-glm.html#meanvariance-relationship",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Mean–variance relationship",
    "text": "Mean–variance relationship\nE[Y]=npandVar(Y)=np(1−p)E[Y] = np \\quad \\text{and} \\quad \\mathrm{Var}(Y) = np(1-p)\n\nVariance is not constant!"
  },
  {
    "objectID": "slides/01-intro-glm.html#odds",
    "href": "slides/01-intro-glm.html#odds",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Odds",
    "text": "Odds\nBecause probabilities must stay between 0 and 1, we work with odds.\nLet p=P(Pass=1)p = P(\\text{Pass}=1), then the odds of passing compare “pass” to “fail”:\n\n\nodds=p1−p,p=oddsodds+1\n\\text{odds} = \\frac{p}{1-p}, \\qquad \\text{p} = \\frac{odds}{odds+1}\n\n\n\n\nIf p=0.80p=0.80, then odds=0.800.20=4\\text{odds}=\\frac{0.80}{0.20}=4.\n\n\n\n\nSo passing is 4-to-1 relative to failing (4 expected passes for 1 fail, on average)."
  },
  {
    "objectID": "slides/01-intro-glm.html#logit-link-log-odds",
    "href": "slides/01-intro-glm.html#logit-link-log-odds",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Logit link (log-odds)",
    "text": "Logit link (log-odds)\nThe logit is the log-odds:\nlogit(p)=log(p1−p)=η.\n\\text{logit}(p)=\\log\\!\\left(\\frac{p}{1-p}\\right)=\\eta.\n\n\n\nThis maps p∈(0,1)p \\in (0,1) to any real number, which makes it easier to model with a linear predictor."
  },
  {
    "objectID": "slides/01-intro-glm.html#why",
    "href": "slides/01-intro-glm.html#why",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Why ?",
    "text": "Why ?\nThe odds have an interesting property when taking the logarithm. We can express a probability pp sing a scale ranging [+∞,−∞][+\\infty,-\\infty]."
  },
  {
    "objectID": "slides/01-intro-glm.html#odds-1",
    "href": "slides/01-intro-glm.html#odds-1",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Odds",
    "text": "Odds\nWith pi=P(Passi=1∣xi)p_i = P(\\text{Pass}_i = 1 \\mid x_i) and xix_i hours studied, the model assumes:\n\n\nlog(pi1−pi)=ηiηi=β0+β1xi\n\\begin{aligned}\n\\log\\left(\\frac{p_i}{1-p_i}\\right) &= \\eta_i \\\\\n\\eta_i &= \\beta_0 + \\beta_1 x_i\n\\end{aligned}\n\nThen the odds:\n\n\npi1−pi=exp(β0+β1xi).\n\\frac{p_i}{1-p_i}=\\exp(\\beta_0+\\beta_1 x_i)."
  },
  {
    "objectID": "slides/01-intro-glm.html#odds-ratios",
    "href": "slides/01-intro-glm.html#odds-ratios",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Odds ratios",
    "text": "Odds ratios\nThen the odds:\npi1−pi=exp(β0+β1xi).\n\\frac{p_i}{1-p_i}=\\exp(\\beta_0+\\beta_1 x_i).\n\n\n\nWhen x=0x = 0\npi1−pi=exp(β0).\n\\frac{p_i}{1-p_i} = \\exp(\\beta_0).\n\n\n\nWhen x=1x = 1\npi1−pi=exp(β0+β1)=exp(β0)exp(β1).\n\\frac{p_i}{1-p_i} =  \\exp(\\beta_0 + \\beta_1) = \\exp(\\beta_0)\\exp(\\beta_1)."
  },
  {
    "objectID": "slides/01-intro-glm.html#odds-ratios-1",
    "href": "slides/01-intro-glm.html#odds-ratios-1",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Odds Ratios",
    "text": "Odds Ratios\nThen the ratio of these two odds is:\n\n\nexp(β0)exp(β1)exp(β0)=exp(β1)\\frac{\\exp(\\beta_0)\\exp(\\beta_1)}{\\exp(\\beta_0)} = \\exp(\\beta_1)\n\n\nThis means that the odds of passing when increasing study hours by 1 is exp(β1)\\exp(\\beta_1) times greater than at the baseline (i.e., when x=0x = 0)."
  },
  {
    "objectID": "slides/01-intro-glm.html#linear-on-log-odds",
    "href": "slides/01-intro-glm.html#linear-on-log-odds",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Linear on log-odds",
    "text": "Linear on log-odds\nA +1 increase in xx changes η\\eta by a constant amount β1\\beta_1. So equal increases in xx correspond to equal increases in log-odds."
  },
  {
    "objectID": "slides/01-intro-glm.html#inverse-link-back-to-probability",
    "href": "slides/01-intro-glm.html#inverse-link-back-to-probability",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Inverse link (back to probability)",
    "text": "Inverse link (back to probability)\nTo go back to probability we apply the inverse-logit:\npi=eηi1+eηi.\np_i = \\frac{e^{\\eta_i}}{1+e^{\\eta_i}}."
  },
  {
    "objectID": "slides/01-intro-glm.html#not-linear-in-probability",
    "href": "slides/01-intro-glm.html#not-linear-in-probability",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Not linear in probability",
    "text": "Not linear in probability\nEqual increases in xx generally do not correspond to equal increases in pp, because p=logit−1(η)p=\\text{logit}^{-1}(\\eta) is nonlinear."
  },
  {
    "objectID": "slides/01-intro-glm.html#numerical-example-beta_10.8",
    "href": "slides/01-intro-glm.html#numerical-example-beta_10.8",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "Numerical example (β1=0.8\\beta_1=0.8)",
    "text": "Numerical example (β1=0.8\\beta_1=0.8)\nHere exp(0.8)≈2.23\\exp(0.8)\\approx 2.23, so each +1 hour multiplies the odds by ~2.23.\n\n\n\n\n\n\n\n\n\n\nBaseline p\nBaseline odds p/(1−p)p/(1-p)\nNew odds =2.23×=2.23\\times odds\nNew p\nΔ\\Delta p\n\n\n\n\n0.10\n0.11\n0.25\n0.20\n+0.10\n\n\n0.20\n0.25\n0.55\n0.36\n+0.16\n\n\n0.36\n0.55\n1.22\n0.55\n+0.20\n\n\n0.55\n1.22\n2.73\n0.73\n+0.18\n\n\n0.73\n2.73\n6.07\n0.86\n+0.13\n\n\n0.86\n6.07\n13.51\n0.93\n+0.07"
  },
  {
    "objectID": "slides/01-intro-glm.html#section",
    "href": "slides/01-intro-glm.html#section",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "",
    "text": "Vecteezy"
  },
  {
    "objectID": "slides/01-intro-glm.html#references",
    "href": "slides/01-intro-glm.html#references",
    "title": "Introduction to Generalized Linear Models for Psychology",
    "section": "References",
    "text": "References"
  }
]
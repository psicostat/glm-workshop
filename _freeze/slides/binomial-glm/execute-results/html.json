{
  "hash": "50227a96fb73f5fe8ff2afc4e274435b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Binomial GLM\nformat:\n    minimal-revealjs:\n        df-print: default\nexecute: \n  echo: true\n---\n\n\n\n```{=html}\n<style>\n    .hg {\n        background-color: yellow;\n    }\n</style>\n```\n\n\n\n\n\n## Logistic distribution and binomial\n\n## Example: @Shimizu2024-xl\n\n@Shimizu2024-xl investigated the processing of emotional faces.\n\n- 6 basic emotions: anger, disgust, fear, happiness, sadness and surprise\n- intensity in % (from 10% to 100% in steps of 10%)\n- 71 participants\n- 377 faces (males and females of different identities)\n- forced-choice procedure with 7 options (6 emotions + neutral). Chance level at $1/7 = 0.14$.\n\n## @Shimizu2024-xl dataset\n\nWe did some pre-processing for the purpose of this example. The original dataset can be found at {{< ai osf >}} [osf.io/zhtbj/](https://osf.io/zhtbj/).\n\nYou can download the dataset for this example at this [link](../data/shimizu2024.rds). It is an `rds` file and you can open it using:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- readRDS(\"shimizu.rds\")\n```\n:::\n\n\nThen we can load some packages:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4)      # for mixed-models\nlibrary(tidyverse) # for data manipulation\nlibrary(ggplot2)   # plotting\n```\n:::\n\n\n## Exploring\n\nFor the purpose if this workshop, we will focus on a single subject (otherwise we should use a mixed-effects model). We also select only the relevant columns.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- subset(dat, id == 22)\ndat <- dat[, c(\"id\", \"age\", \"intensity\", \"emotion_lbl\", \"response_lbl\", \"acc\")]\ndat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 377 × 6\n      id age   intensity emotion_lbl response_lbl   acc\n   <int> <chr>     <dbl> <chr>       <chr>        <int>\n 1    22 53           60 fear        suprise          0\n 2    22 53           60 disgust     disgust          1\n 3    22 53           70 happiness   happiness        1\n 4    22 53          100 happiness   happiness        1\n 5    22 53           60 disgust     sadness          0\n 6    22 53           20 fear        neutral          0\n 7    22 53           10 fear        neutral          0\n 8    22 53          100 sadness     sadness          1\n 9    22 53           90 disgust     disgust          1\n10    22 53           40 happiness   happiness        1\n# ℹ 367 more rows\n```\n\n\n:::\n:::\n\n\n## Exploring\n\n- We have 377 trials and 6 columns. \n- The `intensity` is the intensity (from 10% to 100%) of the facial expression. `emotion_lbl` is the emotion and `response_lbl` is the response. \n- When `emotion_lbl = response_lbl` the `acc = 1` namely a correct response.\n\n## Exploring\n\nWe can calculate the average accuracy for each emotion. Clearly there is a big difference with `fear` being the hardest one and `surprise` the easiest. We remove `neutral` because we have no associated intensity\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat |> \n  group_by(emotion_lbl) |> \n  summarise(p = mean(acc),\n            n = n()) |> \n  arrange(desc(p))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 3\n  emotion_lbl     p     n\n  <chr>       <dbl> <int>\n1 neutral     1         7\n2 surprise    0.8      70\n3 happiness   0.686    70\n4 sadness     0.517    60\n5 disgust     0.35    100\n6 anger       0.333    30\n7 fear        0.05     40\n```\n\n\n:::\n\n```{.r .cell-code}\ndat <- filter(dat, emotion_lbl != \"neutral\")\n```\n:::\n\n\n## Exploring\n\nAlso for intensity, there is a clear increasing pattern. In this specific subject, the highest intensities are not clearly distinguished (maybe a non-linear pattern).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat |> \n  group_by(intensity) |> \n  summarise(p = mean(acc),\n            n = n()) |> \n  arrange(desc(p))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 3\n   intensity      p     n\n       <dbl>  <dbl> <int>\n 1        70 0.703     37\n 2        90 0.676     37\n 3       100 0.676     37\n 4        80 0.649     37\n 5        50 0.622     37\n 6        60 0.622     37\n 7        40 0.486     37\n 8        30 0.324     37\n 9        10 0.0811    37\n10        20 0.0811    37\n```\n\n\n:::\n:::\n\n\n## Exploring, plots\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\ndat |> \n  group_by(emotion_lbl) |> \n  summarise(p = mean(acc),\n            n = n()) |> \n  ggplot(aes(x = fct_reorder(emotion_lbl, p), y = p)) + \n  geom_point(size = 4) +\n  geom_line(group = 1) +\n  ylim(c(0, 1)) +\n  xlab(\"Emotion\") +\n  ylab(\"Accuracy\") +\n  geom_hline(yintercept = 1/7, lty = \"dotted\")\n```\n\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n## Exploring, plots\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\ndat |> \n  group_by(intensity) |> \n  summarise(p = mean(acc),\n            n = n()) |> \n  arrange(desc(p)) |> \n  ggplot(aes(x = intensity, y = p)) + \n  geom_point(size = 4) +\n  geom_line() +\n  ylim(c(0, 1)) +\n  xlab(\"Intensity (%)\") +\n  ylab(\"Accuracy\") +\n  geom_hline(yintercept = 1/7, lty = \"dotted\")\n```\n\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n## Exploring, plots\n\nWe have few trials but we can also explore the interaction between emotion and intensity. There are some emotions where the rate of increase in accuracy as a function of the emotion is faster compared to others.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\ndat |> \n  group_by(intensity, emotion_lbl) |> \n  summarise(p = mean(acc)) |> \n  ggplot(aes(x = intensity, y = p, color = emotion_lbl)) +\n  geom_point(size = 4) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n## Exploring, odds and odds ratios\n\nWe can start exploring the effects calcualting odds and odds ratios.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nodds <- function(p) p / (1 - p)\nor <- function(pn, pd) odds(pn) / odds(pd)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(p_anger <- mean(dat$acc[dat$emotion_lbl == \"anger\"]))\n## [1] 0.3333333\n(p_surprise <- mean(dat$acc[dat$emotion_lbl == \"surprise\"]))\n## [1] 0.8\n\nodds(p_anger)\n## [1] 0.5\nodds(p_surprise)\n## [1] 4\n\nor(p_surprise, p_anger)\n## [1] 8\n```\n:::\n\n\n## Exploring, odds and odds ratios\n\nWe can also create a contingency table:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(dat$acc, dat$emotion_lbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   \n    anger disgust fear happiness sadness surprise\n  0    20      65   38        22      29       14\n  1    10      35    2        48      31       56\n```\n\n\n:::\n\n```{.r .cell-code}\n(all_p <- tapply(dat$acc, dat$emotion_lbl, FUN = mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    anger   disgust      fear happiness   sadness  surprise \n0.3333333 0.3500000 0.0500000 0.6857143 0.5166667 0.8000000 \n```\n\n\n:::\n\n```{.r .cell-code}\nodds(all_p)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     anger    disgust       fear  happiness    sadness   surprise \n0.50000000 0.53846154 0.05263158 2.18181818 1.06896552 4.00000000 \n```\n\n\n:::\n:::\n\n\nWhen the odds are lower than 1, the probability of success is lower than the probability of failure. When the odds are greater than 1 the probability of success is higher.\n\n## Exploring, odds and odds ratios\n\nWe can also calculate all the possible comparisons. Note that depending on the numerator/denominator the odds ratio is different but we can simply take the inverse to switch numerator and numerator. Interpreting odds ratio > 1 is usually more intutive.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n     anger / disgust         anger / fear    anger / happiness \n               0.929                9.500                0.229 \n     anger / sadness     anger / surprise       disgust / fear \n               0.468                0.125               10.231 \n disgust / happiness    disgust / sadness   disgust / surprise \n               0.247                0.504                0.135 \n    fear / happiness       fear / sadness      fear / surprise \n               0.024                0.049                0.013 \n happiness / sadness happiness / surprise   sadness / surprise \n               2.041                0.545                0.267 \n```\n\n\n:::\n:::\n\n\nFor example, `happiness / sadness ~ 2.04` means that the odds (not the probability) of a correct response is 2 times higher for happy faces compared to sad faces. \n\n# Model\n\n## The `glm` function\n\nIn R we can fit a GLM with the `glm` function. The syntax is the same as the `lm` (for standard linear models). We only need to specify the **random component** and the **link function**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm(y ~ x1 + x2 + x3 * x4, # systematic component (linear predictor)\n    data = data,\n    family = binomial(link = \"logit\")) # random component and link function\n```\n:::\n\n\nClearly, the `y` in this example need to be consistent with the chosen family. In this case the model is expecting a 0-1 vector. If we provide labels (characters) or number > 1 or < 0 the function will fail.\n\n::: {.callout-note}\nA `glm` with `family = gaussian(link = \"identity\")` is the same as running a `lm`. Internally `glm` calls `lm` in this case.\n:::\n\n## The null model\n\nWe can start with the easiest model that is a model without the systematic component (with no predictors).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit0 <- glm(acc ~ 1, data = dat, family = binomial(link = \"logit\"))\nsummary(fit0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = acc ~ 1, family = binomial(link = \"logit\"), data = dat)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept) -0.03244    0.10399  -0.312    0.755\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 512.83  on 369  degrees of freedom\nResidual deviance: 512.83  on 369  degrees of freedom\nAIC: 514.83\n\nNumber of Fisher Scoring iterations: 3\n```\n\n\n:::\n:::\n\n\n## The null model\n\n![](img/fit0_summary.png)\n\n## The null model, formally\n\n$$\n\\eta_i = \\beta_0\n$$\n\n$$\np_i = g^{-1}(\\eta_i) = g^{-1}(\\beta_0)\n$$\n\n$g^{-1}(\\cdot)$ is the inverse-logit link:\n\n$$\np_i = \\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}\n$$\n\nIn other terms, the probability can be calculated inverting the logit link function evaluated on the linear predictor $\\eta$. In this case $\\eta$ only contains $\\beta_0$.\n\n## The null model, interpretation\n\nIn this case, the intercept is -0.032. The intercept is the expected value (i.e., the mean) of `y` (accuracy here) when everthing is zero. In this case $\\beta_0$ is just the (logit transformed) overall accuracy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb0 <- coef(fit0)[\"(Intercept)\"]\n\nexp(b0) / (1 + exp(b0)) # inverse logit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n  0.4918919 \n```\n\n\n:::\n\n```{.r .cell-code}\nplogis(b0)              # directly with the dedicated function\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n  0.4918919 \n```\n\n\n:::\n\n```{.r .cell-code}\nmean(dat$acc)           # average accuracy\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4918919\n```\n\n\n:::\n\n```{.r .cell-code}\nlog(odds(mean(dat$acc))) # probability to logit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.03243528\n```\n\n\n:::\n\n```{.r .cell-code}\nqlogis(mean(dat$acc)) # probability to logit with the dedicated function\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.03243528\n```\n\n\n:::\n:::\n\n\n## Categorical predictor, emotion\n\nThen we can include `emotion_lbl` as predictor. Let's see what happens:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_em <- glm(acc ~ emotion_lbl, data = dat, family = binomial(link = \"logit\"))\nsummary(fit_em)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = acc ~ emotion_lbl, family = binomial(link = \"logit\"), \n    data = dat)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)          -0.69315    0.38730  -1.790  0.07350 .  \nemotion_lbldisgust    0.07411    0.44040   0.168  0.86637    \nemotion_lblfear      -2.25129    0.82235  -2.738  0.00619 ** \nemotion_lblhappiness  1.47331    0.46507   3.168  0.00154 ** \nemotion_lblsadness    0.75984    0.46555   1.632  0.10266    \nemotion_lblsurprise   2.07944    0.48917   4.251 2.13e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 512.83  on 369  degrees of freedom\nResidual deviance: 423.88  on 364  degrees of freedom\nAIC: 435.88\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n\n## Categorical predictor, emotion\n\nNow we have 6 coefficients. As in standard linear models, by default, categorical predictors are transformed into dummy variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunique(model.matrix(~ emotion_lbl, data = dat))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   (Intercept) emotion_lbldisgust emotion_lblfear emotion_lblhappiness\n1            1                  0               1                    0\n2            1                  1               0                    0\n3            1                  0               0                    1\n8            1                  0               0                    0\n15           1                  0               0                    0\n42           1                  0               0                    0\n   emotion_lblsadness emotion_lblsurprise\n1                   0                   0\n2                   0                   0\n3                   0                   0\n8                   1                   0\n15                  0                   1\n42                  0                   0\n```\n\n\n:::\n:::\n\n\nThe intercept is the reference level (in this case `anger`) and the other 5 coefficients represent the comparison between all emotions vs anger.\n\n## Categorical predictor, emotion {.smaller}\n\nRemember that we are working on the link-function space. For this reason comparisons are expressed in logit. For example, $\\beta_1$ (`emotion_lbldisgust`) is the comparison between `disgust` and `anger`. Formally:\n\n$$\n\\beta_1 = \\mbox{logit}(p(y = 1 | \\mbox{disgust})) - \\mbox{logit}(p(y = 1 | \\mbox{anger}))\n$$\n\nBut the logit is the logarithm of the odds (let's call $p_a$ and $p_d$ anger and disgust respectively)\n\n$$\n\\log{\\frac{p_d}{1 - p_d}} - \\log{\\frac{p_a}{1 - p_a}}\n$$\n\nA difference of logs can be expressed as the log of the ratio:\n\n$$\n\\log{\\frac{\\frac{p_d}{1 - p_d}}{\\frac{p_a}{1 - p_a}}}\n$$\n\nFinally we can take the exponential to remove the log:\n\n$$\ne^{\\log{\\frac{\\frac{p_d}{1 - p_d}}{\\frac{p_a}{1 - p_a}}}} = \\frac{\\frac{p_d}{1 - p_d}}{\\frac{p_a}{1 - p_a}}\n$$\n\nThis is exactly the odds ratio! This means that taking the exponential of $\\beta_1$ returns the estimated odds ratio for that comparison.\n\n## Categorical predictor\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(fit_em)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         (Intercept)   emotion_lbldisgust      emotion_lblfear \n         -0.69314718           0.07410797          -2.25129179 \nemotion_lblhappiness   emotion_lblsadness  emotion_lblsurprise \n          1.47330574           0.75983856           2.07944154 \n```\n\n\n:::\n\n```{.r .cell-code}\nexp(coef(fit_em))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         (Intercept)   emotion_lbldisgust      emotion_lblfear \n           0.5000000            1.0769231            0.1052632 \nemotion_lblhappiness   emotion_lblsadness  emotion_lblsurprise \n           4.3636364            2.1379310            8.0000000 \n```\n\n\n:::\n:::\n\n\nComparing with the manual calculation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_d <- mean(dat$acc[dat$emotion_lbl == \"disgust\"])\np_a <- mean(dat$acc[dat$emotion_lbl == \"anger\"])\n\nor(p_d, p_a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.076923\n```\n\n\n:::\n:::\n\n\n## Categorical predictor, main effect of `emotion`\n\nWe can also assess the effect of `emotion_lbl` using a Likelihood Ratio Test (LRT). Basically we can compare the model with or without the `emotion_lbl` predictor. Using the LRT we are setting the effect of `emotion_lbl` to zero. This means that the null hypothesis is that all possible contrasts among emotions are zero. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(fit0, fit_em) # comparing two models\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table\n\nModel 1: acc ~ 1\nModel 2: acc ~ emotion_lbl\n  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    \n1       369     512.83                          \n2       364     423.88  5   88.955 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\ncar::Anova(fit_em)  # using the car::Anova\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table (Type II tests)\n\nResponse: acc\n            LR Chisq Df Pr(>Chisq)    \nemotion_lbl   88.955  5  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n## Categorical predictor, specific contrasts of `emotion` levels\n\nWe can also test some specific contrasts using the `emmeans` or the `multcomp` package. For example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(emmeans)\nmm <- emmeans(fit_em, ~ emotion_lbl)\nmm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n emotion_lbl  emmean    SE  df asymp.LCL asymp.UCL\n anger       -0.6931 0.387 Inf    -1.452    0.0659\n disgust     -0.6190 0.210 Inf    -1.030   -0.2081\n fear        -2.9444 0.725 Inf    -4.366   -1.5226\n happiness    0.7802 0.257 Inf     0.276    1.2848\n sadness      0.0667 0.258 Inf    -0.440    0.5730\n surprise     1.3863 0.299 Inf     0.801    1.9719\n\nResults are given on the logit (not the response) scale. \nConfidence level used: 0.95 \n```\n\n\n:::\n:::\n\n\nThese are called **e**stimated **m**rginal **means**. Importantly, the `emmeans` package use the model and not the data. Marginal means will depends on the fitted model.\n\n## Categorical predictor, specific contrasts of `emotion` levels\n\nWe would expect estimated probabilities but we have values in logit (this is why we have negative values). We can also transform the logit into probabilities:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(fit_em, ~ emotion_lbl, type = \"response\")\n```\n:::\n\n\n<pre><code> emotion_lbl  prob     SE  df asymp.LCL asymp.UCL\n anger       0.333 0.0861 Inf    0.1897     0.516\n disgust     0.350 0.0477 Inf    0.2631     0.448\n fear        0.050 0.0345 Inf    0.0125     0.179\n happiness   0.686 0.0555 Inf    0.5685     0.783\n sadness     0.517 0.0645 Inf    0.3918     0.639\n surprise    0.800 0.0478 Inf    0.6901     0.878\n\nConfidence level used: 0.95 \n<span class='hg'>Intervals are back-transformed from the logit scale </span>\n</code></pre>\n\n## What `emmeans` is doing?\n\nTo understand what `emmeans` is doing we need to introduce the term prediction. Given the predictors we ask the model the predicted logit or probability.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# prediction for the first 5 trials. \n# The best prediction of the model is the (logit) mean\n\nhead(predict(fit0, type = \"link\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          1           2           3           4           5           6 \n-0.03243528 -0.03243528 -0.03243528 -0.03243528 -0.03243528 -0.03243528 \n```\n\n\n:::\n\n```{.r .cell-code}\n# prediction for the first 5 trials. \n# the prediction depend on the emotion\n\nhead(predict(fit_em, type = \"link\")) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         1          2          3          4          5          6 \n-2.9444390 -0.6190392  0.7801586  0.7801586 -0.6190392 -2.9444390 \n```\n\n\n:::\n\n```{.r .cell-code}\nhead(predict(fit_em, type = \"response\")) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1         2         3         4         5         6 \n0.0500000 0.3500000 0.6857143 0.6857143 0.3500000 0.0500000 \n```\n\n\n:::\n\n```{.r .cell-code}\nhead(plogis(predict(fit_em, type = \"link\"))) # same\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1         2         3         4         5         6 \n0.0500000 0.3500000 0.6857143 0.6857143 0.3500000 0.0500000 \n```\n\n\n:::\n:::\n\n\n## What `emmeans` is doing?\n\nFor example, to know what is the predicted accuracy for `anger` and `disgust` we can do:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit_em, newdata = data.frame(emotion_lbl = c(\"anger\", \"disgust\")), type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1         2 \n0.3333333 0.3500000 \n```\n\n\n:::\n:::\n\n\nOr manually:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nB <- coef(fit_em)\nc(\n    plogis(B[\"(Intercept)\"]), # anger\n    plogis(B[\"(Intercept)\"] + B[\"emotion_lbldisgust\"]) # disgust\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) (Intercept) \n  0.3333333   0.3500000 \n```\n\n\n:::\n:::\n\n\nOn the logit scale, we can do linear combinations of coefficients. This is not valid on the probability scale, this is the reason why we need the link function.\n\n## What `emmeans` is doing?\n\nFor reproducing the entire `emmeans` output we just need to provide all emotions into `newdata = `\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnd <- data.frame(emotion_lbl = unique(dat$emotion_lbl))\ndata.frame(predict(fit_em, newdata = nd, se = TRUE, type = \"response\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        fit     se.fit residual.scale\n1 0.0500000 0.03445835              1\n2 0.3500000 0.04769696              1\n3 0.6857143 0.05548619              1\n4 0.5166667 0.06451385              1\n5 0.8000000 0.04780914              1\n6 0.3333333 0.08606630              1\n```\n\n\n:::\n:::\n\n\n## Contrasts with `emmeans`\n\nWe can also compute all contrasts across emotions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# or emmeans(fit_em, pairwise ~ emotion_lbl)\npairs(mm, p.adjust = \"bonferroni\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast             estimate    SE  df z.ratio p.value\n anger - disgust       -0.0741 0.440 Inf  -0.168  1.0000\n anger - fear           2.2513 0.822 Inf   2.738  0.0680\n anger - happiness     -1.4733 0.465 Inf  -3.168  0.0192\n anger - sadness       -0.7598 0.466 Inf  -1.632  0.5771\n anger - surprise      -2.0794 0.489 Inf  -4.251  0.0003\n disgust - fear         2.3254 0.755 Inf   3.079  0.0253\n disgust - happiness   -1.3992 0.332 Inf  -4.214  0.0004\n disgust - sadness     -0.6857 0.333 Inf  -2.061  0.3080\n disgust - surprise    -2.0053 0.365 Inf  -5.494  <.0001\n fear - happiness      -3.7246 0.770 Inf  -4.839  <.0001\n fear - sadness        -3.0111 0.770 Inf  -3.910  0.0013\n fear - surprise       -4.3307 0.785 Inf  -5.520  <.0001\n happiness - sadness    0.7135 0.365 Inf   1.956  0.3679\n happiness - surprise  -0.6061 0.394 Inf  -1.537  0.6404\n sadness - surprise    -1.3196 0.395 Inf  -3.341  0.0108\n\nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: tukey method for comparing a family of 6 estimates \n```\n\n\n:::\n:::\n\n\nBe careful to the multiple comparison approach! You can use the `p.adjust =` argument and choose an appropriate method.\n\n## Contrasts with `emmeans`\n\nSome of these contrasts are also the model parameters:\n\n<pre><code> contrast             estimate    SE  df z.ratio p.value\n<span class='hg'> anger - disgust       -0.0741 0.440 Inf  -0.168  1.0000</span>\n<span class='hg'> anger - fear           2.2513 0.822 Inf   2.738  0.0680</span>\n<span class='hg'> anger - happiness     -1.4733 0.465 Inf  -3.168  0.0192</span>\n<span class='hg'> anger - sadness       -0.7598 0.466 Inf  -1.632  0.5771</span>\n<span class='hg'> anger - surprise      -2.0794 0.489 Inf  -4.251  0.0003</span>\n disgust - fear         2.3254 0.755 Inf   3.079  0.0253\n disgust - happiness   -1.3992 0.332 Inf  -4.214  0.0004\n disgust - sadness     -0.6857 0.333 Inf  -2.061  0.3080\n disgust - surprise    -2.0053 0.365 Inf  -5.494  &lt;.0001\n fear - happiness      -3.7246 0.770 Inf  -4.839  &lt;.0001\n fear - sadness        -3.0111 0.770 Inf  -3.910  0.0013\n fear - surprise       -4.3307 0.785 Inf  -5.520  &lt;.0001\n happiness - sadness    0.7135 0.365 Inf   1.956  0.3679\n happiness - surprise  -0.6061 0.394 Inf  -1.537  0.6404\n sadness - surprise    -1.3196 0.395 Inf  -3.341  0.0108\n\nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: tukey method for comparing a family of 6 estimates \n</code></pre>\n\n## Contrasts with `emmeans`\n\nYou can also express the contrasts into the probability space. We are just taking the exponential thus transforming differences of logit into odds ratios.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(mm, type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast             odds.ratio     SE  df null z.ratio p.value\n anger / disgust          0.9286 0.4090 Inf    1  -0.168  1.0000\n anger / fear             9.5000 7.8100 Inf    1   2.738  0.0680\n anger / happiness        0.2292 0.1070 Inf    1  -3.168  0.0192\n anger / sadness          0.4677 0.2180 Inf    1  -1.632  0.5771\n anger / surprise         0.1250 0.0611 Inf    1  -4.251  0.0003\n disgust / fear          10.2308 7.7300 Inf    1   3.079  0.0253\n disgust / happiness      0.2468 0.0819 Inf    1  -4.214  0.0004\n disgust / sadness        0.5037 0.1680 Inf    1  -2.061  0.3080\n disgust / surprise       0.1346 0.0491 Inf    1  -5.494  <.0001\n fear / happiness         0.0241 0.0186 Inf    1  -4.839  <.0001\n fear / sadness           0.0492 0.0379 Inf    1  -3.910  0.0013\n fear / surprise          0.0132 0.0103 Inf    1  -5.520  <.0001\n happiness / sadness      2.0411 0.7440 Inf    1   1.956  0.3679\n happiness / surprise     0.5455 0.2150 Inf    1  -1.537  0.6404\n sadness / surprise       0.2672 0.1060 Inf    1  -3.341  0.0108\n\nP value adjustment: tukey method for comparing a family of 6 estimates \nTests are performed on the log odds ratio scale \n```\n\n\n:::\n:::\n\n\nNotice that: *Tests are performed on the log odds ratio scale*\n\n## Custom contrasts\n\nClearly you can also provide custom contrasts like `contr.sum()` or `MASS::contr.sdiff()` (for comparing the next level with the previous level). For an overview about contrasts coding see @Granziol2025-sy and @Schad2020-ht.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrast(mm, \"consec\") # consec ~ MASS::contr.sdif()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast            estimate    SE  df z.ratio p.value\n disgust - anger       0.0741 0.440 Inf   0.168  0.9999\n fear - disgust       -2.3254 0.755 Inf  -3.079  0.0091\n happiness - fear      3.7246 0.770 Inf   4.839  <.0001\n sadness - happiness  -0.7135 0.365 Inf  -1.956  0.1972\n surprise - sadness    1.3196 0.395 Inf   3.341  0.0039\n\nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: mvt method for 5 tests \n```\n\n\n:::\n\n```{.r .cell-code}\n# see ?contrast\n```\n:::\n\n\n## References",
    "supporting": [
      "binomial-glm_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
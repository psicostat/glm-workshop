{
  "hash": "aaea470605d6122ba90895ba6a8f562d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Binomial GLM example\nsubtitle: Generalized Linear Models Workshop\nformat: minimal-revealjs\nexecute: \n  echo: true\ndate: last-modified\ndate-format: \"*[Last modified:] DD-MM-YYYY*\"\ntoc: true\n---\n\n\n\n\n\n\n\n# A dataset example\n\n## Example: @Shimizu2024-xl\n\n```{=html}\n<style>\n    .hg {\n        background-color: yellow;\n    }\n</style>\n```\n\n@Shimizu2024-xl investigated the processing of emotional faces.\n\n- 6 basic emotions: anger, disgust, fear, happiness, sadness and surprise\n- intensity in % (from 10% to 100% in steps of 10%)\n- 71 participants\n- 377 faces (males and females of different identities)\n- forced-choice procedure with 7 options (6 emotions + neutral). Chance level at $1/7 = 0.14$.\n\n## @Shimizu2024-xl dataset\n\nWe did some pre-processing for the purpose of this example. The original dataset can be found at {{< ai osf >}} [osf.io/zhtbj/](https://osf.io/zhtbj/).\n\nYou can download the dataset for this example at this [link](../data/shimizu2024.rds). It is an `rds` file and you can open it using:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat <- readRDS(\"shimizu.rds\")\n```\n:::\n\n\nThen we can load some packages:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(lme4)      # for mixed-models\nlibrary(tidyverse) # for data manipulation\nlibrary(ggplot2)   # plotting\n```\n:::\n\n\n## Exploring\n\nFor the purpose if this workshop, we will focus on a single subject (otherwise we should use a mixed-effects model). We also select only the relevant columns.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat <- subset(dat, id == 22)\ndat <- dat[, c(\"id\", \"age\", \"intensity\", \"emotion_lbl\", \"response_lbl\", \"acc\")]\ndat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 377 × 6\n      id age   intensity emotion_lbl response_lbl   acc\n   <int> <chr>     <dbl> <chr>       <chr>        <int>\n 1    22 53           60 fear        suprise          0\n 2    22 53           60 disgust     disgust          1\n 3    22 53           70 happiness   happiness        1\n 4    22 53          100 happiness   happiness        1\n 5    22 53           60 disgust     sadness          0\n 6    22 53           20 fear        neutral          0\n 7    22 53           10 fear        neutral          0\n 8    22 53          100 sadness     sadness          1\n 9    22 53           90 disgust     disgust          1\n10    22 53           40 happiness   happiness        1\n# ℹ 367 more rows\n```\n\n\n:::\n:::\n\n\n## Exploring\n\n- We have 377 trials and 6 columns. \n- The `intensity` is the intensity (from 10% to 100%) of the facial expression. `emotion_lbl` is the emotion and `response_lbl` is the response. \n- When `emotion_lbl = response_lbl` the `acc = 1` namely a correct response.\n\n## Exploring\n\nWe can calculate the average accuracy for each emotion. Clearly there is a big difference with `fear` being the hardest one and `surprise` the easiest. We remove `neutral` because we have no associated intensity\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat |> \n  group_by(emotion_lbl) |> \n  summarise(p = mean(acc),\n            n = n()) |> \n  arrange(desc(p))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 3\n  emotion_lbl     p     n\n  <chr>       <dbl> <int>\n1 neutral     1         7\n2 surprise    0.8      70\n3 happiness   0.686    70\n4 sadness     0.517    60\n5 disgust     0.35    100\n6 anger       0.333    30\n7 fear        0.05     40\n```\n\n\n:::\n\n```{.r .cell-code}\ndat <- filter(dat, emotion_lbl != \"neutral\")\n```\n:::\n\n\n\n\n## Exploring\n\nAlso for intensity, there is a clear increasing pattern. In this specific subject, the highest intensities are not clearly distinguished (maybe a non-linear pattern).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat |> \n  group_by(intensity) |> \n  summarise(p = mean(acc),\n            n = n()) |> \n  arrange(desc(p))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 3\n   intensity      p     n\n       <dbl>  <dbl> <int>\n 1        70 0.703     37\n 2        90 0.676     37\n 3       100 0.676     37\n 4        80 0.649     37\n 5        50 0.622     37\n 6        60 0.622     37\n 7        40 0.486     37\n 8        30 0.324     37\n 9        10 0.0811    37\n10        20 0.0811    37\n```\n\n\n:::\n:::\n\n\n## Exploring, plots\n\n\n::: {.cell layout-align=\"center\" output-location='slide'}\n\n```{.r .cell-code}\ndat |> \n  group_by(emotion_lbl) |> \n  summarise(p = mean(acc),\n            n = n()) |> \n  ggplot(aes(x = fct_reorder(emotion_lbl, p), y = p)) + \n  geom_point(size = 4) +\n  geom_line(group = 1) +\n  ylim(c(0, 1)) +\n  xlab(\"Emotion\") +\n  ylab(\"Accuracy\") +\n  geom_hline(yintercept = 1/7, lty = \"dotted\")\n```\n\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-7-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Exploring, plots\n\n\n::: {.cell layout-align=\"center\" output-location='slide'}\n\n```{.r .cell-code}\ndat |> \n  group_by(intensity) |> \n  summarise(p = mean(acc),\n            n = n()) |> \n  arrange(desc(p)) |> \n  ggplot(aes(x = intensity, y = p)) + \n  geom_point(size = 4) +\n  geom_line() +\n  ylim(c(0, 1)) +\n  xlab(\"Intensity (%)\") +\n  ylab(\"Accuracy\") +\n  geom_hline(yintercept = 1/7, lty = \"dotted\")\n```\n\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-8-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Exploring, plots\n\nWe have few trials but we can also explore the interaction between emotion and intensity. There are some emotions where the rate of increase in accuracy as a function of the emotion is faster compared to others.\n\n\n::: {.cell layout-align=\"center\" output-location='slide'}\n\n```{.r .cell-code}\ndat |> \n  group_by(intensity, emotion_lbl) |> \n  summarise(p = mean(acc)) |> \n  ggplot(aes(x = intensity, y = p, color = emotion_lbl)) +\n  geom_point(size = 4) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-9-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Exploring, odds and odds ratios\n\nWe can start exploring the effects calcualting odds and odds ratios.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nodds <- function(p) p / (1 - p)\nor <- function(pn, pd) odds(pn) / odds(pd)\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(p_anger <- mean(dat$acc[dat$emotion_lbl == \"anger\"]))\n## [1] 0.3333333\n(p_surprise <- mean(dat$acc[dat$emotion_lbl == \"surprise\"]))\n## [1] 0.8\n\nodds(p_anger)\n## [1] 0.5\nodds(p_surprise)\n## [1] 4\n\nor(p_surprise, p_anger)\n## [1] 8\n```\n:::\n\n\n## Exploring, odds and odds ratios\n\nWe can also create a contingency table:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntable(dat$acc, dat$emotion_lbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   \n    anger disgust fear happiness sadness surprise\n  0    20      65   38        22      29       14\n  1    10      35    2        48      31       56\n```\n\n\n:::\n\n```{.r .cell-code}\n(all_p <- tapply(dat$acc, dat$emotion_lbl, FUN = mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    anger   disgust      fear happiness   sadness  surprise \n0.3333333 0.3500000 0.0500000 0.6857143 0.5166667 0.8000000 \n```\n\n\n:::\n\n```{.r .cell-code}\nodds(all_p)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     anger    disgust       fear  happiness    sadness   surprise \n0.50000000 0.53846154 0.05263158 2.18181818 1.06896552 4.00000000 \n```\n\n\n:::\n:::\n\n\nWhen the odds are lower than 1, the probability of success is lower than the probability of failure. When the odds are greater than 1 the probability of success is higher.\n\n## Exploring, odds and odds ratios\n\nWe can also calculate all the possible comparisons. Note that depending on the numerator/denominator the odds ratio is different but we can simply take the inverse to switch numerator and numerator. Interpreting odds ratio > 1 is usually more intutive.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n     anger / disgust         anger / fear    anger / happiness \n               0.929                9.500                0.229 \n     anger / sadness     anger / surprise       disgust / fear \n               0.468                0.125               10.231 \n disgust / happiness    disgust / sadness   disgust / surprise \n               0.247                0.504                0.135 \n    fear / happiness       fear / sadness      fear / surprise \n               0.024                0.049                0.013 \n happiness / sadness happiness / surprise   sadness / surprise \n               2.041                0.545                0.267 \n```\n\n\n:::\n:::\n\n\nFor example, `happiness / sadness ~ 2.04` means that the odds (not the probability) of a correct response is 2 times higher for happy faces compared to sad faces. \n\n# Model\n\n## The `glm` function\n\nIn R we can fit a GLM with the `glm` function. The syntax is the same as the `lm` (for standard linear models). We only need to specify the **random component** and the **link function**.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglm(y ~ x1 + x2 + x3 * x4, # systematic component (linear predictor)\n    data = data,\n    family = binomial(link = \"logit\")) # random component and link function\n```\n:::\n\n\nClearly, the `y` in this example need to be consistent with the chosen family. In this case the model is expecting a 0-1 vector. If we provide labels (characters) or number > 1 or < 0 the function will fail.\n\n::: {.callout-note}\nA `glm` with `family = gaussian(link = \"identity\")` is the same as running a `lm`. Internally `glm` calls `lm` in this case.\n:::\n\n## The null model\n\nWe can start with the easiest model that is a model without the systematic component (with no predictors).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit0 <- glm(acc ~ 1, data = dat, family = binomial(link = \"logit\"))\nsummary(fit0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = acc ~ 1, family = binomial(link = \"logit\"), data = dat)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept) -0.03244    0.10399  -0.312    0.755\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 512.83  on 369  degrees of freedom\nResidual deviance: 512.83  on 369  degrees of freedom\nAIC: 514.83\n\nNumber of Fisher Scoring iterations: 3\n```\n\n\n:::\n:::\n\n\n## The null model\n\n![](img/fit0_summary.png)\n\n## The null model, formally\n\n$$\n\\eta_i = \\beta_0\n$$\n\n$$\np_i = g^{-1}(\\eta_i) = g^{-1}(\\beta_0)\n$$\n\n$g^{-1}(\\cdot)$ is the inverse-logit link:\n\n$$\np_i = \\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}\n$$\n\nIn other terms, the probability can be calculated inverting the logit link function evaluated on the linear predictor $\\eta$. In this case $\\eta$ only contains $\\beta_0$.\n\n## The null model, interpretation\n\nIn this case, the intercept is -0.032. The intercept is the expected value (i.e., the mean) of `y` (accuracy here) when everthing is zero. In this case $\\beta_0$ is just the (logit transformed) overall accuracy.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nb0 <- coef(fit0)[\"(Intercept)\"]\nexp(b0) / (1 + exp(b0)) # inverse logit\n## (Intercept) \n##   0.4918919\nplogis(b0)              # directly with the dedicated function\n## (Intercept) \n##   0.4918919\nmean(dat$acc)           # average accuracy\n## [1] 0.4918919\nlog(odds(mean(dat$acc))) # probability to logit\n## [1] -0.03243528\nqlogis(mean(dat$acc)) # probability to logit with the dedicated function\n## [1] -0.03243528\n```\n:::\n\n\n## Categorical predictor, emotion\n\nThen we can include `emotion_lbl` as predictor. Let's see what happens:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_em <- glm(acc ~ emotion_lbl, data = dat, family = binomial(link = \"logit\"))\nsummary(fit_em)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = acc ~ emotion_lbl, family = binomial(link = \"logit\"), \n    data = dat)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)          -0.69315    0.38730  -1.790  0.07350 .  \nemotion_lbldisgust    0.07411    0.44040   0.168  0.86637    \nemotion_lblfear      -2.25129    0.82235  -2.738  0.00619 ** \nemotion_lblhappiness  1.47331    0.46507   3.168  0.00154 ** \nemotion_lblsadness    0.75984    0.46555   1.632  0.10266    \nemotion_lblsurprise   2.07944    0.48917   4.251 2.13e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 512.83  on 369  degrees of freedom\nResidual deviance: 423.88  on 364  degrees of freedom\nAIC: 435.88\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n\n## Categorical predictor, emotion\n\nNow we have 6 coefficients. As in standard linear models, by default, categorical predictors are transformed into dummy variables:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nunique(model.matrix(~ emotion_lbl, data = dat))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   (Intercept) emotion_lbldisgust emotion_lblfear emotion_lblhappiness\n1            1                  0               1                    0\n2            1                  1               0                    0\n3            1                  0               0                    1\n8            1                  0               0                    0\n15           1                  0               0                    0\n42           1                  0               0                    0\n   emotion_lblsadness emotion_lblsurprise\n1                   0                   0\n2                   0                   0\n3                   0                   0\n8                   1                   0\n15                  0                   1\n42                  0                   0\n```\n\n\n:::\n:::\n\n\nThe intercept is the reference level (in this case `anger`) and the other 5 coefficients represent the comparison between all emotions vs anger.\n\n## Categorical predictor, emotion {.smaller}\n\nRemember that we are working on the link-function space where comparisons are expressed in logit. $\\beta_1$ (`emotion_lbldisgust`) is the comparison between `disgust` and `anger`. Formally:\n\n$$\n\\beta_1 = \\mbox{logit}(p(y = 1 | \\mbox{disgust})) - \\mbox{logit}(p(y = 1 | \\mbox{anger}))\n$$\n\nBut the logit is the logarithm of the odds (let's call $p_a$ and $p_d$ anger and disgust respectively)\n\n$$\n\\log{\\frac{p_d}{1 - p_d}} - \\log{\\frac{p_a}{1 - p_a}}\n$$\n\nA difference of logs can be expressed as the log of the ratio:\n\n$$\n\\log{\\frac{\\frac{p_d}{1 - p_d}}{\\frac{p_a}{1 - p_a}}}\n$$\n\nFinally we can take the exponential to remove the log:\n\n$$\ne^{\\log{\\frac{\\frac{p_d}{1 - p_d}}{\\frac{p_a}{1 - p_a}}}} = \\frac{\\frac{p_d}{1 - p_d}}{\\frac{p_a}{1 - p_a}}\n$$\n\nThis is exactly the odds ratio! This means that taking the exponential of $\\beta_1$ returns the estimated odds ratio for that comparison.\n\n## Categorical predictor\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncoef(fit_em)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         (Intercept)   emotion_lbldisgust      emotion_lblfear \n         -0.69314718           0.07410797          -2.25129179 \nemotion_lblhappiness   emotion_lblsadness  emotion_lblsurprise \n          1.47330574           0.75983856           2.07944154 \n```\n\n\n:::\n\n```{.r .cell-code}\nexp(coef(fit_em))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         (Intercept)   emotion_lbldisgust      emotion_lblfear \n           0.5000000            1.0769231            0.1052632 \nemotion_lblhappiness   emotion_lblsadness  emotion_lblsurprise \n           4.3636364            2.1379310            8.0000000 \n```\n\n\n:::\n:::\n\n\nComparing with the manual calculation:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np_d <- mean(dat$acc[dat$emotion_lbl == \"disgust\"])\np_a <- mean(dat$acc[dat$emotion_lbl == \"anger\"])\n\nor(p_d, p_a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.076923\n```\n\n\n:::\n:::\n\n\n## Main effect of `emotion`\n\nWe can also assess the effect of `emotion_lbl` using a Likelihood Ratio Test (LRT). Basically we can compare the model with or without the `emotion_lbl` predictor. Using the LRT we are setting the effect of `emotion_lbl` to zero. This means that the null hypothesis is that all possible contrasts among emotions are zero. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nanova(fit0, fit_em) # comparing two models\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table\n\nModel 1: acc ~ 1\nModel 2: acc ~ emotion_lbl\n  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    \n1       369     512.83                          \n2       364     423.88  5   88.955 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\ncar::Anova(fit_em)  # using the car::Anova\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table (Type II tests)\n\nResponse: acc\n            LR Chisq Df Pr(>Chisq)    \nemotion_lbl   88.955  5  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n## Confidence intervals\n\nThe confidence intervals for model parameters can be extracted with the `confint.default()` function. These are called Wald confidence intervals. They are computed as:\n\n$$\n\\beta \\pm q_{\\alpha/2} \\mbox{SE}_\\beta\n$$\n\nWhere $q$ is the critical test statistics ($z$ in this case) at $\\alpha$ level and $\\mbox{SE}_\\beta$ is the standard error.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfits <- summary(fit_em)$coefficients\nfits\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                        Estimate Std. Error    z value     Pr(>|z|)\n(Intercept)          -0.69314718  0.3872983 -1.7896983 0.0735024219\nemotion_lbldisgust    0.07410797  0.4404044  0.1682725 0.8663688694\nemotion_lblfear      -2.25129179  0.8223512 -2.7376280 0.0061884033\nemotion_lblhappiness  1.47330574  0.4650676  3.1679388 0.0015352381\nemotion_lblsadness    0.75983856  0.4655543  1.6321158 0.1026550954\nemotion_lblsurprise   2.07944154  0.4891684  4.2509728 0.0000212844\n```\n\n\n:::\n:::\n\n\n## Confidence intervals\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(z <- abs(qnorm(0.05/2))) # critical test statistics at alpha/2 (two tails)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.959964\n```\n\n\n:::\n\n```{.r .cell-code}\ndata.frame(\n  lower = fits[, \"Estimate\"] - z * fits[, \"Std. Error\"],\n  upper = fits[, \"Estimate\"] + z * fits[, \"Std. Error\"]\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                          lower       upper\n(Intercept)          -1.4522380  0.06594361\nemotion_lbldisgust   -0.7890688  0.93728475\nemotion_lblfear      -3.8630706 -0.63951297\nemotion_lblhappiness  0.5617900  2.38482150\nemotion_lblsadness   -0.1526311  1.67230825\nemotion_lblsurprise   1.1206891  3.03819397\n```\n\n\n:::\n:::\n\n\n\n## Confidence intervals\n\nOr directly using the `confint.default()`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconfint.default(fit_em)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                          2.5 %      97.5 %\n(Intercept)          -1.4522380  0.06594361\nemotion_lbldisgust   -0.7890688  0.93728475\nemotion_lblfear      -3.8630706 -0.63951297\nemotion_lblhappiness  0.5617900  2.38482150\nemotion_lblsadness   -0.1526311  1.67230825\nemotion_lblsurprise   1.1206891  3.03819397\n```\n\n\n:::\n:::\n\n\n## Confidence intervals\n\nBut these are confidence intervals of log odds ratios (differences in logit). To obtain confidence intervals of odds ratios we can just exponentiate the upper and lower bound:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nexp(confint.default(fit_em))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                          2.5 %     97.5 %\n(Intercept)          0.23404591  1.0681665\nemotion_lbldisgust   0.45426761  2.5530399\nemotion_lblfear      0.02100341  0.5275493\nemotion_lblhappiness 1.75380897 10.8571245\nemotion_lblsadness   0.85844631  5.3244438\nemotion_lblsurprise  3.06696696 20.8675218\n```\n\n\n:::\n:::\n\n\nNotice that, for differences in logit the *null* value is zero. Taking $e^0 = 1$ thus the *null* value of an odds ratio is 1 (numerator is the same as the denominator).\n\n## Confidence intervals\n\nThe real default for confidence intervals using just `confint()` (and not `confint.default()`) are the so-called profile likelihood confindence intervals. The main difference is that Wald confidence intervals are symmetric by definition while the profile likelihood not necessarly.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconfint(fit_em)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                          2.5 %      97.5 %\n(Intercept)          -1.4942114  0.04295714\nemotion_lbldisgust   -0.7729310  0.96810000\nemotion_lblfear      -4.1872943 -0.80517570\nemotion_lblhappiness  0.5830921  2.41838129\nemotion_lblsadness   -0.1359529  1.70183598\nemotion_lblsurprise   1.1479829  3.07708494\n```\n\n\n:::\n\n```{.r .cell-code}\n# you can also do exp(confint(fit_em))\n```\n:::\n\n\nIn this case they are very similar to the Wald but is not always like this.\n\n::: aside\nSee this [post](https://thestatsgeek.com/2014/02/08/wald-vs-likelihood-ratio-test/) for a nice overview.\n:::\n\n## Specific contrasts of `emotion` levels\n\nWe can also test some specific contrasts using the `emmeans` or the `multcomp` package. For example:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(emmeans)\nmm <- emmeans(fit_em, ~ emotion_lbl)\nmm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n emotion_lbl  emmean    SE  df asymp.LCL asymp.UCL\n anger       -0.6931 0.387 Inf    -1.452    0.0659\n disgust     -0.6190 0.210 Inf    -1.030   -0.2081\n fear        -2.9444 0.725 Inf    -4.366   -1.5226\n happiness    0.7802 0.257 Inf     0.276    1.2848\n sadness      0.0667 0.258 Inf    -0.440    0.5730\n surprise     1.3863 0.299 Inf     0.801    1.9719\n\nResults are given on the logit (not the response) scale. \nConfidence level used: 0.95 \n```\n\n\n:::\n:::\n\n\nThese are called **e**stimated **m**rginal **means**. Importantly, the `emmeans` package use the model and not the data. Marginal means will depends on the fitted model.\n\n## Specific contrasts of `emotion` levels\n\nWe would expect estimated probabilities but we have values in logit (this is why we have negative values). We can also transform the logit into probabilities:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nemmeans(fit_em, ~ emotion_lbl, type = \"response\")\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n<pre><code> emotion_lbl  prob     SE  df asymp.LCL asymp.UCL\n anger       0.333 0.0861 Inf    0.1897     0.516\n disgust     0.350 0.0477 Inf    0.2631     0.448\n fear        0.050 0.0345 Inf    0.0125     0.179\n happiness   0.686 0.0555 Inf    0.5685     0.783\n sadness     0.517 0.0645 Inf    0.3918     0.639\n surprise    0.800 0.0478 Inf    0.6901     0.878\n\nConfidence level used: 0.95 \n<span class='hg'>Intervals are back-transformed from the logit scale </span>\n</code></pre>\n:::\n\n\n## What `emmeans` is doing?\n\nTo understand what `emmeans` is doing we need to introduce the term prediction. Given the predictors we ask the model the predicted logit or probability.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# prediction for the first 5 trials. \n# The best prediction of the model is the (logit) mean\n\nhead(predict(fit0, type = \"link\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          1           2           3           4           5           6 \n-0.03243528 -0.03243528 -0.03243528 -0.03243528 -0.03243528 -0.03243528 \n```\n\n\n:::\n\n```{.r .cell-code}\n# prediction for the first 5 trials. \n# the prediction depend on the emotion\n\nhead(predict(fit_em, type = \"link\")) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         1          2          3          4          5          6 \n-2.9444390 -0.6190392  0.7801586  0.7801586 -0.6190392 -2.9444390 \n```\n\n\n:::\n\n```{.r .cell-code}\nhead(predict(fit_em, type = \"response\")) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1         2         3         4         5         6 \n0.0500000 0.3500000 0.6857143 0.6857143 0.3500000 0.0500000 \n```\n\n\n:::\n\n```{.r .cell-code}\nhead(plogis(predict(fit_em, type = \"link\"))) # same\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1         2         3         4         5         6 \n0.0500000 0.3500000 0.6857143 0.6857143 0.3500000 0.0500000 \n```\n\n\n:::\n:::\n\n\n## What `emmeans` is doing?\n\nFor example, to know what is the predicted accuracy for `anger` and `disgust` we can do:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(fit_em, newdata = data.frame(emotion_lbl = c(\"anger\", \"disgust\")), type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1         2 \n0.3333333 0.3500000 \n```\n\n\n:::\n:::\n\n\nOr manually:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nB <- coef(fit_em)\nc(\n    plogis(B[\"(Intercept)\"]), # anger\n    plogis(B[\"(Intercept)\"] + B[\"emotion_lbldisgust\"]) # disgust\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) (Intercept) \n  0.3333333   0.3500000 \n```\n\n\n:::\n:::\n\n\nOn the logit scale, we can do linear combinations of coefficients. This is not valid on the probability scale, this is the reason why we need the link function.\n\n## What `emmeans` is doing?\n\nFor reproducing the entire `emmeans` output we just need to provide all emotions into `newdata = `\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnd <- data.frame(emotion_lbl = unique(dat$emotion_lbl))\ndata.frame(predict(fit_em, newdata = nd, se = TRUE, type = \"response\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        fit     se.fit residual.scale\n1 0.0500000 0.03445835              1\n2 0.3500000 0.04769696              1\n3 0.6857143 0.05548619              1\n4 0.5166667 0.06451385              1\n5 0.8000000 0.04780914              1\n6 0.3333333 0.08606630              1\n```\n\n\n:::\n:::\n\n\n## Contrasts with `emmeans`\n\nWe can also compute all contrasts across emotions:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# or emmeans(fit_em, pairwise ~ emotion_lbl)\npairs(mm, p.adjust = \"bonferroni\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast             estimate    SE  df z.ratio p.value\n anger - disgust       -0.0741 0.440 Inf  -0.168  1.0000\n anger - fear           2.2513 0.822 Inf   2.738  0.0680\n anger - happiness     -1.4733 0.465 Inf  -3.168  0.0192\n anger - sadness       -0.7598 0.466 Inf  -1.632  0.5771\n anger - surprise      -2.0794 0.489 Inf  -4.251  0.0003\n disgust - fear         2.3254 0.755 Inf   3.079  0.0253\n disgust - happiness   -1.3992 0.332 Inf  -4.214  0.0004\n disgust - sadness     -0.6857 0.333 Inf  -2.061  0.3080\n disgust - surprise    -2.0053 0.365 Inf  -5.494  <.0001\n fear - happiness      -3.7246 0.770 Inf  -4.839  <.0001\n fear - sadness        -3.0111 0.770 Inf  -3.910  0.0013\n fear - surprise       -4.3307 0.785 Inf  -5.520  <.0001\n happiness - sadness    0.7135 0.365 Inf   1.956  0.3679\n happiness - surprise  -0.6061 0.394 Inf  -1.537  0.6404\n sadness - surprise    -1.3196 0.395 Inf  -3.341  0.0108\n\nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: tukey method for comparing a family of 6 estimates \n```\n\n\n:::\n:::\n\n\nBe careful to the multiple comparison approach! You can use the `p.adjust =` argument and choose an appropriate method.\n\n## Contrasts with `emmeans`\n\nSome of these contrasts are also the model parameters:\n\n\n::: {.cell layout-align=\"center\"}\n<pre><code> contrast             estimate    SE  df z.ratio p.value\n<span class='hg'> anger - disgust       -0.0741 0.440 Inf  -0.168  1.0000</span>\n<span class='hg'> anger - fear           2.2513 0.822 Inf   2.738  0.0680</span>\n<span class='hg'> anger - happiness     -1.4733 0.465 Inf  -3.168  0.0192</span>\n<span class='hg'> anger - sadness       -0.7598 0.466 Inf  -1.632  0.5771</span>\n<span class='hg'> anger - surprise      -2.0794 0.489 Inf  -4.251  0.0003</span>\n disgust - fear         2.3254 0.755 Inf   3.079  0.0253\n disgust - happiness   -1.3992 0.332 Inf  -4.214  0.0004\n disgust - sadness     -0.6857 0.333 Inf  -2.061  0.3080\n disgust - surprise    -2.0053 0.365 Inf  -5.494  &lt;.0001\n fear - happiness      -3.7246 0.770 Inf  -4.839  &lt;.0001\n fear - sadness        -3.0111 0.770 Inf  -3.910  0.0013\n fear - surprise       -4.3307 0.785 Inf  -5.520  &lt;.0001\n happiness - sadness    0.7135 0.365 Inf   1.956  0.3679\n happiness - surprise  -0.6061 0.394 Inf  -1.537  0.6404\n sadness - surprise    -1.3196 0.395 Inf  -3.341  0.0108\n\nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: tukey method for comparing a family of 6 estimates \n</code></pre>\n:::\n\n\n## Contrasts with `emmeans`\n\nYou can also express the contrasts into the probability space. We are just taking the exponential thus transforming differences of logit into odds ratios.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npairs(mm, type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast             odds.ratio     SE  df null z.ratio p.value\n anger / disgust          0.9286 0.4090 Inf    1  -0.168  1.0000\n anger / fear             9.5000 7.8100 Inf    1   2.738  0.0680\n anger / happiness        0.2292 0.1070 Inf    1  -3.168  0.0192\n anger / sadness          0.4677 0.2180 Inf    1  -1.632  0.5771\n anger / surprise         0.1250 0.0611 Inf    1  -4.251  0.0003\n disgust / fear          10.2308 7.7300 Inf    1   3.079  0.0253\n disgust / happiness      0.2468 0.0819 Inf    1  -4.214  0.0004\n disgust / sadness        0.5037 0.1680 Inf    1  -2.061  0.3080\n disgust / surprise       0.1346 0.0491 Inf    1  -5.494  <.0001\n fear / happiness         0.0241 0.0186 Inf    1  -4.839  <.0001\n fear / sadness           0.0492 0.0379 Inf    1  -3.910  0.0013\n fear / surprise          0.0132 0.0103 Inf    1  -5.520  <.0001\n happiness / sadness      2.0411 0.7440 Inf    1   1.956  0.3679\n happiness / surprise     0.5455 0.2150 Inf    1  -1.537  0.6404\n sadness / surprise       0.2672 0.1060 Inf    1  -3.341  0.0108\n\nP value adjustment: tukey method for comparing a family of 6 estimates \nTests are performed on the log odds ratio scale \n```\n\n\n:::\n:::\n\n\nNotice that: *Tests are performed on the log odds ratio scale*\n\n## Custom contrasts\n\nClearly you can also provide custom contrasts like `contr.sum()` or `MASS::contr.sdiff()` (for comparing the next level with the previous level). For an overview about contrasts coding see @Granziol2025-sy and @Schad2020-ht.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncontrast(mm, \"consec\") # consec ~ MASS::contr.sdif()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast            estimate    SE  df z.ratio p.value\n disgust - anger       0.0741 0.440 Inf   0.168  0.9999\n fear - disgust       -2.3254 0.755 Inf  -3.079  0.0090\n happiness - fear      3.7246 0.770 Inf   4.839  <.0001\n sadness - happiness  -0.7135 0.365 Inf  -1.956  0.1974\n surprise - sadness    1.3196 0.395 Inf   3.341  0.0034\n\nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: mvt method for 5 tests \n```\n\n\n:::\n\n```{.r .cell-code}\n# see ?emmeans::contrast\n```\n:::\n\n\nHere we are comparing 2 vs 1, 2 vs 3, 3 vs 4, etc.\n\n## Plotting\n\nThere are several options to plot the model results. In this case we could plot the predicted probability for each emotion and the confidence intervals. You can use the `effects` package or `ggeffects` (that internally uses `effects`) to create `ggplot2` objects.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggeffects)\n# grid of prediction and CI (as we did with emmeans or predict())\nggeffect(fit_em, \"emotion_lbl\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Predicted probabilities of acc\n\nemotion_lbl | Predicted |     95% CI\n------------------------------------\nanger       |      0.33 | 0.19, 0.52\ndisgust     |      0.35 | 0.26, 0.45\nfear        |      0.05 | 0.01, 0.18\nhappiness   |      0.69 | 0.57, 0.78\nsadness     |      0.52 | 0.39, 0.64\nsurprise    |      0.80 | 0.69, 0.88\n```\n\n\n:::\n:::\n\n\n## Plotting\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# this return a ggplot2 object, you can add layers with +\nplot(ggeffect(fit_em, \"emotion_lbl\"))\n```\n\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-39-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Numerical predictor, effect of `intensity`\n\nNow let's fit a model with only the effect of intensity.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_int <- glm(acc ~ intensity, data = dat, family = binomial(link = \"logit\"))\nsummary(fit_int)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = acc ~ intensity, family = binomial(link = \"logit\"), \n    data = dat)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.797692   0.263646  -6.819 9.19e-12 ***\nintensity    0.031976   0.004291   7.452 9.19e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 512.83  on 369  degrees of freedom\nResidual deviance: 447.05  on 368  degrees of freedom\nAIC: 451.05\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n## Numerical predictor, effect of `intensity`\n\nNow the intercept is the logit accuracy when `intensity` is zero (that is not really a meaningful value). $\\beta_1$ here is the expected increase in logit for a unit increase in intensity. In other terms, moving from intensity e.g., 10 to 11 increase the logit of 0.03.\n\nAs for the emotion, we can exponentiate $\\beta_1$ obtaining the odds ratio of increasing 1 unit in intensity:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nexp(coef(fit_int))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)   intensity \n  0.1656809   1.0324926 \n```\n\n\n:::\n:::\n\n\nWe have an odds ratio of 1.03 that is quite low (1 is the null value). But this is a scale problem, 1 point in the intensity scale is meaningless.\n\n## Numerical predictor, effect of `intensity`\n\nBefore improving the model, notice that the story is the same regardless having categorical or numerical predictor. For categorical predictors the coefficients are odds ratios (or difference in logit) comparing levels (`anger` vs `fear`). For numerical predictor the coefficients are odds ratios (or difference in logit) comparing values separated by 1 unit (in the scale of the predictor).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(pp <- predict(fit_int, newdata = data.frame(intensity = c(15, 16))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1         2 \n-1.318054 -1.286078 \n```\n\n\n:::\n\n```{.r .cell-code}\ndiff(pp) # same as b1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         2 \n0.03197586 \n```\n\n\n:::\n\n```{.r .cell-code}\nexp(diff(pp))  # same as exp(b1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       2 \n1.032493 \n```\n\n\n:::\n:::\n\n\n## Numerical predictor, effect of `intensity`\n\nIn fact, we can clearly see that on the logit scale the effect is linear while on the probability scale is not linear.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# pairs of unit differences in different positions of x\ndiffs <- list(c(10, 11), c(50, 51), c(80, 81))\nsapply(diffs, function(d) diff(predict(fit_int, data.frame(intensity = d))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         2          2          2 \n0.03197586 0.03197586 0.03197586 \n```\n\n\n:::\n:::\n\n\nBut is not the same when we take differences in probabilities\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# pairs of unit differences in different positions of x\ndiffs <- list(c(10, 11), c(50, 51), c(80, 81))\nsapply(diffs, function(d) diff(predict(fit_int, data.frame(intensity = d), type = \"response\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          2           2           2 \n0.004884715 0.007927308 0.006900733 \n```\n\n\n:::\n:::\n\n\n## Numerical predictor, effect of `intensity`\n\nSame increase in `intensity` produces a different increase on the probability scale but not on the logit scale. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-45-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Numerical predictor, marginal effects\n\nThis means that for interpreting results in the probability scale (what we actually want) we cannot think in linear terms (as in standard linear regression). For each value of `intensity` we have a different slope (i.e., derivative).\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-46-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Marginal effects\n\nNow let's plot all the red slopes and see what we can learn:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-47-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Marginal effects: a really comprehensive framework\n\nThe `marginaleffects` [package](https://marginaleffects.com/) provide a very complete and comprehensive framework to compute marginal effects for several models. You can have a very detailed overview of the theory and the functions reading:\n\n- The main paper by @Arel-Bundock2024-zl\n- [https://www.youtube.com/watch?v=ANDC_kkAjeM](https://www.youtube.com/watch?v=ANDC_kkAjeM)\n\n## Marginal effects of `intensity`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(marginaleffects)\nsl <- slopes(fit_int)\nsl\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n Estimate Std. Error     z Pr(>|z|)     S   2.5 %  97.5 %\n  0.00796   0.001061  7.51   <0.001  43.9 0.00589 0.01004\n  0.00796   0.001061  7.51   <0.001  43.9 0.00589 0.01004\n  0.00762   0.000938  8.12   <0.001  51.0 0.00578 0.00946\n  0.00507   0.000362 14.01   <0.001 145.7 0.00436 0.00578\n  0.00796   0.001061  7.51   <0.001  43.9 0.00589 0.01004\n--- 360 rows omitted. See ?print.marginaleffects ---\n  0.00484   0.000351 13.77   <0.001 140.8 0.00415 0.00552\n  0.00748   0.000905  8.27   <0.001  52.7 0.00571 0.00925\n  0.00507   0.000362 14.01   <0.001 145.7 0.00436 0.00578\n  0.00694   0.000725  9.57   <0.001  69.7 0.00552 0.00836\n  0.00748   0.000905  8.27   <0.001  52.7 0.00571 0.00925\nTerm: intensity\nType: response\nComparison: dY/dX\n```\n\n\n:::\n:::\n\n\n## Marginal effects of `intensity`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# average marginal effect\navg_slopes(fit_int)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n Estimate Std. Error  z Pr(>|z|)    S   2.5 %  97.5 %\n  0.00664   0.000602 11   <0.001 91.7 0.00546 0.00782\n\nTerm: intensity\nType: response\nComparison: dY/dX\n```\n\n\n:::\n\n```{.r .cell-code}\n# max marginal effect\nfilter(sl, estimate == max(estimate))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n Estimate Std. Error    z Pr(>|z|)    S   2.5 % 97.5 %\n  0.00796    0.00106 7.51   <0.001 43.9 0.00589   0.01\n  0.00796    0.00106 7.51   <0.001 43.9 0.00589   0.01\n  0.00796    0.00106 7.51   <0.001 43.9 0.00589   0.01\n  0.00796    0.00106 7.51   <0.001 43.9 0.00589   0.01\n  0.00796    0.00106 7.51   <0.001 43.9 0.00589   0.01\n--- 27 rows omitted. See ?print.marginaleffects ---\n  0.00796    0.00106 7.51   <0.001 43.9 0.00589   0.01\n  0.00796    0.00106 7.51   <0.001 43.9 0.00589   0.01\n  0.00796    0.00106 7.51   <0.001 43.9 0.00589   0.01\n  0.00796    0.00106 7.51   <0.001 43.9 0.00589   0.01\n  0.00796    0.00106 7.51   <0.001 43.9 0.00589   0.01\nTerm: intensity\nType: response\nComparison: dY/dX\n```\n\n\n:::\n:::\n\n\n## Plotting\n\nThe pattern for intensity is almost linear, this is why we have more than one maximum.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(ggeffect(fit_int, \"intensity\"))\n```\n\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-50-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Inverse estimation\n\nWe can also do what is called inverse estimation (common in Psychophysics). We can ask the model what is the level of `intensity` required to achieve a certain accuracy.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nMASS::dose.p(fit_int, p = 0.75)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Dose       SE\np = 0.75: 90.57784 5.916972\n```\n\n\n:::\n:::\n\n\nWe need roughly 90% of `intensity` to have an accuracy of 75%.\n\n## Improving the model\n\nWe have two problems in terms of intepretability in this model:\n\n- The intercept is meaningless because 0% intensity is not a plausible value\n- Intensity from 0% to 100% in steps of 1% is too granular\n\nThus we can center the variable on the minimum (0 become 10%) and rescale the variable from 0 (10%) to 10 (100%) where the unit increase is 10%.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat$intensity10 <- (dat$intensity - 10) / 10\n```\n:::\n\n\n## Additive model, `intensity` and `emotion`\n\nWe can now fit a model with the additive effect of emotion and intensity. To simplify the pattern let's keep only two emotions.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_int_emo <- glm(acc ~ intensity10 + emotion_lbl, data = dat, family = binomial(link = \"logit\"), subset = emotion_lbl %in% c(\"anger\", \"surprise\"))\n\nsummary(fit_int_emo)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = acc ~ intensity10 + emotion_lbl, family = binomial(link = \"logit\"), \n    data = dat, subset = emotion_lbl %in% c(\"anger\", \"surprise\"))\n\nCoefficients:\n                    Estimate Std. Error z value Pr(>|z|)    \n(Intercept)          -5.2155     1.2367  -4.217 2.47e-05 ***\nintensity10           0.8363     0.1858   4.502 6.73e-06 ***\nemotion_lblsurprise   4.1623     1.0010   4.158 3.21e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 128.207  on 99  degrees of freedom\nResidual deviance:  63.942  on 97  degrees of freedom\nAIC: 69.942\n\nNumber of Fisher Scoring iterations: 6\n```\n\n\n:::\n:::\n\n\n## Additive model, `intensity` and `emotion`\n\nThe interpretation is the same as before. The `intercept` is the expected logit when everthing is zero (intensity = 10 and emotion = anger).\n\n`intensity` is the increase in logit accuracy for a unit (10%) increase in intensity controlling for `emotion_lbl`.\n\n`emotion_lblsurprise` is the logit difference between anger and surprise controlling for `emotion_lbl`.\n\n## Additive model, `intensity` and `emotion`\n\nWe can have also the two main effects (not really useful with a factor with two levels):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncar::Anova(fit_int_emo)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table (Type II tests)\n\nResponse: acc\n            LR Chisq Df Pr(>Chisq)    \nintensity10   44.305  1  2.809e-11 ***\nemotion_lbl   32.834  1  1.004e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n## Additive model, `intensity` and `emotion`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(patchwork) # composing plots\n\nplot(ggeffect(fit_int_emo, \"intensity10\")) + \n  plot(ggeffect(fit_int_emo, \"emotion_lbl\")) \n```\n\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-55-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Additive model, `intensity` and `emotion`\n\nThis means that we have an odds ratio of 64.22 in favour of anger and an odds ratio of 2.31 for a unit increase in intensity.\n\nWe can also compute again the marginal effects for `intensity`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\navg_slopes(fit_int_emo)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n        Term         Contrast Estimate Std. Error     z Pr(>|z|)    S  2.5 %\n emotion_lbl surprise - anger   0.4667    0.07159  6.52   <0.001 33.7 0.3263\n intensity10 dY/dX              0.0852    0.00751 11.35   <0.001 96.7 0.0704\n 97.5 %\n 0.6070\n 0.0999\n\nType: response\n```\n\n\n:::\n:::\n\n\nThis means that we have on average a 8% of increase in accuracy across different levels of `intensity`.\n\n## Interaction model, `intensity` and `emotion`\n\nThe final model that we can try is including the interaction between `intensity` and `emotion`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_emo_x_int <- glm(acc ~ intensity10 * emotion_lbl, \n                     data = dat, family = binomial(link = \"logit\"), \n                     subset = emotion_lbl %in% c(\"anger\", \"surprise\"))\n\nsummary(fit_emo_x_int)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = acc ~ intensity10 * emotion_lbl, family = binomial(link = \"logit\"), \n    data = dat, subset = emotion_lbl %in% c(\"anger\", \"surprise\"))\n\nCoefficients:\n                                Estimate Std. Error z value Pr(>|z|)   \n(Intercept)                      -3.1654     1.1766  -2.690  0.00714 **\nintensity10                       0.4841     0.1933   2.504  0.01228 * \nemotion_lblsurprise               1.0226     1.4454   0.707  0.47929   \nintensity10:emotion_lblsurprise   0.9782     0.4738   2.065  0.03897 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 128.207  on 99  degrees of freedom\nResidual deviance:  58.274  on 96  degrees of freedom\nAIC: 66.274\n\nNumber of Fisher Scoring iterations: 7\n```\n\n\n:::\n:::\n\n\n## Interaction model, `intensity` and `emotion`\n\nWith interactions, always visualize first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(ggeffect(fit_emo_x_int, terms = c(\"intensity10\", \"emotion_lbl\")))\n```\n\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-58-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Interaction model, `intensity` and `emotion`\n\nClearly the effect of intensity is not the same for `anger` and `surprise`. The participant reaches high accuracies faster for `surprised` faces compared to `angry` faces.\n\n- `intercept`: is the expected logit for `anger` and `intensity` 0 (10%). Can be considered as the accuracy for the hardest angry face.\n- `intensity10`: is the increase in logit accuracy for a unit increase (10%) in intensity for `angry` faces (the red slope in the previous plot)\n- `emotion_lblsurprise`: is the logit difference (log odds ratio) between `anger` and `surprise` when `intensity` is 0 (10%). Is a conditional log odds ratio for a fixed value of intensity.\n- `intensity10:emotion_lblsurprise`: this is the actual interaction. Is the logit difference of the two slopes (in logit). Is the red slope vs the blue slope.\n\n## Interaction model, `intensity` and `emotion`\n\nLet's improve a little bit the intepretation. We can *center* the emotion applying not the dummy (or treatment) coding but the sum to zero coding.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndatsub <- filter(dat, emotion_lbl %in% c(\"anger\", \"surprise\"))\ndatsub$emotion_lbl <- factor(datsub$emotion_lbl)\ncontrasts(datsub$emotion_lbl) <- -contr.sum(2)/2\ncontrasts(datsub$emotion_lbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]\nanger    -0.5\nsurprise  0.5\n```\n\n\n:::\n:::\n\n\n## Interaction model, `intensity` and `emotion`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_emo_x_int2 <- glm(acc ~ intensity10 * emotion_lbl, \n                      data = datsub, \n                      family = binomial(link = \"logit\"))\n\nsummary(fit_emo_x_int2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = acc ~ intensity10 * emotion_lbl, family = binomial(link = \"logit\"), \n    data = datsub)\n\nCoefficients:\n                         Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -2.6541     0.7227  -3.672  0.00024 ***\nintensity10                0.9732     0.2369   4.108 3.99e-05 ***\nemotion_lbl1               1.0226     1.4454   0.707  0.47929    \nintensity10:emotion_lbl1   0.9782     0.4738   2.065  0.03897 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 128.207  on 99  degrees of freedom\nResidual deviance:  58.274  on 96  degrees of freedom\nAIC: 66.274\n\nNumber of Fisher Scoring iterations: 7\n```\n\n\n:::\n:::\n\n\n## Interaction model, `intensity` and `emotion`\n\nThe parameters are intepreted in the same way but now we have a different meaning of 0:\n\n- The intercept is the average logit accuracy when intensity is 10%\n- `intensity10`: is the slope when `emotion_lbl` is 0 but 0 now is in the middle of `anger` and `surprise`. This means that `intensity10` is the main effect of intensity controlling for emotion.\n- The interaction is the same as before as well as the other `emotion_lbl1` parameter.\n\n## Interaction model, `intensity` and `emotion`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnames(fit_emo_x_int$coefficients) <- names(fit_emo_x_int2$coefficients) # just for a better output, dangerous otherwise\ncar::compareCoefs(fit_emo_x_int, fit_emo_x_int2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCalls:\n1: glm(formula = acc ~ intensity10 * emotion_lbl, family = binomial(link = \n  \"logit\"), data = dat, subset = emotion_lbl %in% c(\"anger\", \"surprise\"))\n2: glm(formula = acc ~ intensity10 * emotion_lbl, family = binomial(link = \n  \"logit\"), data = datsub)\n\n                         Model 1 Model 2\n(Intercept)               -3.165  -2.654\nSE                         1.177   0.723\n                                        \nintensity10                0.484   0.973\nSE                         0.193   0.237\n                                        \nemotion_lbl1                1.02    1.02\nSE                          1.45    1.45\n                                        \nintensity10:emotion_lbl1   0.978   0.978\nSE                         0.474   0.474\n                                        \n```\n\n\n:::\n:::\n\n\n# Diagnostics\n\n## Deviance\n\nWhen using the `summary()` function we can see that there is as section about *Deviance*:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(fit_int)\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n<pre><code>\nCall:\nglm(formula = acc ~ intensity, family = binomial(link = \"logit\"), \n    data = dat)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.797692   0.263646  -6.819 9.19e-12 ***\nintensity    0.031976   0.004291   7.452 9.19e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n<span class='hg'>    Null deviance: 512.83  on 369  degrees of freedom</span>\n<span class='hg'>Residual deviance: 447.05  on 368  degrees of freedom</span>\n<span class='hg'>AIC: 451.05</span>\n\nNumber of Fisher Scoring iterations: 4\n\n</code></pre>\n:::\n\n\nThis information can be used to assess the goodness of fit of the model and also to compute pseudo-$R^2$ values (see later).\n\n## Deviance\n\nWe need to define three types of models:\n\n- **Null Model**: a model without predictors (only the intercept)\n- **Actual Model**: the model we fitted with predictors of interest\n- **Saturated Model**: a model fitting the data perfectly (no error)\n\n## Deviance and likelihood\n\nThis is a visual rappresentation of the three models. The current model should be always between the null and the saturated.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-64-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Deviance and likelihood\n\nWe can simplify the idea of likelihood and deviance thinking about the distance between the fitted line and the points. As the distance decreases, the likelihood of the model increases.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-65-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Deviance\n\nThe null and residual deviance that we see in the model output can be calculated as:\n\n$$\nD_{\\mbox{null}} = 2 -(log(\\mathcal{L}_{\\mbox{null}}) - log(\\mathcal{L}_{\\mbox{sat}}))\n$$\n$$\nD_{\\mbox{resid}} = 2 -(log(\\mathcal{L}_{\\mbox{current}}) - log(\\mathcal{L}_{\\mbox{sat}}))\n$$\n\n## Deviance\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat$id <- factor(1:nrow(dat))\nfit_cur <- fit_int # current model\nfit_sat <- glm(acc ~ 0 + id, data = dat, family = binomial(link = \"logit\"))\nfit_null <- glm(acc ~ 1, data = dat, family = binomial(link = \"logit\"))\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# residual\n2 * -(logLik(fit_cur) - logLik(fit_sat))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'log Lik.' 447.0538 (df=2)\n```\n\n\n:::\n\n```{.r .cell-code}\n# null\n2 * -(logLik(fit_null) - logLik(fit_sat))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'log Lik.' 512.8316 (df=1)\n```\n\n\n:::\n:::\n\n\n## Deviance, LRT\n\n![](img/lrt.svg)\n\n## $R^2$\n\nThe $R^2$ cannot be computed as in standard linear regression. There are different types of pseudo-$R^2$ for example:\n\n- Likelihood ratio $R^2_L$\n- Cox and Snell $R^2_{CS}$\n- Nagelkerke $R^2_N$\n- McFadden $R^2_{McF}$\n- Tjur $R^2_T$\n\nAll these methods are based on the deviance and/or the likelihood of current/null/saturated models.\n\n::: aside\nSee [https://en.wikipedia.org/wiki/Pseudo-R-squared](https://en.wikipedia.org/wiki/Pseudo-R-squared) for a nice overview.\n:::\n\n## $R^2$\n\nThe `performance` R package (used to plot diagnostics and other modelling-related metrics) implements most of the pseudo-$R^2$ values. The default for binomial models is the method proposed by @Tjur2009-ml.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# performance::r2_tjur()\nperformance::r2(fit_emo_x_int2) \n## # R2 for Logistic Regression\n##   Tjur's R2: 0.578\nperformance::r2_coxsnell(fit_emo_x_int2)\n## Cox & Snell's R2 \n##        0.5030847\nperformance::r2_mcfadden(fit_emo_x_int2)\n## # R2 for Generalized Linear Regression\n##        R2: 0.545\n##   adj. R2: 0.530\nperformance::r2_nagelkerke(fit_emo_x_int2)\n## Nagelkerke's R2 \n##       0.6962745\n```\n:::\n\n\n## Residuals\n\nDiagnostics in GLMs is more complex than in standard linear models. The main reason is that residuals are more complex due to the link function. For example these are the residuals of the last model we fitted.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-69-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Residuals\n\nThere are few problems with residuals in GLM:\n\n- **Mean and variance are linked**. This means that as the mean increase also the variance increase violating the standard homoschedasticity assumption. This mainly happens with standard *raw* residuals and in GLM we need to use other residuals (e.g., Pearson, Deviance, etc.)\n- **Residuals (even the Pearson or Deviance) are problematic for *discrete* GLM** (such as Binomial or Poisson), see the plot in the previous slide.\n- **Residuals for non-normal distributions are not expected to be normally distributed** even when the model is well specified.\n- **There are no standard and universal way** to assess the residuals pattern.\n\n## Residuals, a proposal\n\n> The `DHARMa` package uses a simulation-based approach to create readily interpretable scaled (quantile) residuals for fitted generalized linear (mixed) models\n\nThese residuals seems quite promising as an unified framework but I haven't systematically explored this possibility.\n\nThe `DHARMa` package has a nice [documentation](https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html) explaining the idea and how to use the quantile residuals [see also @Dunn1996-yd; @Dunn2018-ww].\n\n## Residuals, a proposal\n\n> The resulting residuals are standardized to values between 0 and 1 and can be interpreted as intuitively as residuals from a linear regression.\n\n\n::: {.cell layout-align=\"center\" output-location='slide'}\n\n```{.r .cell-code}\nlibrary(DHARMa)\nplot(simulateResiduals(fit_emo_x_int2))\n```\n\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-70-1.svg){fig-align='center'}\n:::\n:::\n\n\n# Monte Carlo simulations (MCS)\n\n## Why simulating data?\n\nIf you plan to use a GLM and you want to compute some inferential properties (statistical power, type-1 error, etc.) there are usually no analytical (i.e., formula-based) methods to solve the problem.\n\nThe only way to do a power analysis with a logistic regression is to simulate data and re-fit the model multiple times to see the long-run behaviour.\n\nMCS are also useful to understand more deeply a certain statistical model or procedure.\n\n## General MCS workflow\n\n1. Define the Data Generation Process (DGP)\n2. Define the sample size, number of trials, conditions, etc.\n3. Simulate data using random number generations and the DGP\n4. Fit the target model\n5. Repeat 3-4 a large number of times maybe with different features (e.g., different sample size) defined in 2\n6. Summarise the simulation results. For example, counting the number of times the p value is significant (i.e., estimating the statistical power)\n\n::: aside\nWe have also a longer [document](https://filippogambarota.github.io/notes/mcs-workflow/) about implementing a MCS in R.\n:::\n\n## 1. Data Generation Process (DGP)\n\nLet's try estimating the statistical power for the `intensity` effect in our example.\n\nWe will simulate data according to a Binomial GLM with a logit link function.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbinomial(link = \"logit\")\nqlogis() # link function\nplogis() # inverse link function\n```\n:::\n\n\n## 2. Experiment features\n\nWe will simulate a single subject doing $n$ trials. The main predictor is `intensity` ranging from 10% to 100% in steps of 10%.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(intensity <- seq(10, 100, 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  10  20  30  40  50  60  70  80  90 100\n```\n\n\n:::\n:::\n\n\n## 3. Random number generation\n\nIn R, to simulate data you can use the `r*` function. Each implemented distribution in R has the associated `r*` function (as the `p*` and `q*` function we used before).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# simulate 10 numbers from a gaussian distribution\n# with mu = 10 and sigma = 5\nrnorm(n = 10, mean = 10, sd = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  5.428778  1.897033 12.783843 10.742513  8.194715 15.738527  8.714334\n [8] 24.877813 14.417813 14.328772\n```\n\n\n:::\n:::\n\n\n## 3. Random number generation\n\nFor the Binomial we have `rbinom`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# n = number of rows in our case\n# size = number of trials\n# p = probability of success in each trial\nrbinom(n = 1, size = 1, prob = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n\n```{.r .cell-code}\nrbinom(n = 5, size = 1, prob = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0 1 1 1 0\n```\n\n\n:::\n\n```{.r .cell-code}\nrbinom(n = 5, size = 10, prob = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4 7 6 6 6\n```\n\n\n:::\n:::\n\n\n## 3. Random number generation\n\nNotice that `rbinom` is not really intutive because it works both on the binary and the binomial form.\n\nThis means to generate $k$ bernoulli trials with results 0 or 1\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrbinom(10, 1, 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 1 1 0 1 0 0 1 1 1 0\n```\n\n\n:::\n:::\n\n\nThis is the same but aggregating:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# the result is the number of successes over 10 \n# bernoulli trials\nrbinom(1, 10, 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6\n```\n\n\n:::\n:::\n\n\nDepending if you want to work with 0-1 values or number of successes/failures (aggregated) you should use one of the two strategies.\n\n## 3. Random number generation\n\nThe crucial part of `rbinom` is that the `prob =` argument is vectorized. This means that if you simulate $n$ trials you can provide $n$ probabilities.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 20 different probabilities\n(p <- seq(0.1, 0.9, length.out = 20))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0.1000000 0.1421053 0.1842105 0.2263158 0.2684211 0.3105263 0.3526316\n [8] 0.3947368 0.4368421 0.4789474 0.5210526 0.5631579 0.6052632 0.6473684\n[15] 0.6894737 0.7315789 0.7736842 0.8157895 0.8578947 0.9000000\n```\n\n\n:::\n:::\n\n\nThis means that for the last trials, the probability of success (1) is larger.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrbinom(n = 20, size = 1, prob = p)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 1 1 0 1 1\n```\n\n\n:::\n:::\n\n\n## Dataset\n\nWe can start with the deterministic part of the simulation, the experiment.\n\nWe simulate an experiment with `nt` trials with random `intensity` values from 10 to 100. We could also create a balanced version.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnt <- 100 # number of trials\n\ndat <- data.frame(\n  intensity = sample(intensity, nt, replace = TRUE)\n)\n\nhead(dat) # first 6 rows\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  intensity\n1       100\n2        70\n3        40\n4        40\n5        30\n6        30\n```\n\n\n:::\n:::\n\n\n## Systematic component and random component\n\nThe model can be formalized as:\n\n$$\np_i = g^{-1}(\\beta_0 + \\beta_1 \\times \\mbox{intensity}_i)\n$$\n\n$$\ny_i \\sim \\mathrm{Bernoulli}(p_i) \\qquad y_i \\in \\{0,1\\}\n$$\n\nWhere $g^{-1}$ is the inverse logit function (`qlogis`). This can be reas as, for each trial $i$ the true probability of success is a function of the `intensity`-$i$. The 0-1 outcome comes from a Bernoulli distribution with probability of success $p_i$. This means that according to $\\beta_1$ different intensity values will have a different probability of success.\n \nFinally $\\beta_0 + \\beta_1 \\times \\mbox{intensity}_i$ (i.e., the linear predictor $\\eta_i$) need to be simulated in logit scale, then converted back into probabilities.\n\n## Systematic component and random component\n\n$\\beta_0$ is the (logit) probability of success for intensity 10%, centering the `intensity` on the minimum. $\\beta_1$ is the increase in logit for a unit increase in intensity. Let's rescale intensity to be between 0 (10%) and 10 (100%).\n\n\n::: {.cell layout-align=\"center\" output-location='slide'}\n\n```{.r .cell-code  code-fold=\"true\"}\nb0 <- qlogis(0.1) # logit of success when intensity = 10%\nb1 <- 1\nintensity10 <- (intensity - 10) / 10\neta <- b0 + b1 * intensity10\n\ndata.frame(\n  intensity,\n  eta\n) |> \n  ggplot(aes(x = intensity, y = plogis(eta))) +\n  geom_point(size = 5) +\n  geom_line() +\n  xlab(\"Intensity\") +\n  ylab(\"Accuracy\") +\n  ggtitle(latex2exp::TeX(\"$\\\\beta_1 = 1$\"))\n```\n\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-80-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Systematic component and random component\n\nYou can choose different $\\beta_1$ values according to your hypothesis.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-81-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Systematic component and random component\n\nNow we can simply apply the same functions to the full dataset. Let's stick with $\\beta_1 = 0.5$ for the moment.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nb0 <- qlogis(0.1)\nb1 <- 1\ndat$intensity10 <- with(dat, (intensity - 10)/10)\ndat$eta <- with(dat, b0 + b1 * intensity10) # link function space\ndat$p <- plogis(dat$eta) # inverse link (probability)\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  intensity intensity10        eta         p\n1       100           9  6.8027754 0.9988905\n2        70           6  3.8027754 0.9781781\n3        40           3  0.8027754 0.6905679\n4        40           3  0.8027754 0.6905679\n5        30           2 -0.1972246 0.4508531\n6        30           2 -0.1972246 0.4508531\n```\n\n\n:::\n:::\n\n\n## Simulate the 0-1 response\n\nFinally we need to simulate the 0-1 response for each trial/observation:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat$acc <- rbinom(nrow(dat), 1, dat$p)\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  intensity intensity10        eta         p acc\n1       100           9  6.8027754 0.9988905   1\n2        70           6  3.8027754 0.9781781   1\n3        40           3  0.8027754 0.6905679   1\n4        40           3  0.8027754 0.6905679   0\n5        30           2 -0.1972246 0.4508531   1\n6        30           2 -0.1972246 0.4508531   1\n```\n\n\n:::\n:::\n\n\nFor each row/trial, `p` is the true accuracy according to `intensity` and `acc` is the actual simulated response.\n\n## Simulate the 0-1 response\n\nAlways plot your simulated data to check the results:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-84-1.svg){fig-align='center'}\n:::\n:::\n\n\n## 4. Fit the target model\n\nNow we can fit the desired model:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- glm(acc ~ intensity10, data = dat, family = binomial(link = \"logit\"))\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = acc ~ intensity10, family = binomial(link = \"logit\"), \n    data = dat)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -1.6110     0.5646  -2.853  0.00433 ** \nintensity10   0.9875     0.2263   4.363 1.28e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 107.855  on 99  degrees of freedom\nResidual deviance:  58.473  on 98  degrees of freedom\nAIC: 62.473\n\nNumber of Fisher Scoring iterations: 7\n```\n\n\n:::\n:::\n\n\n## 5. Repeat the process\n\nNow the core of MCS. By repeating the data generation and model fitting we are simulating the randomness that could happen in real data collection. Better wrapping everything into a function:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsim_data <- function(nt, b0, b1){\n  dat <- data.frame(\n    intensity = sample(seq(10, 100, 10), nt, replace = TRUE)\n  )\n  dat$intensity10 <- with(dat, (intensity - 10)/10)\n  dat$eta <- with(dat, b0 + b1 * intensity10)\n  dat$p <- plogis(dat$eta)\n  dat$acc <- rbinom(nrow(dat), 1, dat$p)\n  return(dat)\n}\n\nfit_model <- function(data){\n  glm(acc ~ intensity10, data = data, family = binomial(link = \"logit\"))\n}\n\nsummary_model <- function(fit){\n  # keep only intensity coefficients\n  data.frame(summary(fit)$coefficients)[2, ]\n}\n```\n:::\n\n\n## 5. Repeat the process\n\nAnd a overall simulation function:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndo_sim <- function(nsim, nt, b0, b1){\n  res <- replicate(nsim, {\n    dat <- sim_data(nt, b0, b1)\n    fit <- fit_model(dat)\n    summary_model(fit)\n  }, simplify = FALSE)\n  res <- do.call(rbind, res)\n  rownames(res) <- NULL\n  names(res) <- c(\"b\", \"se\", \"z\", \"p\")\n  return(res)\n}\n\ndo_sim(5, 100, b0, b1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          b        se        z            p\n1 1.3036353 0.3597669 3.623555 2.905811e-04\n2 0.8680892 0.1713114 5.067318 4.034595e-07\n3 0.7513822 0.1532364 4.903418 9.418318e-07\n4 1.3768539 0.3007452 4.578142 4.691248e-06\n5 0.8732376 0.1683584 5.186778 2.139633e-07\n```\n\n\n:::\n:::\n\n\n## 5. Repeat the process\n\n`nsim` is the number of simulations. Usually larger is better considering computational constraints. If feasible, 5000 or 10000 is usually more than enough. For complex models where 5000 or 10000 is not an option, try never go below 1000. See @Burton2006-hl and @Koehler2009-do for discussion about the number of simulations.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntictoc::tic()\nsim <- do_sim(1000, nt = 50, b0 = qlogis(0.1), b1 = 0.5)\ntictoc::toc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1.696 sec elapsed\n```\n\n\n:::\n:::\n\n\nIn this case we are lucky, running 1000 simulations only takes ~ 1.5 seconds. Sometimes can also takes hours, days or weeks.\n\n## 6. Results\n\nEach row is the parameter estimated from a simulated dataset. The statistical power is the % of p values lower than $\\alpha$ of the total number of simulations:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(sim)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          b        se        z            p\n1 0.4928971 0.1356198 3.634403 2.786251e-04\n2 0.8227963 0.2068834 3.977102 6.976032e-05\n3 0.6340695 0.1654294 3.832872 1.266561e-04\n4 0.4271856 0.1311341 3.257625 1.123488e-03\n5 0.9516707 0.2573823 3.697498 2.177347e-04\n6 1.0863456 0.3113269 3.489405 4.840970e-04\n```\n\n\n:::\n\n```{.r .cell-code}\nsum(sim$p <= 0.05) / 1000\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.996\n```\n\n\n:::\n\n```{.r .cell-code}\n# or mean(sum(sim$p <= 0.05))\n```\n:::\n\n\nThe power is pretty high with 50 trials. Let's try a lower effect with different number of trials.\n\n## More conditions\n\nWe can try different number of trials. For each `n` we simulate 1000 datasets, fit the models, extract the p values and estimate the statistical power.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- c(10, 50, 80, 100, 150, 200)\npower <- rep(NA, length(n))\n\nfor(i in 1:length(power)){\n  res <- do_sim(1000, n[i], b0 = qlogis(0.1), b1 = 0.2)\n  power[i] <- mean(res$p <= 0.05)\n}\n\npower\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.000 0.362 0.520 0.603 0.820 0.912\n```\n\n\n:::\n:::\n\n\n## More conditions\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-91-1.svg){fig-align='center'}\n:::\n:::\n\n\n# Latent formulation of the binomial model\n\n## Latent formulation of the binomial model\n\nTo clarify a little bit the terminology the *logit* function (link function $g(\\cdot)$) is:\n\n$$\nq = \\log{\\frac{p}{1 - p}}\n$$\n\nThe inverse of the logit is called *logistic* (inverse link function $g^{-1}(\\cdot)$) function:\n\n$$\np = \\frac{e^p}{1 + e^p}\n$$\n\n## Latent formulation of the binomial model\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-92-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Latent formulation of the binomial model\n\n- Instead of thinking about a binary variable, you can think about e.g. accuracy as a continous measure from $-\\infty$ to $+\\infty$. \n- Then you can imagine to cut this continous measure using a threshold. Everthing above the threshold takes the value 1 otherwise 0.\n- Using the *logit* function we are assuming that this underlying *latent* variable is a standard **logistic** distribution.\n- The standard logistic distribution is a continous variable (similar to the Gaussian) with mean $\\mu = 0$ and $\\sigma^2 = \\frac{s^2 \\pi^2}{3}$. Given that $s^2 = 1$ in the standard logistic distribution the variance is $\\frac{\\pi^2}{3}$.\n\n::: aside\n[https://en.wikipedia.org/wiki/Logistic_distribution](https://en.wikipedia.org/wiki/Logistic_distribution)\n:::\n\n## Latent formulation of the binomial model\n\nYou can draw the logistic distribution using `dlogis()`. The comulative distribution function `plogis()` is the *logistic* function and the quantile function `qlogis()` is the *logit* function.\n\n## Latent formulation of the binomial model\n\nIn this framework, saying that the logit in one condition is 0 means that the probability is 0.5 thus 50% of the observations are expected to have a value of 1 and 50% a value of 0.\n\nA shift in the latent distribution represents also a shift in the proportions of 1 and 0. Regression coefficients (in logit) represent the shift in the latent distribution and odds ratios are the shift in the odds/proportions.\n\n## Latent formulation of the binomial model\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-93-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Latent formulation of the binomial model\n\nIn other terms, the logistic regression can be expressed as a linear model predicting shifts in the mean of the logistic distribution as a functions of predictors as in standard linear models assuming normality.\n\nThe parameters $\\beta$ are shifts in the underlying latent distribution. $\\beta = 1$ is like saying that the difference between the groups is 1 standard deviation on the latent scale.\n\n## Latent formulation of the binomial model\n\nPractically this means that we can write the same model in the latent form:\n\n$$\nz_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n$$\n\n$$\n\\epsilon_i \\sim \\mbox{Logistic}(0, 1)\n$$\n\nWhere $z_i$ is the latent value (e.g., logit). Errors are distributed according to a standard logistic distribution.\n\nThen the observed 0-1 values:\n\n$$\n\\begin{cases}\n1 & \\text{if } z_i > 0, \\\\\n0 & \\text{if } z_i \\le 0 .\n\\end{cases}\n$$\n\n## Latent formulation of the binomial model\n\nIn R we can show this very easily. We can simulate a latent shift and the corresponding pattern in probabilities using the standard and latent form of the logistic regression.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np1 <- 0.5 # probability of group 1\np2 <- 0.8 # probability of group 2\n(d <- qlogis(p2) - qlogis(p1)) # latent shift, log odds ratio\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.386294\n```\n\n\n:::\n\n```{.r .cell-code}\n(OR <- exp(d)) # odds ratio\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4\n```\n\n\n:::\n:::\n\n\n## Latent formulation of the binomial model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# number of observations, large to show the pattern\nn <- 10000\n\n# using rbinom\nx <- rep(0:1, each = n/2) # condition 1 and condition 2\np <- plogis(qlogis(p1) + d * x)\nyb <- rbinom(n, 1, p)\n\n# latent form\nz <- qlogis(p1) + x * d + rlogis(n, 0, 1)\nyl <- ifelse(z > 0, 1, 0)\n\ntapply(yb, x, mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     0      1 \n0.5022 0.7974 \n```\n\n\n:::\n\n```{.r .cell-code}\ntapply(yl, x, mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     0      1 \n0.5006 0.8058 \n```\n\n\n:::\n:::\n\n\n## Latent formulation of the binomial model\n\nThe beauty of the latent intepretation is that if you change the error part from a logistic to a gaussian distribution, you obtain the **probit** model.\n\nIn the probit the idea is the same but shifts are in units of a standard normal distribution that can be intepreted as Cohen's $d$.\n\n# Probit link function\n\n## Probit link function\n\nSometimes, the *probit* link function can be used. The main difference is that when using the *logit* link function the underlying distribution is called logistic. When using the *probit* link function the underlying distribution is Gaussian.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-96-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Probit link function\n\nWhen using the **probit link** the parameters are interpreted as difference in *z-scores* associated with a unit increase in the predictors. In fact probabilities are mapped into *z-scores* using the cumulative normal distribution.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np1 <- 0.8\np2 <- 0.6\n\nqlogis(c(p1, p2)) # log(odds(p1)), logit link\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.3862944 0.4054651\n```\n\n\n:::\n\n```{.r .cell-code}\nqnorm(c(p1, p2)) # probit link\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8416212 0.2533471\n```\n\n\n:::\n\n```{.r .cell-code}\nlog(or(p1, p2)) # ~ beta1, logit link\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9808293\n```\n\n\n:::\n\n```{.r .cell-code}\nqnorm(p1) - qnorm(p2) # ~beta1, probit link\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5882741\n```\n\n\n:::\n:::\n\n\n## Probit link function\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-98-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Why using the probit link function?\n\nThe probit can be a really useful choice:\n\n- A binomial GLM with a probit link can be used to estimate Signal Detection Theory ($d$' and criterion) parameters [see @DeCarlo1998-ay; @DeCarlo2010-lj]. \n- The parameters are intepreted as differences in standard deviations similar to Cohen's $d$. This is also very useful in simulations.\n\n# Odds Ratio (OR) to Cohen's $d$\n\n## Odds Ratio (OR) to Cohen's $d$\n\nThe Odds Ratio can be considered an effect size measure. We can transform the OR into other effect size measure [@Sanchez-Meca2003-ji; @Borenstein2009-mo].\n\n$$\nd = \\log(OR) \\frac{\\sqrt{3}}{\\pi}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# in R with the effectsize package\nor <- 1.5\neffectsize::logoddsratio_to_d(log(or))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2235446\n```\n\n\n:::\n\n```{.r .cell-code}\n# or \neffectsize::oddsratio_to_d(or)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2235446\n```\n\n\n:::\n:::\n\n\n## Odds Ratio (OR) to Cohen's $d$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](binomial-glm_files/figure-revealjs/unnamed-chunk-100-1.svg){fig-align='center'}\n:::\n:::\n\n\n# Binary vs binomial data structure\n\n## Binary vs binomial data structure\n\nIn our facial expression example we used the binary data structure. This means that the vector of the response variable contains 0 and 1.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n     id age   intensity emotion_lbl response_lbl   acc\n  <int> <chr>     <dbl> <chr>       <chr>        <int>\n1    22 53           60 fear        suprise          0\n2    22 53           60 disgust     disgust          1\n3    22 53           70 happiness   happiness        1\n4    22 53          100 happiness   happiness        1\n5    22 53           60 disgust     sadness          0\n6    22 53           20 fear        neutral          0\n```\n\n\n:::\n:::\n\n\n## Binary vs binomial data structure\n\nThe same dataset can be also used in the so-called binomial format. In this case we can aggregate `emotion_lbl` and `intensity` counting the number of correct responses.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat_binomial <- dat |> \n  group_by(intensity) |> \n  summarise(nt = n(),      # total number of trials\n            nc = sum(acc), # number of correct responses\n            nf = nt - nc,  # number of fails\n            acc = nc / nt) # proportion of correct response\n\nhead(dat_binomial)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n  intensity    nt    nc    nf    acc\n      <dbl> <int> <int> <int>  <dbl>\n1        10    37     3    34 0.0811\n2        20    37     3    34 0.0811\n3        30    37    12    25 0.324 \n4        40    37    18    19 0.486 \n5        50    37    23    14 0.622 \n6        60    37    23    14 0.622 \n```\n\n\n:::\n:::\n\n\n## Binary vs binomial data structure\n\nThe dataset contains **exactly** the same information, we are not losing anything by aggregating. We can check it by fitting two models. Notice the way of writing the model in the binomial way:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_binary   <- glm(acc ~ intensity, data = dat, family = binomial(link = \"logit\"))\nfit_binomial <- glm(cbind(nc, nf) ~ intensity, data = dat_binomial, family = binomial(link = \"logit\"))\n\ncar::compareCoefs(fit_binary, fit_binomial)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCalls:\n1: glm(formula = acc ~ intensity, family = binomial(link = \"logit\"), data = \n  dat)\n2: glm(formula = cbind(nc, nf) ~ intensity, family = binomial(link = \n  \"logit\"), data = dat_binomial)\n\n            Model 1 Model 2\n(Intercept)  -1.798  -1.798\nSE            0.264   0.264\n                           \nintensity   0.03198 0.03198\nSE          0.00429 0.00429\n                           \n```\n\n\n:::\n:::\n\n\n## Binary vs binomial data structure\n\nA third option is also modelling directly the proportions providing the `weigths = ` argument as the number of trials.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_prop <- glm(acc ~ intensity, data = dat_binomial, weights = nt, family = binomial(link = \"logit\")) \ncar::compareCoefs(fit_binary, fit_binomial, fit_prop)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCalls:\n1: glm(formula = acc ~ intensity, family = binomial(link = \"logit\"), data = \n  dat)\n2: glm(formula = cbind(nc, nf) ~ intensity, family = binomial(link = \n  \"logit\"), data = dat_binomial)\n3: glm(formula = acc ~ intensity, family = binomial(link = \"logit\"), data = \n  dat_binomial, weights = nt)\n\n            Model 1 Model 2 Model 3\n(Intercept)  -1.798  -1.798  -1.798\nSE            0.264   0.264   0.264\n                                   \nintensity   0.03198 0.03198 0.03198\nSE          0.00429 0.00429 0.00429\n                                   \n```\n\n\n:::\n:::\n\n\nAgain, exactly the same.\n\n## Binary vs binomial data structure\n\nWhy using the binary vs the binomial format? Depends on the type of predictors! \n\nIn our (and similar) case, \"aggregating\" with categorical predictor is possible without losing information. The advantage is that models in the binomial format are very fast (especially for complex models such as mixed-effects models). Also the diagnostics in terms of residuals are better [see @Gelman2020-tg, p. 253]\n\nIf you have predictors at the 0-1 level you cannot aggregate without losing information. In our case, imagine to have for each trial the 0-1 accuracy and the reaction time. You need to use the binary format otherwise you have to bin the reaction time variable (losing information).\n\n## References",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
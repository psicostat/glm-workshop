{
  "hash": "1de6cd0546f55bc331aea5d5368dba1c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Introduction to Generalized Linear Models for Psychology\nsubtitle: Generalized Linear Models Workshop \nformat: \n  minimal-revealjs:\n    slide-number: true\n    html-math-method: mathjax\nexecute: \n  echo: true\ndate: last-modified\ndate-format: \"*[Last modified:] DD-MM-YYYY*\"\ntoc: true\n#html-math-method: mathml\nfig-dpi: 300\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n# Linear Regression {.section}\n\n## Linear Regression\n\nEstimate the expected (average) outcome given predictors.\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='left'}\n:::\n:::\n\n\n> The [expected value](https://en.wikipedia.org/wiki/Expected_value) of\n> a random variable with a finite number of outcomes is a weighted\n> average of all possible outcomes.\n\n## Linear Regression\n\nA constant change in **x** leads to a constant change in the **expected outcome**.\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Normal linear regression: `lm(y~x)`\n\nFor a fixed $x_i$, the model describes the **distribution of possible outcomes** around the expected value.\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-4-1.svg){fig-align='left'}\n:::\n:::\n\n\n\n# Probability Distributions {.section}\n\n## Why probability distributions?\n\nOur models do not fit our data exactly, we predict **the average** value of the outcome given the predictors.\n\n\n::: fragment\n- Probability distributions help us **characterize** the **variation** that remains ***after*** predicting the average.\n\n- Quantify **uncertainty in the estimated parameters** of the model!\n:::\n\n\n## Probability Distributions\n\nA **probability distribution** describes all possible values a random variable $Y$ can take and their associated probabilities, given certain parameters.\n\n\\\n\n$$f(y \\mid \\boldsymbol{\\theta})$$\n\n\\\n\nWhere:\n\n- $y$ is a specific value \n- $\\boldsymbol{\\theta}$ is a vector of parameters (e.g., $\\mu$, $\\sigma^2$)\n- $f(\\cdot)$ is the probability function\n\n\n\n## Discrete vs. Continuous Variables\n\n:::: columns\n::: column\n**Continuous** random variables (time, weight, temperature, etc.)\n\n- **Probability Density Function (PDF)**\n- $f(y \\mid \\boldsymbol{\\theta})$\n- The **probability density** at $y$ (not a probability!)\n- $P(Y = y) = 0$ for any specific value\n:::\n::: column\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-5-1.svg)\n:::\n:::\n\n\n:::\n:::::\n\n## Discrete vs. Continuous Variables\n\n:::: columns\n::: column\n**Discrete** random variables (counts, binary outcomes, etc.)\n\n- **Probability Mass Function (PMF)**\n- $P(Y = y \\mid \\boldsymbol{\\theta})$ \n- The **probability** of observing exactly $y$\n- $P(Y = 3) = 0.25$ means 25% chance of getting 3\n\n:::\n::: column\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-6-1.svg)\n:::\n:::\n\n:::\n:::::\n\n## Distributions and linear regression\n\nPredict the **average value of $Y$** and quantify the remaining variation:\n\n::: incremental\n- **Expected value (mean)**: $E[Y]=\\mu$.  \n  The long-run average of $Y$ under its distribution.\n\n- **Conditional expected value**: $E[Y\\mid X]=\\mu(X)$.  \n  The long-run average outcome **for a fixed predictor value** $X=x$.\n\n- **Variance**: $\\mathrm{Var}(Y)$ (or, in regression, $\\mathrm{Var}(Y\\mid X)$).  \n  How much outcomes vary around the mean (overall or at a given $x$).\n\n:::\n\n::: fragment\nIn normal linear and generalized linear models we generally include **predictors** on the **mean** of the distribution.\n:::\n\n\n## Why is it important?\n\nThe **definitions** of expected value and variance are universal,\nbut how we compute them (and what they equal) depends on the distribution’s parameters. \n\n::: fragment\nFor a **Normal** distribution $Y \\sim \\mathcal{N}(\\mu,\\sigma^2)$,\nthe parameters are exactly the mean and variance.\n\n::: incremental\n- **Expected value (mean):** $E[Y]=\\mu$. \n- **Variance:** $\\mathrm{Var}(Y)=\\sigma^2$. \n:::\n:::\n\n## \n\n| Distribution | Support | mean: $E[Y]$ | variance: $\\mathrm{Var}(Y)$ |\n|------------------|------------------|:----------------:|:-----------------:|\n| **Normal**: $f(y \\text{ | } \\mu,\\sigma^2)$ | $y \\in \\mathbb{R}$ | $\\mu$ | $\\sigma^2$ |\n| **Gamma**: $f(y \\text{ | } \\alpha,\\lambda)$ | $y \\in (0,\\infty)$ | $\\alpha/\\lambda$ | $\\alpha/\\lambda^2$ |\n| **Binomial**: $f(y \\text{ | } n,p)$ | $y \\in \\{0,1,\\dots,n\\}$ | $np$ | $np(1-p)$ |\n| **Poisson**: $f(y \\text{ | } \\lambda)$ | $y \\in \\{0,1,2,\\dots\\}$ | $\\lambda$ | $\\lambda$ |\n\n\n# The Normal distribution {.section}\n\n## Normal distribution\n\nA Normal distribution has parameters $\\mu$ (mean) and $\\sigma^2$\n(variance):\n\n$$f(y \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( - \\frac{(y - \\mu)^2}{2\\sigma^2} \\right)$$\n\n<br/>\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-7-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Normal distribution\n\nA Normal distribution has parameters $\\mu$ (mean) and $\\sigma^2$\n(variance):\n\n$$f(y \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( - \\frac{(y - \\mu)^2}{2\\sigma^2} \\right)$$\n\n<br/>\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-8-1.svg){fig-align='center'}\n:::\n:::\n\n  \n\n\n## Normal distribution\n\nA Normal distribution has parameters $\\mu$ (mean) and $\\sigma^2$\n(variance):\n\n$$f(y \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( - \\frac{(y - \\mu)^2}{2\\sigma^2} \\right)$$\n\n<br/>\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-9-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Normal linear regression\n\n:::: columns\n::: column \n$\\mu_i = \\beta_0 + \\beta_1 x_i,$ \n\n$y_i \\sim \\mathcal{N}(\\mu_i,\\sigma^2)$\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-10-1.svg){fig-align='left'}\n:::\n:::\n\n\n:::\n\n::: column \n::: fragment\n$\\quad y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i,$\n\n$\\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-11-1.svg){fig-align='left'}\n:::\n:::\n\n:::\n:::\n\n:::::\n\n::: fragment\nTwo equivalent ways of expressing the same modeling assumption.\n:::\n\n\n## Normal distribution\n\n-   **Support**: $-\\infty, +\\infty$\n-   mean = mode = median\n-   **Mean** (expected value): $\\mu$\n-   **Variance**: $\\sigma^2$\n-   **Independence:** changing the mean does *not* change the variance!\n\n## Why does it matter?\n\nReal-world measurements are not automatically Normal, **Normality** is a **modeling choice**.\n\n-   **Support**: what range of values is possible\n-   **Mean** (expected value): what value to predict on average\n-   **Variance**: how much variation around the mean\n-   **Mean–variance relationship**: does variance change with the mean?\n\n\n\n\n## Reaction Times\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-12-1.svg)\n:::\n:::\n\n\n## Exam pass/fail\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-13-1.svg)\n:::\n:::\n\n\n\n## Number of errors in a task\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-14-1.svg)\n:::\n:::\n\n\n\n\n## Example: Passing the exam\n\nProbability of passing the exam as a function of how many hours students\nstudy.\n\n::::: columns\n::: {.column width=\"30%\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n     id studyh passed\n1     1     29      1\n2     2     79      1\n3     3     41      1\n4     4     88      1\n5     5     94      1\n6     6      5      0\n7     7     53      0\n8     8     89      1\n9     9     55      1\n10   10     46      1\n11   11     96      1\n12   12     45      1\n13   13     68      1\n14   14     57      0\n15   15     10      0\n16   16     90      1\n17   17     25      0\n18   18      4      1\n19   19     33      0\n20   20     95      1\n21   21     89      1\n22   22     69      1\n23   23     64      1\n24   24     99      1\n25   25     66      1\n26   26     71      0\n27   27     54      1\n28   28     59      1\n29   29     29      0\n30   30     15      0\n31   31     96      1\n32   32     90      1\n33   33     69      1\n34   34     80      1\n35   35      2      0\n36   36     48      1\n37   37     76      1\n38   38     22      1\n39   39     32      0\n40   40     23      0\n41   41     14      0\n42   42     41      1\n43   43     41      1\n44   44     37      1\n45   45     15      1\n46   46     14      0\n47   47     23      0\n48   48     47      1\n49   49     27      0\n50   50     86      1\n51   51      5      0\n52   52     44      1\n53   53     80      1\n54   54     12      0\n55   55     56      1\n56   56     21      0\n57   57     13      0\n58   58     75      1\n59   59     90      1\n60   60     37      1\n61   61     67      1\n62   62      9      0\n63   63     38      1\n64   64     27      0\n65   65     81      1\n66   66     45      1\n67   67     81      1\n68   68     81      1\n69   69     79      1\n70   70     44      1\n71   71     75      1\n72   72     63      1\n73   73     71      1\n74   74      0      0\n75   75     48      1\n76   76     22      0\n77   77     38      1\n78   78     61      1\n79   79     35      0\n80   80     11      0\n81   81     24      1\n82   82     67      1\n83   83     42      1\n84   84     79      1\n85   85     10      0\n86   86     43      1\n87   87     98      1\n88   88     89      1\n89   89     89      1\n90   90     18      1\n91   91     13      0\n92   92     65      1\n93   93     34      0\n94   94     66      1\n95   95     32      0\n96   96     19      0\n97   97     78      1\n98   98      9      0\n99   99     47      1\n100 100     51      1\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"70%\"}\n::: fragment\n\n::: {.cell}\n\n```{.r .cell-code}\n# number of students that have passed the exam\nsum(dat_exam$passed) \n#> [1] 39\n# proportion of students that have passed the exam\nmean(dat_exam$passed) \n#> [1] 0.39\n\n# study hours and passing the exam\ntapply(dat_exam$studyh, dat_exam$passed, mean)\n#>        0        1 \n#> 37.52459 69.05128 \n#> \ntable(dat_exam$passed, cut(dat_exam$studyh, breaks = 4))\n#>    (1.9,26.2] (26.2,50.5] (50.5,74.8] (74.8,99.1]\n#>  0         23          22          10           6\n#>  1          2           6          10          21\n#>  \ntapply(dat_exam$passed, cut(dat_exam$studyh, breaks = 4), mean)\n#>   (0,24.8]   (24.8,49.5]   (49.5,74.2]   (74.2,99.1] \n#>   0.0800000     0.2142857     0.5000000     0.7777778\n```\n:::\n\n::: \n:::\n:::::\n\n## Visualize data\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-17-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Fitting a Normal Linear Model\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-18-1.svg){fig-align='left'}\n:::\n:::\n\n\n::: fragment\nHow does this look to you?\n::: \n\n## Fitting a Generalized Linear Model\n\nThe model should consider both the **support** of the $y$ variable and the **non-linear pattern**!\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-19-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Example: Reaction times\n\nReaction times on two-choice questions as a function of hours studied.\n\n::::: columns\n::: {.column width=\"35%\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n     id studyh         rt\n1     1     29  33.874158\n2     2     79   6.640343\n3     3     41  34.995848\n4     4     88   5.828311\n5     5     94   2.431803\n6     6      5  24.984741\n7     7     53   8.210221\n8     8     89   5.506648\n9     9     55  16.412949\n10   10     46  14.078309\n11   11     96   9.175749\n12   12     45   8.770755\n13   13     68  10.749554\n14   14     57  15.919140\n15   15     10  54.679986\n16   16     90   7.171171\n17   17     25  74.819825\n18   18      4  49.932109\n19   19     33  16.599006\n20   20     95   2.888791\n21   21     89   6.548021\n22   22     69   9.058849\n23   23     64   5.355270\n24   24     99   2.456604\n25   25     66   9.968016\n26   26     71  10.180864\n27   27     54  11.867775\n28   28     59   8.223566\n29   29     29  35.066819\n30   30     15  72.715225\n31   31     96   4.913368\n32   32     90   4.123894\n33   33     69   9.990744\n34   34     80   5.113008\n35   35      2  75.652694\n36   36     48  12.383083\n37   37     76  12.534166\n38   38     22  29.525602\n39   39     32  51.083314\n40   40     23  32.212360\n41   41     14  51.492232\n42   42     41  42.390660\n43   43     41  21.420568\n44   44     37  38.708747\n45   45     15  34.617161\n46   46     14  46.308540\n47   47     23  44.218815\n48   48     47  22.215179\n49   49     27  28.647402\n50   50     86   4.076351\n51   51      5  76.968889\n52   52     44  16.899778\n53   53     80   8.228880\n54   54     12  57.617081\n55   55     56  14.095732\n56   56     21  27.639242\n57   57     13  73.355891\n58   58     75  10.950845\n59   59     90   5.841234\n60   60     37  27.683213\n61   61     67  12.550214\n62   62      9 114.145735\n63   63     38  30.036094\n64   64     27  21.505963\n65   65     81   6.573745\n66   66     45  11.161376\n67   67     81  10.117532\n68   68     81  10.373743\n69   69     79  12.968275\n70   70     44  21.023414\n71   71     75   6.249403\n72   72     63  17.451597\n73   73     71   3.678978\n74   74      0  94.914728\n75   75     48  16.388884\n76   76     22  50.542559\n77   77     38  20.268758\n78   78     61  10.157155\n79   79     35   9.763869\n80   80     11  25.284120\n81   81     24  47.332562\n82   82     67  21.561711\n83   83     42   9.911588\n84   84     79   1.341846\n85   85     10  62.018904\n86   86     43  11.517841\n87   87     98   3.771747\n88   88     89   6.397266\n89   89     89   4.149168\n90   90     18  29.031131\n91   91     13  75.940742\n92   92     65   5.803764\n93   93     34  71.641900\n94   94     66  11.387666\n95   95     32  36.461655\n96   96     19  31.961301\n97   97     78   8.170944\n98   98      9 119.428719\n99   99     47   9.172191\n100 100     51  15.341197\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"65%\"}\n::: fragment\n\n::: {.cell}\n\n```{.r .cell-code}\nrange(dat_rt$rt)\n#>  1.341846 119.428719\n\n# mean RT as a function of studyh\ntapply(dat_rt$rt, cut(dat_rt$studyh, breaks = 4), mean)\n#>  (0,24.8]   (24.8,49.5]   (49.5,74.2]   (74.2,99.1] \n#>      56.10084      26.73179      11.39825       6.46455 \n```\n:::\n\n:::\n:::\n:::::\n\n## Visualize data\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-22-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Fitting a Normal Linear Model\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-23-1.svg){fig-align='left'}\n:::\n:::\n\n\n::: fragment\nHow does this look to you?\n::: \n\n## Fitting a Generalized Linear Model\n\nThe model should consider both the **support** of the $y$ variable and the **non-linear pattern**!\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-24-1.svg){fig-align='left'}\n:::\n:::\n\n\n# Generalized Linear Models (GLM) {.section}\n\n## General ideas\n\n::: incremental\n-   using distributions beyond the Normal\n-   modeling non linear functions on the response scale\n-   taking into account mean-variance relationships\n:::\n\n## The three ingredients of a GLM\n\n::: incremental\n1.  **Random Component**: Choose a distribution\n\n2.  **Systematic Component**: Linear predictor\n    $\\eta_i = \\beta_0 + \\beta_1 x_i$\n\n3.  **Link Function**: Transform the mean $g(\\mu_i) = \\eta_i$\n:::\n\n## Random component\n\nThe **random component** specifies a probability model for $y_i$:\n\n<br/>\n\n$$\ny_i \\mid x_i \\sim \\text{Distribution}(\\text{parameters}).\n$$\n\n<br/>\n\n::: {.fragment fragment-index=\"1\"}\nWhat **support**?\n\n-   Binary: $\\{0,1\\}$ $\\rightarrow$ $\\text{Bernoulli}(p)$ (or\n    $\\text{Binomial}(1,p)$)\n-   Counts: $\\{0,1,2,\\ldots\\}$ $\\rightarrow$ $\\text{Poisson}(\\lambda)$\n-   Any real number: $(-\\infty,\\infty)$ $\\rightarrow$\n    $\\text{Normal}(\\mu,\\sigma^2)$\n:::\n\n## Random component \n\nChoosing a distribution specifies not only the mean, but also how the variance depends on the mean (**variance function** $V(\\mu)$):\n\n$$\n\\mu = E(Y \\mid X), \\qquad \\mathrm{Var}(Y \\mid X) = \\phi \\, V(\\mu),\n$$\n\n\\\n\nOnce you model $\\mu(x)$, the variance is also determined **at each x**.\n\nExamples of **variance function** $V(\\mu)$:\n\n- $V(\\mu) = \\mu$ for Poisson, \n- $V(\\mu) = \\mu(1-\\mu)$ for Bernoulli\n\n\n## Systematic Component\n\nThe **systematic component** is *exactly the same* as in *normal* linear\nregression: \n\n<br/>\n\n$$\n\\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_k x_{ik}.\n$$ <br/>\n\nBasically it describes how the expected value (i.e., the mean) of the chosen distribution (the random component) varies\naccording to the predictors.\n\n## Link Function\n\nThe **link function** $g(\\cdot)$ connects the expected value (mean)\n$\\mu_i$ of the distribution to the linear predictor $\\eta_i$:\n\n\n$$\ng(\\mu_i) = \\eta_i\n$$\n\nInverse link:\n\n$$\n\\mu_i = g^{-1}(\\eta_i)\n$$\n\n-   The linear predictor $\\eta_i$ can be any real number:\n    $(-\\infty, +\\infty)$\n-   But $\\mu_i$ (the mean) is **constrained** by the distribution's\n    support\n-   The link function **transforms** $\\mu_i$ to be unbounded\n\n## Common link functions\n\n| Distribution | Support of $y$ | Link | Purpose |\n|----------------|----------------|------------------------|----------------|\n| Normal | $(-\\infty,\\infty)$ | Identity: $g(\\mu)=\\mu$ | No transformation |\n| Binomial | $\\{0,1,\\ldots,n\\}$ | Logit on $p$: $g(p_i)=\\log\\!\\left(\\frac{p}{1-p}\\right)$, where $p=\\mu/n$ | Probability $\\to \\mathbb{R}$ |\n| Poisson | $\\{0,1,2,\\ldots\\}$ | Log: $g(\\mu)=\\log(\\mu)$ | Positive $\\to \\mathbb{R}$ |\n| Gamma | $(0,\\infty)$ | Log: $g(\\mu)=\\log(\\mu)$ | Positive $\\to \\mathbb{R}$ |\n\n## Normal + identity\n\nFor example, a Generalized Linear Model with the **Normal** family and\n**identity link** can be written as:\n\n<br/>\n\n$$\n\\begin{aligned}\ny_i &\\sim \\mathcal{N}(\\mu_i, \\sigma^2) && \\text{Random component} \\\\\n\\mu_i &= \\eta_i && \\text{Link (identity)} \\\\\n\\eta_i &= \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik} && \\text{Systematic component}\n\\end{aligned}\n$$\n\n## Bernoulli + logit\n\nFor example, a Generalized Linear Model with the **Bernoulli** family\nand **logit link** can be written as:\n\n<br/>\n\n$$\n\\begin{aligned}\ny_i &\\sim \\text{Bernoulli}(p_i) && \\text{Random component} \\\\\np_i &= \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} && \\text{Inverse link (logit)} \\\\\n\\eta_i &= \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik} && \\text{Systematic component}\n\\end{aligned}\n$$\n\n## Bernoulli + logit\n\nFor example, a Generalized Linear Model with the **Bernoulli** family\nand **logit link** can be written as:\n\n<br/>\n\n$$\n\\begin{aligned}\ny_i &\\sim \\text{Bernoulli}(p_i) && \\text{Random component} \\\\\n\\text{logit}(p_i) &= \\log\\!\\left(\\frac{p_i}{1-p_i}\\right)=\\eta_i && \\text{Link (logit)} \\\\\n\\eta_i &= \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik} && \\text{Systematic component}\n\\end{aligned}\n$$\n\n\n\n\n# Binomial Logistic Regression {.section}\n\n## Random component\n\nThe ***Bernoulli(p)*** or the ***Binomial(n,p)*** distributions can be used as\n**random component** when we have a binary dependent variable or the\nnumber of successes over the total number of trials.\n\n\\\n\n-   **Binary** (pass/fail), one trial per person; or\n-   **Counts of successes** out of $n$ trials (e.g., items correct out of $n$).\n\n\n## Bernoulli Distribution \n\nSuppose you collected data on whether each student passed the exam.  Let $K \\in \\{0, 1\\}$ denote the outcome, where 0 = fail and 1 = pass.\n\n:::: columns\n::: {.column width=\"30%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-25-1.svg)\n:::\n:::\n\n\n:::\n\n::: {.column width=\"70%\"}\n\n$$\nP(K = k) =\n\\begin{cases}\np       & \\text{if } k = 1 \\text{ (pass)}, \\\\\n1-p & \\text{if } k = 0 \\text{ (fail)}.\n\\end{cases}\n$$\n\n:::fragment\n\nEquivalently:\n\n$$\nP(K = k) = p^k(1-p)^{1-k}, \\quad k \\in \\{0, 1\\}\n$$\n:::\n\n\n:::fragment\n\n- Mean: $E[K] = p$\n- Variance: $\\text{Var}(K) = p(1-p)$\n\n:::\n:::\n:::::\n\n\n## Binomial Distribution \n\nSuppose $n$ students take the same exam and let $k$ be the number who pass.\n\n$$\nP(K = k) \\;=\\; \\binom{n}{k}\\, p^{k}\\,(1-p)^{n-k}, \\quad k = 0, 1, \\ldots, n\n$$\n\n:::: columns\n::: {.column width=\"60%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-26-1.svg)\n:::\n:::\n\n\n:::\n::: {.column width=\"40%\"}\n\n\n- Mean: $E[K] = np$\n- Variance: $\\text{Var}(K) = np(1-p)$\n- Sum of $n$ independent Bernoulli trials\n- Bernuolli special case of Binomial when $n = 1$\n:::\n:::::\n\n\n## Bernoulli\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrbinom(n = 1, size = 1, prob = 0.7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmany = rbinom(n = 100000, size = 1, prob = 0.7); head(many)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 1 1 0 1 1\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Mean\n(p = mean(many))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.70097\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Variance\n(p*(1-p))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2096111\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(many)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2096132\n```\n\n\n:::\n:::\n\n\n## Binomial\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn = 10; p = 0.7\nrbinom(n = 1, size = n, prob = 0.7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmany = rbinom(n = 100000, size = n, prob = p); head(many)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6 7 5 5 6 7\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn*p # Mean count: E(Y) = np \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(many)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.00063\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn*p*(1-p) # Variance count\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.1\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(many)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.105251\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(many/n) # Mean proportion = p\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.700063\n```\n\n\n:::\n:::\n\n\n\n## Binomial distribution: $n = 20$, $p = 0.5$\n\nWhat is the expected number of students passing the exam?\n\n::: fragment\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-36-1.svg)\n:::\n:::\n\n\n:::\n\n## Binomial distribution: $n = 20$, $p = 0.9$\n\nWhat is the expected number of students passing the exam?\n\n::: fragment\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-37-1.svg)\n:::\n:::\n\n\n::: \n\n## Mean–variance relationship\n\n$$E[Y] = np \\quad \\text{and} \\quad \\mathrm{Var}(Y) = np(1-p)$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-38-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Why not model probability directly?\n\n\\\n**Problem for binary outcomes:**  \n\\\n\\\n\\\n\\\n\\\n\n**Solution:**  \n\\\n\\\n\n\n\n## Why not model probability directly?\n\n\\\n**Problem for binary outcomes:**  \nA linear model can predict values outside, but probabilities must stay between 0 and 1.\n\\\n\\\n\\\n\n**Solution:**  \nTransform $p$ using a **link function** that maps $[0,1]$ to $(-\\infty, +\\infty)$, then model *that* with a linear predictor.\n\n\n# Odds, Odds Ratio and Log-Odds {.section}\n\n## Odds\n\nThe **odds** compare the probability of success to the probability of failure:\n\n$$\n\\text{odds}(p) = \\frac{p}{1-p}\n$$\n\n\\\n\n::: fragment\n**Example:** If $p=0.80$ (80% chance of passing), then\n\\\n\n$$\\text{odds}=\\frac{0.80}{0.20}=4$$\n\nThis means 4 successes per 1 failure, on average.\n:::\n\n\\\n\n::: fragment\n**Range:** Odds go from $0$ to $+\\infty$, but cannot be negative.\n:::\n\n\n## Log-odds (logit)\n\nTaking the **logarithm** of the odds gives the **logit**:\n\n\n$$\n\\text{logit}(p) = \\log\\!\\left(\\frac{p}{1-p}\\right)\n$$\n\n::: fragment\n**Example:** If $p=0.80$, then $\\text{odds}=4$ and\n\n\\\n\n$$\\text{logit}(0.80) = \\log(4) \\approx 1.39$$\n\n\\\n:::\n\n::: fragment\n**Range:** Log-odds span $-\\infty$ to $+\\infty$ \n\n\\\n\nThis can be modeled linearly: $\\text{logit}(p_i) = \\beta_0 + \\beta_1 x_i$\n:::\n\n\n## Odds and Log-odds\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-39-1.svg){fig-align='left'}\n:::\n:::\n\n\n\nLog-Odds: “Equally strong” probabilities on opposite sides of 0.5 become equal in magnitude but opposite in sign.\n\n\n## Example\n\nLet $y_i \\in \\{0,1\\}$ (Pass), and\n$p_i = P(y_i=1 \\mid x_i)$ where $x_i$ = hours studied. \n\n\\\n\n::: fragment\n\n::: columns\n::: {.column width=\"52%\"}\n**Odds**: $o_i = \\dfrac{p_i}{1-p_i}$\n\\\n\\\n\\\n\n**Log-odds (logit)**:\n$\\text{logit}(p_i)=\\log\\!\\left(\\dfrac{p_i}{1-p_i}\\right)$.\n:::\n\n::: {.column width=\"48%\"}\n\\\n\\\n**Linear predictor**:\n$$\n\\text{logit}(p_i) = \\eta_i,\\quad\n\\eta_i = \\beta_0 + \\beta_1 x_i .\n$$\n:::\n:::::\n:::\n\n\\\n\n::: fragment\n$\\beta_1$ describes change in *log-odds*, not probability. What does that mean?\n:::\n\n\n\n## From Log-Odds to Odds\n\n**Starting point**: $\\log\\!\\left(\\dfrac{p(x)}{1-p(x)}\\right) =  \\beta_0 + \\beta_1 x$ \n\n\\\n\n::: fragment\nExponentiating both sides gives the **odds**: \n\n$$\n\\text{odds}(x) = \\frac{p(x)}{1-p(x)} = \\exp(\\beta_0 + \\beta_1 x)\n$$ \n:::\n\n\\\n\n::: fragment\nUsing properties of exponents: $\\exp(\\beta_0 + \\beta_1 x) = \\exp(\\beta_0) \\cdot \\exp(\\beta_1 x)$\n:::\n\n## Comparing Two Values of $x$: The Odds Ratio\n\n**When $x = 0$**: \n\n$$\n\\text{odds}(0) = \\exp(\\beta_0)\n$$\n\n\\\n\n::: fragment\n**When $x = 1$**: \n\n$$\n\\text{odds}(1) = \\exp(\\beta_0 + \\beta_1) = \\exp(\\beta_0) \\cdot \\exp(\\beta_1)\n$$\n:::\n\n\\\n\n::: fragment\n**The ratio**:\n\n$$\n\\frac{\\text{odds}(1)}{\\text{odds}(0)} = \\frac{\\exp(\\beta_0) \\cdot \\exp(\\beta_1)}{\\exp(\\beta_0)} = \\exp(\\beta_1)\n$$\n:::\n\n## The Odds Ratio\n\nThis ratio is constant **regardless of where you start**!\n\n\\\n\n**For any 1-unit increase** (from $x$ to $x+1$):\n\n$$\n\\frac{\\text{odds}(x+1)}{\\text{odds}(x)}\n=\n\\frac{\\exp(\\beta_0) \\cdot \\exp(\\beta_1(x+1))}{\\exp(\\beta_0) \\cdot \\exp(\\beta_1 x)}\n$$\n\n::: fragment\nThe $\\exp(\\beta_0)$ terms cancel:\n\n$$\n\\frac{\\text{odds}(x+1)}{\\text{odds}(x)}\n=\n\\frac{\\exp(\\beta_1(x+1))}{\\exp(\\beta_1 x)}\n= \\exp(\\beta_1)\n$$\n:::\n\n\n\\\n\n::: fragment\nThis constant multiplier is the **odds ratio (OR)**.\n\n\\\n\n$\\rightarrow$ $\\exp(\\beta_1)$ tells us how the odds multiply for each 1-unit increase in $x$.\n:::\n\n## Concrete Example: One-Unit Change\n\nWith $\\beta_1 = 0.8$, **for a 1-unit increase** ($x \\to x+1$):\n\n\\\n\n$$\n\\frac{\\text{odds}(x+1)}{\\text{odds}(x)} = \\exp(\\beta_1) = \\exp(0.8) \\approx 2.23\n$$\n\n\\\n\n::: fragment\n**What this means**: \n\n- Studying 1 more hour multiplies your odds of passing by 2.23 (not the probability!)\n- This is true whether you go from 2 to 3 hours, 5 to 6 hours, or 10 to 11 hours\n:::\n\n\n## Linear on log-odds\n\nEqual changes in $x$ $\\rightarrow$ equal changes in log-odds  \n**BUT** odds multiply by constant factor $\\exp(\\beta_1)$\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-40-1.svg){fig-align='left'}\n:::\n:::\n\n\nWith $\\beta_0=0, \\beta_1=0.8$: each +1 step in $x$ adds $0.8$ to log-odds, multiplies odds by $\\exp(0.8) \\approx 2.23$\n\n## Generalizing: Any Two Values\n\n**For any difference** $\\Delta x = x_2 - x_1$:\n\\\n\n$$\n\\frac{\\text{odds}(x_2)}{\\text{odds}(x_1)}\n= \\frac{\\exp(\\beta_0 + \\beta_1 x_2)}{\\exp(\\beta_0 + \\beta_1 x_1)}\n= \\exp\\!\\bigl(\\beta_1(x_2-x_1)\\bigr)\n$$\n\n\\\n\n::: fragment\n**Example** ($\\beta_1=0.8$): comparing $x=6$ to $x=2$ ($\\Delta x = 4$):\n\n\\\n\n$$\n\\frac{\\text{odds}(6)}{\\text{odds}(2)}\n= \\exp\\!\\bigl(0.8 \\times 4\\bigr)\n= \\exp(3.2) \\approx 24.5\n$$\n\n\\\n\nStudying 4 more hours multiplies odds by 24.5!\n:::\n\n\n## What About Probability?\n\nWe've been working with **odds** and **odds ratios**, but what about $p(x)$?\n\n\\\n\n::: fragment\n**From odds back to probability**:\n\n$$\np(x) = \\frac{\\text{odds}(x)}{1 + \\text{odds}(x)} = \\frac{\\exp(\\beta_0 + \\beta_1 x)}{1 + \\exp(\\beta_0 + \\beta_1 x)}\n$$\n\n:::\n\n\\\n\n::: fragment\nUnlike odds, probability changes are **not constant**!\n\n- If $p = 0.1$, adding 1 hour might increase it to $p = 0.20$ (+0.10)\n- If $p = 0.5$, adding 1 hour might increase it to $p = 0.69$ (+0.19)\n- If $p = 0.9$, adding 1 hour might increase it to $p = 0.96$ (+0.06)\n\n:::\n\n## Not linear in probability\n\n$\\rightarrow$ **Same $\\beta_1$, same OR (2.23×), but different probability changes!**\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-41-1.svg){fig-align='left'}\n:::\n:::\n\n\n\n## Let's try\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta0 = 0\n# odds?\n# p?\n```\n:::\n\n\n::: fragment\n\n::: {.cell}\n\n```{.r .cell-code}\n(odds = exp(beta0))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n\n:::\n\n::: fragment\n\n::: {.cell}\n\n```{.r .cell-code}\n(p = odds/(1+odds))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5\n```\n\n\n:::\n\n```{.r .cell-code}\n(p = exp(beta0)/(1+exp(beta0)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5\n```\n\n\n:::\n:::\n\n:::\n\n## Let's try\n\n::: fragment\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta1 = 1\n# what happen moving from x = 0 to x = 1?\n```\n:::\n\n:::\n\n::: fragment\n\n::: {.cell}\n\n```{.r .cell-code}\n(odds1 = odds*exp(beta1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.718282\n```\n\n\n:::\n\n```{.r .cell-code}\n(odds1 = exp(beta0)*exp(beta1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.718282\n```\n\n\n:::\n:::\n\n:::\n\n::: fragment\n\n::: {.cell}\n\n```{.r .cell-code}\n(p1 = odds1 /(1+odds1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7310586\n```\n\n\n:::\n\n```{.r .cell-code}\n(p1 = exp(beta0 + beta1)/(1+exp(beta0+beta1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7310586\n```\n\n\n:::\n:::\n\n:::\n\n\n## Let's try\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta0 = 3\n# odds?\n# p?\n```\n:::\n\n\n::: fragment\n\n::: {.cell}\n\n```{.r .cell-code}\n(odds = exp(beta0))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 20.08554\n```\n\n\n:::\n:::\n\n:::\n\n::: fragment\n\n::: {.cell}\n\n```{.r .cell-code}\n(p = odds/(1+odds))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9525741\n```\n\n\n:::\n\n```{.r .cell-code}\n(p = exp(beta0)/(1+exp(beta0)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9525741\n```\n\n\n:::\n:::\n\n:::\n\n## Let's try\n::: fragment\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta1 = 1\n# what happen moving from x = 0 to x = 1?\n```\n:::\n\n:::\n\n::: fragment\n\n::: {.cell}\n\n```{.r .cell-code}\n(odds1 = odds*exp(beta1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 54.59815\n```\n\n\n:::\n:::\n\n:::\n\n::: fragment\n\n::: {.cell}\n\n```{.r .cell-code}\n(p1 = odds1 /(1+odds1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9820138\n```\n\n\n:::\n\n```{.r .cell-code}\n(p1 = exp(beta0 + beta1)/(1+exp(beta0+beta1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9820138\n```\n\n\n:::\n:::\n\n:::\n\n\n##\n\n\n![](/img/coffe.png){fig-align=\"center\" width=\"500\"}\n\n\n[Vecteezy](https://www.vecteezy.com/free-png/funny)\n\n\n# Extra slides \n\n## The GLM variance structure\n\n::: incremental\n\n- In GLMs, variance is **not constant** — it changes with the mean: $\\mathrm{Var}(Y\\mid X) = \\phi V(\\mu)$\n\n- The **variance function** $V(\\mu)$ defines how variance depends on the mean (e.g., $V(\\mu) = \\mu$ for Poisson, $V(\\mu) = \\mu(1-\\mu)$ for binomial)\n\n- Once you model $\\mu(x)$, the variance is also determined **at each x** (up to the scale $\\phi$).\n\n- **Heteroscedasticity** is implicit in GLMs\n\n:::\n\n## What is dispersion?\n\n::: incremental\n\n- **Dispersion** quantifies how spread out the data are around the mean\n\n- In the **Normal distribution**: $Y \\sim N(\\mu, \\sigma^2)$, the dispersion parameter is $\\phi = \\sigma^2$ (the variance)\n\n- $\\sigma^2$ controls spread **independently** of $\\mu$ — you can have any mean with any variance\n\n- This **separation** between location ($\\mu$) and scale ($\\sigma^2$) is a defining feature of the Normal family\n\n:::\n\n## Dispersion ($\\phi$) is fixed\n\n::: incremental\n\n- Different families handle dispersion **differently**\n\n- **Normal**: $\\text{Var}(Y) = \\sigma^2$ (constant, independent of mean)\n\n- **Poisson**: $\\text{Var}(Y) = \\mu$ (variance **equals** the mean, $\\phi = 1$ fixed)\n\n- **Binomial**: $\\text{Var}(Y) = n\\mu(1-\\mu)$ (variance is a **function** of the mean, $\\phi = 1$ fixed)\n\n- Fixing $\\phi = 1$ means the **entire variance is determined by** $V(\\mu)$ — no additional free parameter for \"extra variability\".\n\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
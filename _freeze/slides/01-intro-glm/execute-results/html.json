{
  "hash": "51e9b69dfd4200108832f1510ae32003",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Introduction to Generalized Linear Models for Psychology\nsubtitle: Generalized Linear Models Workshop \nformat: minimal-revealjs\nexecute: \n  echo: true\ndate: last-modified\ndate-format: \"*[Last modified:] DD-MM-YYYY*\"\ntoc: true\nhtml-math-method: mathml\nfig-dpi: 300\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n# Linear Regression {.section}\n\n## Linear Regression\n\nEstimate the expected (average) outcome given predictors.\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='left'}\n:::\n:::\n\n\n> The [expected value](https://en.wikipedia.org/wiki/Expected_value) of\n> a random variable with a finite number of outcomes is a weighted\n> average of all possible outcomes.\n\n## Linear Regression\n\nA constant change in **x** leads to a constant change in the **expected outcome**.\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Normal linear regression: `lm(y~x)`\n\nFor a fixed $x_i$, the model describes the **distribution of possible outcomes** around the expected value.\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-4-1.svg){fig-align='left'}\n:::\n:::\n\n\n\n# The Normal distribution {.section}\n\n## Normal distribution\n\nA Normal distribution has parameters $\\mu$ (mean) and $\\sigma^2$\n(variance):\n\n$$f(y \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( - \\frac{(y - \\mu)^2}{2\\sigma^2} \\right)$$\n\n<br/>\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-5-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Normal distribution\n\nA Normal distribution has parameters $\\mu$ (mean) and $\\sigma^2$\n(variance):\n\n$$f(y \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( - \\frac{(y - \\mu)^2}{2\\sigma^2} \\right)$$\n\n<br/>\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-6-1.svg){fig-align='center'}\n:::\n:::\n\n  \n\n\n## Normal distribution\n\nA Normal distribution has parameters $\\mu$ (mean) and $\\sigma^2$\n(variance):\n\n$$f(y \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( - \\frac{(y - \\mu)^2}{2\\sigma^2} \\right)$$\n\n<br/>\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-7-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Normal linear regression\n\n:::: columns\n::: column \n$\\mu_i = \\beta_0 + \\beta_1 x_i,$ \n\n$Y_i \\mid x_i \\sim \\mathcal{N}(\\mu_i,\\sigma^2)$\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-8-1.svg){fig-align='left'}\n:::\n:::\n\n\n:::\n\n::: column \n::: fragment\n$\\quad Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i,$\n\n$\\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-9-1.svg){fig-align='left'}\n:::\n:::\n\n:::\n:::\n\n:::::\n\n::: fragment\n“The distribution of the errors” and “the distribution of the data given x” are two equivalent ways of expressing the same modeling assumption.\n:::\n\n\n## Normal distribution\n\n-   **Support**: $-\\infty, +\\infty$\n-   **Mean** (expected value): $\\mu$\n-   **Variance**: $\\sigma^2$\n-   **Independence:** changing the mean does *not* change the variance!\n\n## Why does it matter?\n\nReal-world measurements are not automatically Normal, **Normality** is a **modeling choice**.\n\n-   **Support**: what range of values is possible\n-   **Mean** (expected value): what value to predict on average\n-   **Variance**: how much variation around the mean\n-   **Mean–variance relationship**: does variance change with the mean?\n\n\n## Reaction Times\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-10-1.svg)\n:::\n:::\n\n\n## Exam pass/fail\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-11-1.svg)\n:::\n:::\n\n\n\n## Number of errors in a task\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-12-1.svg)\n:::\n:::\n\n\n\n# Probability Distributions and Random Variables {.section}\n\n## Why probability distributions?\n\nOur models do not fit our data exactly ...\n\n::: incremental\n1.  **Predict the average** value of the outcome given the predictors\n2.  **Summarize the variation** in this prediction\n:::\n\n::: fragment\nProbability distributions help us **characterize** the **variation** that remains ***after*** predicting the average.\n\n$\\rightarrow$ Quantify **uncertainty in the estimated parameters** of the model!\n:::\n\n\n\n\n## Probability Distributions\n\nA **probability distribution** describes all possible values a random variable $Y$ can take and their associated probabilities, given certain parameters.\n\n\\\n\n$$f(y \\mid \\boldsymbol{\\theta})$$\n\n\\\n\nWhere:\n\n- $Y$ is the random variable (uppercase)\n- $y$ is a specific value it can take (lowercase)\n- $\\boldsymbol{\\theta}$ is a vector of parameters (e.g., mean, variance)\n- $f(\\cdot)$ is the probability function\n\n## Discrete vs. Continuous Variables\n\n:::: columns\n::: column\n**Discrete** random variables (counts, binary outcomes, etc.)\n\n- **Probability Mass Function (PMF)**\n- $P(Y = y \\mid \\boldsymbol{\\theta})$ \n- The **probability** of observing exactly $y$\n- $P(Y = 3) = 0.25$ means 25% chance of getting 3\n\n:::\n::: column\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-13-1.svg)\n:::\n:::\n\n:::\n:::::\n\n## Discrete vs. Continuous Variables\n\n:::: columns\n::: column\n**Continuous** random variables (time, weight, temperature, etc.)\n\n- **Probability Density Function (PDF)**\n- $f(y \\mid \\boldsymbol{\\theta})$\n- The **probability density** at $y$ (not a probability!)\n- $P(Y = y) = 0$ for any specific value\n:::\n::: column\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-14-1.svg)\n:::\n:::\n\n\n:::\n:::::\n\n## Distributions and linear regression\n\nPredict the **average value of $Y$** and quantify the remaining variation:\n\n::: incremental\n- **Expected value (mean)**: $E[Y]=\\mu$.  \n  The long-run average of $Y$ under its distribution.\n\n- **Conditional expected value**: $E[Y\\mid X]=\\mu(X)$.  \n  The long-run average outcome **for a fixed predictor value** $X=x$.\n\n- **Variance**: $\\mathrm{Var}(Y)$ (or, in regression, $\\mathrm{Var}(Y\\mid X)$).  \n  How much outcomes vary around the mean (overall or at a given $x$).\n\n:::\n\n::: fragment\nIn normal linear and generalized linear models we generally include **predictors** on the **mean** of the distribution.\n:::\n\n\n## Why is it important?\n\nThe **definitions** of expected value and variance are universal,\nbut how we compute them (and what they equal) depends on the distribution’s parameters. \n\n::: fragment\nFor a **Normal** distribution $Y \\sim \\mathcal{N}(\\mu,\\sigma^2)$,\nthe parameters are exactly the mean and variance.\n\n::: incremental\n- **Expected value (mean):** $E[Y]=\\mu$. \n- **Variance:** $\\mathrm{Var}(Y)=\\sigma^2$. \n:::\n:::\n\n\n## \n\n| Distribution | Support | mean: $E[Y]$ | variance: $\\mathrm{Var}(Y)$ |\n|------------------|------------------|:----------------:|:-----------------:|\n| **Normal**: $f(y \\text{ | } \\mu,\\sigma^2)$ | $y \\in \\mathbb{R}$ | $\\mu$ | $\\sigma^2$ |\n| **Gamma**: $f(y \\text{ | } \\alpha,\\beta)$ | $y \\in (0,\\infty)$ | $\\alpha/\\beta$ | $\\alpha/\\beta^2$ |\n| **Binomial**: $f(y \\text{ | } n,p)$ | $y \\in \\{0,1,\\dots,n\\}$ | $np$ | $np(1-p)$ |\n| **Poisson**: $f(y \\text{ | } \\lambda)$ | $y \\in \\{0,1,2,\\dots\\}$ | $\\lambda$ | $\\lambda$ |\n\n\n\n## Example: Passing the exam\n\nProbability of passing the exam as a function of how many hours students\nstudy.\n\n::::: columns\n::: {.column width=\"30%\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n   id studyh passed\n1   1     29      0\n2   2     79      1\n3   3     41      1\n4   4     88      1\n5   5     94      1\n6   6      5      0\n7   7     53      0\n8   8     89      1\n9   9     55      1\n10 10     46      0\n11 11     96      1\n12 12     45      0\n13 13     68      1\n14 14     57      0\n15 15     10      0\n16 16     90      1\n17 17     25      0\n18 18      4      0\n19 19     33      1\n20 20     95      1\n21 21     89      1\n22 22     69      0\n23 23     64      0\n24 24     99      1\n25 25     66      1\n26 26     71      1\n27 27     54      0\n28 28     59      1\n29 29     29      0\n30 30     15      0\n31 31     96      1\n32 32     90      1\n33 33     69      1\n34 34     80      0\n35 35      2      0\n36 36     48      0\n37 37     76      0\n38 38     22      1\n39 39     32      1\n40 40     23      0\n41 41     14      0\n42 42     41      0\n43 43     41      0\n44 44     37      0\n45 45     15      0\n46 46     14      0\n47 47     23      0\n48 48     47      0\n49 49     27      0\n50 50     86      1\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"70%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n# number of students that have passed the exam\nsum(dat_exam$passed) \n#> [1] 22\n# proportion of students that have passed the exam\nmean(dat_exam$passed) \n#> [1] 0.44\n\n# study hours and passing the exam\ntapply(dat_exam$studyh, dat_exam$passed, mean)\n#>        0        1 \n#> 35.46429 73.04545\n#> \ntable(dat_exam$passed, cut(dat_exam$studyh, breaks = 4))\n#>    (1.9,26.2] (26.2,50.5] (50.5,74.8] (74.8,99.1]\n#>  0         11          10           5           2\n#>  1          1           3           6          12\n#>  \ntapply(dat_exam$passed, cut(dat_exam$studyh, breaks = 4), mean)\n#>   (1.9,26.2] (26.2,50.5] (50.5,74.8] (74.8,99.1] \n#>   0.08333333  0.23076923  0.54545455  0.85714286\n```\n:::\n\n:::\n:::::\n\n## Visualize data\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-17-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Fitting a Normal Linear Model\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-18-1.svg){fig-align='left'}\n:::\n:::\n\n\n**Do you see something strange?**\n\n## Fitting a Generalized Linear Model\n\nThe model should consider both the **support** of the $y$ variable and\nthe **non-linear pattern**!\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-19-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Example: Reaction times\n\nReaction times on two-choice questions as a function of hours studied.\n\n::::: columns\n::: {.column width=\"35%\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n   id studyh         rt\n1   1     29  10.947819\n2   2     79   3.540496\n3   3     41  11.268397\n4   4     88   4.392048\n5   5     94   3.715430\n6   6      5  58.248106\n7   7     53  20.919784\n8   8     89   6.736173\n9   9     55  17.673947\n10 10     46  17.588858\n11 11     96   4.606094\n12 12     45   9.470837\n13 13     68   8.466946\n14 14     57  12.035441\n15 15     10  88.036080\n16 16     90   2.616037\n17 17     25  40.789503\n18 18      4  39.961460\n19 19     33  25.712424\n20 20     95   3.359597\n21 21     89   4.885902\n22 22     69  10.306010\n23 23     64   5.981269\n24 24     99   1.489239\n25 25     66   5.558787\n26 26     71   9.449445\n27 27     54  16.912798\n28 28     59   9.531816\n29 29     29  68.481525\n30 30     15  21.572576\n31 31     96   4.640696\n32 32     90   5.915181\n33 33     69   9.313805\n34 34     80   9.680069\n35 35      2 149.169447\n36 36     48  13.338629\n37 37     76   4.569221\n38 38     22  25.811964\n39 39     32  36.203759\n40 40     23  36.008034\n41 41     14  24.000655\n42 42     41  13.996114\n43 43     41  21.102289\n44 44     37  28.233519\n45 45     15  38.237882\n46 46     14  31.721792\n47 47     23  41.982604\n48 48     47  27.842142\n49 49     27  38.937573\n50 50     86   4.649678\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"65%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nrange(dat_rt$rt)\n#>  [1]   1.489239 149.169447\n\n# mean RT as a function of studyh\ntapply(dat_rt$rt, cut(dat_rt$studyh, breaks = 4), mean)\n#>  (1.9,26.2] (26.2,50.5] (50.5,74.8] (74.8,99.1] \n#>   49.628342   24.855683   11.468186    4.628276 \n\n# variance RT as a function of studyh\ntapply(dat_rt$rt, cut(dat_rt$studyh, breaks = 4), var)\n#> (1.9,26.2] (26.2,50.5] (50.5,74.8] (74.8,99.1] \n#> 1303.176485  265.296600   24.566131    3.790422\n```\n:::\n\n:::\n:::::\n\n## Visualize data\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-22-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Fitting a Normal Linear Model\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-23-1.svg){fig-align='left'}\n:::\n:::\n\n\n**Do you see something strange?**\n\n## Fitting a Generalized Linear Model\n\nThe model should consider both the **support** of the $y$ variable and\nthe **non-linear pattern**!\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-24-1.svg){fig-align='left'}\n:::\n:::\n\n\n# Generalized Linear Models (GLM) {.section}\n\n## General ideas\n\n::: incremental\n-   using distributions beyond the Normal\n-   modeling non linear functions on the response scale\n-   taking into account mean-variance relationships\n:::\n\n## The three ingredients of a GLM\n\n::: incremental\n1.  **Random Component**: Choose a distribution\n\n2.  **Systematic Component**: Linear predictor\n    $\\eta_i = \\beta_0 + \\beta_1 x_i$\n\n3.  **Link Function**: Transform the mean $g(\\mu_i) = \\eta_i$\n:::\n\n## Random component\n\nThe **random component** specifies a probability model for $y_i$:\n\n<br/>\n\n$$\ny_i \\mid x_i \\sim \\text{Distribution}(\\text{parameters}).\n$$\n\n<br/>\n\n::: {.fragment fragment-index=\"1\"}\nWhat **support**?\n\n-   Binary: $\\{0,1\\}$ $\\rightarrow$ $\\text{Bernoulli}(p)$ (or\n    $\\text{Binomial}(1,p)$)\n-   Counts: $\\{0,1,2,\\ldots\\}$ $\\rightarrow$ $\\text{Poisson}(\\lambda)$\n-   Any real number: $(-\\infty,\\infty)$ $\\rightarrow$\n    $\\text{Normal}(\\mu,\\sigma^2)$\n:::\n\n## Random component (mean–variance)\n\nChoosing a distribution specifies not only the mean, but also how the variance depends on the mean:\n\n$$\n\\mathrm{Var}(Y \\mid X) = \\phi \\, V(\\mu), \\qquad \\mu = E(Y \\mid X),\n$$\n\n\nwhere $V(\\mu)$ is the **variance function** (determined by the family) and $\\phi$ is the dispersion parameter.\n\n::: fragment\n-   **Normal**: $V(\\mu) = 1$ (constant variance, $\\phi = \\sigma^2$)\n-   **Bernoulli**: $V(\\mu) = \\mu(1-\\mu)$ (typically $\\phi = 1$)\n-   **Poisson**: $V(\\mu) = \\mu$ (typically $\\phi = 1$)\n\n:::callout-note\nIf $\\phi>1$ but you fit a Poisson/Bernoulli model with $\\phi=1$, the coefficient estimates $\\hat\\beta$ may look similar, but the **standard errors are smaller**.\n:::\n\n:::\n\n## Systematic Component\n\nThe **systematic component** is *exactly the same* as in *normal* linear\nregression: we predict a linear combination of predictors.\n\n<br/>\n\n$$\n\\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_k x_{ik}.\n$$ <br/>\n\nBasically it describes how the expected value (i.e., the mean) of the chosen distribution (the random component) varies\naccording to the predictors.\n\n## Link Function\n\nThe **link function** $g(\\cdot)$ connects the expected value (mean)\n$\\mu_i$ of the distribution to the linear predictor $\\eta_i$:\n\n\\\n\n$$\ng(\\mu_i) = \\eta_i\n$$\n\n\\\n\n-   The linear predictor $\\eta_i$ can be any real number:\n    $(-\\infty, +\\infty)$\n-   But $\\mu_i$ (the mean) is **constrained** by the distribution's\n    support\n-   The link function **transforms** $\\mu_i$ to be unbounded\n\n## Common link functions\n\n| Distribution | Support of $y$ | Link | Purpose |\n|----------------|----------------|------------------------|----------------|\n| Normal | $(-\\infty,\\infty)$ | Identity: $g(\\mu)=\\mu$ | No transformation |\n| Binomial | $\\{0,1,\\ldots,n\\}$ | Logit on $p$: $g(p_i)=\\log\\!\\left(\\frac{p}{1-p}\\right)$, where $p=\\mu/n$ | Probability $\\to \\mathbb{R}$ |\n| Poisson | $\\{0,1,2,\\ldots\\}$ | Log: $g(\\mu)=\\log(\\mu)$ | Positive $\\to \\mathbb{R}$ |\n| Gamma | $(0,\\infty)$ | Log: $g(\\mu)=\\log(\\mu)$ | Positive $\\to \\mathbb{R}$ |\n\n## Normal + identity\n\nFor example, a Generalized Linear Model with the **Normal** family and\n**identity link** can be written as:\n\n<br/>\n\n$$\n\\begin{aligned}\ny_i &\\sim \\mathcal{N}(\\mu_i, \\sigma^2) && \\text{Random component} \\\\\n\\mu_i &= \\eta_i && \\text{Link (identity)} \\\\\n\\eta_i &= \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik} && \\text{Systematic component}\n\\end{aligned}\n$$\n\n## Bernoulli + logit\n\nFor example, a Generalized Linear Model with the **Bernoulli** family\nand **logit link** can be written as:\n\n<br/>\n\n$$\n\\begin{aligned}\ny_i \\mid x_i &\\sim \\text{Bernoulli}(p_i) && \\text{Random component} \\\\\n\\text{logit}(p_i) &= \\log\\!\\left(\\frac{p_i}{1-p_i}\\right)=\\eta_i && \\text{Link (logit)} \\\\\n\\eta_i &= \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik} && \\text{Systematic component}\n\\end{aligned}\n$$\n\n# Binomial Logistic Regression {.section}\n\n## Binary outcomes and counts\n\nThe ***Bernoulli*** or the ***Binomial*** distributions can be used as\n**random component** when we have a binary dependent variable or the\nnumber of successes over the total number of trials.\n\n\\\n\n-   **Binary** (pass/fail), one trial per person; or\n-   **Counts of successes** out of $n$ trials (e.g., items correct out\n    of $n$).\n\n## Bernoulli (one student)\n\nLet $k \\in \\{0,1\\}$ indicate whether a student passes the exam:\n\n\\\n\n$$\nf(k; p) =\n\\begin{cases}\np       & \\text{if } k = 1, \\\\\nq = 1-p & \\text{if } k = 0.\n\\end{cases}\n$$\n\n\\\n\nThe probability mass function ${\\displaystyle f}$ of the Bernoulli\ndistribution, over possible outcomes $k$, is $f(k,p) = p^k(1-p)^{1-k}$\n\n\\\n\nWhere $p$ is the probability of success and $k$ the two possible results\n0 and 1. The mean is $p$ and the variance is $p(1-p)$\n\n## One student takes the exam\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrbinom(n = 1, size = 1, prob = 0.7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n\n\n. . .\n\nOver 10,000 students?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmany = rbinom(n = 100000, size = 1, prob = 0.7); head(many)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0 1 0 1 1 1\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Mean\n(p = mean(many))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.70083\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Variance\n(p*(1-p))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2096673\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(many)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2096694\n```\n\n\n:::\n:::\n\n\n## Binomial (a class of students)\n\nNow let $k$ be the number of students who pass out of $n$ students who\ntake the same exam.\n\n\\\n\nThe probability of having $k$ success (e.g., 0, 1, 2, etc.), out of $n$\ntrials with a probability of success $p$ (and failing $q = 1 - p$) is:\n\n\\\n\n$$\nf(n, k, p) \\;=\\; \\binom{n}{k}\\, p^{k}\\,(1-p)^{\\,n-k}\n$$\n\n\\\n\nThe $np$ is the mean of the binomial distribution and $np(1-p)$ is the\nvariance. The binomial distribution is just the repetition of $n$\nindependent Bernoulli trials.\n\n## Ten students take the exam\n\nHow many pass?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn = 10; p = 0.7\nrbinom(n = 1, size = n, prob = 0.7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 9\n```\n\n\n:::\n:::\n\n\n. . .\n\nOver 10,000 repetition ...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmany = rbinom(n = 100000, size = n, prob = p); head(many)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5 8 6 6 8 8\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn*p # Mean count: E(Y) = np \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(many)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.00082\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn*p*(1-p) # Variance count: Var(y) = np(1-p)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.1\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(many)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.1049\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(many/n) # Mean proportion = p\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.700082\n```\n\n\n:::\n:::\n\n\n## Binomial distribution: $n = 20$, $p = 0.9$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-34-1.svg)\n:::\n:::\n\n\n## Binomial distribution: $n = 20$, $p = 0.5$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-35-1.svg)\n:::\n:::\n\n\n## Mean–variance relationship\n\n$E[Y] = np \\quad \\text{and} \\quad \\mathrm{Var}(Y) = np(1-p)$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-36-1.svg)\n:::\n:::\n\n\n**Variance is not constant!**\n\n# Odds and Logit Link {.section}\n\n## Why not model probability directly?\n\n\\\n\n**Problem for binary outcomes:**\n\\\n\\\n\\\n\n**Solution:**  \n\n\n## Why not model probability directly?\n\n\\\n\n**Problem for binary outcomes:** \nA linear model can predict values outside [0,1], but probabilities must stay between 0 and 1.\n\n\\\n\n**Solution:** \nTransform the probability to a scale that ranges from $-\\infty$ to $+\\infty$, then model *that* with a linear predictor. \n\n\n## Odds\n\nThe **odds** compare the probability of success to the probability of failure: \n\\\n\n$$\n\\text{odds} = \\frac{p}{1-p}, \\qquad p = \\frac{\\text{odds}}{\\text{odds}+1}\n$$ \n\n\\\n\n::: fragment\n**Example:** If $p=0.80$ (80% chance of passing), then \n\\\n\n$$\n\\text{odds}=\\frac{0.80}{0.20}=4.\n$$ \n\n\\\nWe say the odds are **4-to-1** in favor of passing (expect 4 passes for every 1 failure, on average). \n:::\n\n\\\n\n::: fragment\n**Range:** Odds go from 0 to $+\\infty$, but still can't be negative. \n:::\n\n\n## Logit link\n\nTaking the **logarithm** of the odds gives us a scale from $-\\infty$ to $+\\infty$:\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-37-1.svg){fig-align='left'}\n:::\n:::\n\n\n\n\n## Inverse logit link \n\n\n$$\n\\text{logit}(p)=\\log\\!\\left(\\frac{p}{1-p}\\right)=\\eta, \\qquad p = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)}.\n$$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-38-1.svg){fig-align='center'}\n:::\n:::\n\n\n**This is what we model linearly:** $\\eta = \\beta_0 + \\beta_1 x$.\n\n## Example\n\nLet $Y_i \\in \\{0,1\\}$ (Pass), and\n$p_i = P(Y_i=1 \\mid x_i)$ where $x_i$ = hours studied. \n\n\\\n\n::: columns\n::: {.column width=\"52%\"}\n**Odds**: $o_i = \\dfrac{p_i}{1-p_i}$  (how much more likely pass than fail). \n\n\\\n\n\\\n**Log-odds (logit)**:\n$\\text{logit}(p_i)=\\log\\!\\left(\\dfrac{p_i}{1-p_i}\\right)$.\n:::\n\n::: {.column width=\"48%\"}\n\\\n\\\n\\\n**Model**:\n$$\n\\text{logit}(p_i) = \\eta_i,\\quad\n\\eta_i = \\beta_0 + \\beta_1 x_i .\n$$\n:::\n:::\n\n\\\n\n::: fragment\n\nWe model the **log-odds** as linear in $x$. What does this mean for the **odds** and **probabilities**?\n:::\n\n## Odds ratios\n\nOur model: $\\text{logit}(p_i) = \\beta_0 + \\beta_1 x_i$ \n\n\\\n\nExponentiating both sides gives the **odds**: \n\n\\\n\n$$\no(x) = \\frac{p(x)}{1-p(x)} = \\exp(\\beta_0 + \\beta_1 x).\n$$ \n\n\\\n\n**At $x=0$:** $o(0)=\\exp(\\beta_0)$.\n\n\\\n\n**At $x=1$:** $o(1)=\\exp(\\beta_0+\\beta_1)=\\exp(\\beta_0)\\exp(\\beta_1)$. \n\n\\\n\n## Odds ratios\n\n**Odds ratio** for +1 hour:\n\n\\\n\n$$\n\\frac{o(1)}{o(0)}=\\frac{\\exp(\\beta_0)\\exp(\\beta_1)}{\\exp(\\beta_0)}\n=\\exp(\\beta_1).\n$$ \n\n\\\n\nso $\\exp(\\beta_1)$ is the multiplicative change in odds for each additional study hour. \n\n## Linear on log-odds\n\nEqual changes in $x$  $\\rightarrow$ equal changes in $\\eta$\n(because $\\eta$ is linear in $x$), but not in odds. Each +1 increase in $x$ multiplies the odds by $\\exp(\\beta_1)$. \n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-39-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Not linear in probability\n\nEqual changes in $x$ generally **do not** correspond to equal\nchanges in $p$.\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-40-1.svg){fig-align='left'}\n:::\n:::\n\n\n## \n\n![](/img/coffe.png){fig-align=\"center\" width=\"500\"}\n\n[Vecteezy](https://www.vecteezy.com/free-png/funny)\n\n## References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
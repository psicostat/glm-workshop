{
  "hash": "2fc354347e37ddd75c94a6d5cdf2d9f0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Introduction to Generalized Linear Models for Psychology\nsubtitle: Generalized Linear Models Workshop \nformat: minimal-revealjs\nexecute: \n  echo: true\ndate: last-modified\ndate-format: \"*[Last modified:] DD-MM-YYYY*\"\ntoc: true\nhtml-math-method: mathml\nfig-dpi: 300\n---\n\n\n\n# Linear Regression  {.section}\n\n## Linear Regression \n\nEstimate the expected (average) outcome given predictors.\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='left'}\n:::\n:::\n\n\n> The [expected value](https://en.wikipedia.org/wiki/Expected_value) of a random variable with a finite number of outcomes is a weighted average of all possible outcomes.\n\n## Linear Regression\n\nA constant change in **x** leads to a constant change in the **expected outcome**.\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='left'}\n:::\n:::\n\n\n\n## Normal linear regression: `lm(y~x)`\n\nFor a fixed $x_i$, the model describes the **distribution of possible outcomes** around the mean $\\mu_i$.\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-4-1.svg){fig-align='left'}\n:::\n:::\n\n\n> The [expected value](https://en.wikipedia.org/wiki/Expected_value) of a random variable with a finite number of outcomes is a weighted average of all possible outcomes.\n\n# The Normal distribution {.section}\n\n## Normal distribution\n\nA Normal distribution has parameters $\\mu$ (mean) and $\\sigma^2$ (variance):\n\n$$f(y \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( - \\frac{(y - \\mu)^2}{2\\sigma^2} \\right)$$\n\n<br/>\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-5-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Normal distribution\n\nA Normal distribution has parameters $\\mu$ (mean) and $\\sigma^2$ (variance):\n\n$$f(y \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( - \\frac{(y - \\mu)^2}{2\\sigma^2} \\right)$$\n\n<br/>\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-6-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Normal distribution\n\nA Normal distribution has parameters $\\mu$ (mean) and $\\sigma^2$ (variance):\n\n$$f(y \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( - \\frac{(y - \\mu)^2}{2\\sigma^2} \\right)$$\n\n<br/>\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-7-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Normal linear regression\n\n:::: columns \n:::column\n\\\n$y_i \\mid x_i \\sim \\mathcal{N}(\\mu_i,\\sigma^2), \\qquad \\mu_i = \\beta_0 + \\beta_1 x_i$\n:::\n::: column\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-8-1.svg){fig-align='left'}\n:::\n:::\n\n:::\n:::::\n::: fragment\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-9-1.svg){fig-align='left'}\n:::\n:::\n\n:::\n\n## Normal distribution\n\n-   **Support**: $-\\infty, +\\infty$\n-   **Mean** (expected value): $\\mu$\n-   **Variance**: $\\sigma^2$\n-   **Independence:** changing the mean does *not* change the variance!\n\n\n## Why do these matter for regression?\n\nWhen building a model, we need to know:\n\n-   **Support**: what range of values is possible\n-   **Mean** (expected value): what value to predict on average\n-   **Variance**: how much variation around the mean\n-   **Mean–variance relationship**: does variance change with the mean?\n\n## Where the Normal model breaks down\n\n### Reaction Times\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-10-1.svg)\n:::\n:::\n\n\n## Where the Normal model breaks down\n\n### Exam pass/fail\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-11-1.svg)\n:::\n:::\n\n\n## Where the Normal model breaks down\n\n### Number of errors in a task\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-12-1.svg)\n:::\n:::\n\n\n# Probability distributions {.section}\n\n## Understanding distributions\n\nA probability distribution of a random variable $Y$ describes the **probabilities assigned to each possible value** $y$, given certain parameters values.\n\n\\\n\n$$f(y \\mid \\boldsymbol{\\theta})$$\n\n\\\n\n-   $y$ is a specific value\n-   $\\boldsymbol{\\theta}$ is a vector of variables (parameters) \n-   $f(\\cdot)$ is the function itself\n\n## Understanding distributions\n\n\n- **Discrete** random variables: counts, binary, etc.\n\n$\\rightarrow$ $f(Y\\mid \\boldsymbol{\\theta})$ becomes $f(Y = y\\mid \\boldsymbol{\\theta})$ because we are assigning a certain probability to a **discrete** variables. The function is called probability **mass** function (PMF). \n\n::: fragment\n\n- **Continuous** random variables: time, weight, etc.\n\n$\\rightarrow$ $f(Y\\mid \\boldsymbol{\\theta})$ and the function is called probability **density** function (PDF).\n\n:::\n\n\\\n\n::: fragment\n\nThe **PMF** return the probability associated with a certain value of $y$\nwhile the **PDF** returns the probability density of a certain value of  $y$\n \n:::\n\n## Moments of the distributions\n\nWhen we use the distributions, we are usually interested in some properties describing the distribution:\n\n::: incremental\n- **First moment**: is the expected value (i.e., the **mean**) of the distribution\n- **Second moment**: is the **variance** of the distribution\n- Third moment: is the skewness\n- Fourth moment: is the kurtosis\n:::\n\n## Why is it important?\n\nMoments are always the same but the way we actual compute them could change according to the distribution.\n\nIn normal linear and generalized linear models we generally include **predictors** on the **mean** of the distribution. \n\nIn the case of ***Normal*** distribution the **mean** and the **variance** are the **same as the parameters** defining the distribution.\n\n\n## Why is important?\n\n| Distribution | Support | mean: $E[Y]$ | variance: $\\mathrm{Var}(Y)$ |\n|------------------|------------------|:----------------:|:------------------:|\n| **Normal**: $f(y \\text{ | } \\mu,\\sigma^2)$ | $y \\in \\mathbb{R}$ | $\\mu$ | $\\sigma^2$ |\n| **Gamma**: $f(y \\text{ | } \\alpha,\\beta)$ | $y \\in (0,\\infty)$ | $\\alpha/\\beta$ | $\\alpha/\\beta^2$ |\n| **Binomial**: $f(y \\text{ | } n,p)$ | $y \\in \\{0,1,\\dots,n\\}$ | $np$ | $np(1-p)$ |\n| **Poisson**: $f(y \\text{ | } \\lambda)$ | $y \\in \\{0,1,2,\\dots\\}$ | $\\lambda$ | $\\lambda$ |\n\n\n## Example: Passing the exam\n\nProbability of passing the exam as a function of how many hours students study:\n\n:::: columns \n::: {.column width=\"30%\"}\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n   id studyh passed\n1   1     29      0\n2   2     79      1\n3   3     41      1\n4   4     88      1\n5   5     94      1\n6   6      5      0\n7   7     53      0\n8   8     89      1\n9   9     55      1\n10 10     46      0\n11 11     96      1\n12 12     45      0\n13 13     68      1\n14 14     57      0\n15 15     10      0\n16 16     90      1\n17 17     25      0\n18 18      4      0\n19 19     33      1\n20 20     95      1\n21 21     89      1\n22 22     69      0\n23 23     64      0\n24 24     99      1\n25 25     66      1\n26 26     71      1\n27 27     54      0\n28 28     59      1\n29 29     29      0\n30 30     15      0\n31 31     96      1\n32 32     90      1\n33 33     69      1\n34 34     80      0\n35 35      2      0\n36 36     48      0\n37 37     76      0\n38 38     22      1\n39 39     32      1\n40 40     23      0\n41 41     14      0\n42 42     41      0\n43 43     41      0\n44 44     37      0\n45 45     15      0\n46 46     14      0\n47 47     23      0\n48 48     47      0\n49 49     27      0\n50 50     86      1\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"70%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n# number of students that have passed the exam\nsum(dat_exam$passed) \n#> [1] 22\n# proportion of students that have passed the exam\nmean(dat_exam$passed) \n#> [1] 0.44\n\n# study hours and passing the exam\ntapply(dat_exam$studyh, dat_exam$passed, mean)\n#>        0        1 \n#> 35.46429 73.04545\n#> \ntable(dat_exam$passed, cut(dat_exam$studyh, breaks = 4))\n#>    (1.9,26.2] (26.2,50.5] (50.5,74.8] (74.8,99.1]\n#>  0         11          10           5           2\n#>  1          1           3           6          12\n#>  \ntapply(dat_exam$passed, cut(dat_exam$studyh, breaks = 4), mean)\n#>   (1.9,26.2] (26.2,50.5] (50.5,74.8] (74.8,99.1] \n#>   0.08333333  0.23076923  0.54545455  0.85714286\n```\n:::\n\n:::\n::::\n\n\n## Example: Passing the exam\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-15-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Fitting a Normal Linear Model\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-16-1.svg){fig-align='left'}\n:::\n:::\n\n\n\n**Do you see something strange?**\n\n\n## Fitting a Generalized Linear Model\n\nThe model should consider both the **support** of the $y$ variable and the **non-linear pattern**! \n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-17-1.svg){fig-align='left'}\n:::\n:::\n\n\n\n# Generalized Linear Models (GLM) {.section}\n\n## General idea\n\n::: incremental\n- using distributions beyond the Gaussian\n- modeling non linear functions on the response scale\n- taking into account mean-variance relationships\n:::\n## The three ingredients of a GLM\n\n::: incremental\n1.  **Random Component**: Choose a distribution \n\n2.  **Systematic Component**: Linear predictor $\\eta_i = \\beta_0 + \\beta_1 x_i$\n\n3.  **Link Function**: Transform the mean $g(\\mu_i) = \\eta_i$\n\n:::\n\n## 1. Random component\n\nThe **random component** specifies a probability model for $y_i$:\n\n<br/>\n\n$$\ny_i \\mid x_i \\sim \\text{Distribution}(\\text{parameters}).\n$$\n\n<br/>\n\n::: {.fragment fragment-index=\"1\"}\nWhat **support**?\n\n- Binary: $\\{0,1\\}$ $\\rightarrow$ $\\text{Bernoulli}(p)$ (or $\\text{Binomial}(1,p)$)\n- Counts: $\\{0,1,2,\\ldots\\}$ $\\rightarrow$ $\\text{Poisson}(\\lambda)$\n- Any real number: $(-\\infty,\\infty)$ $\\rightarrow$ $\\text{Normal}(\\mu,\\sigma^2)$\n:::\n\n## 1. Random component (mean–variance)\n\nChoosing a distribution specifies not only the mean, but also how the variance depends on the mean:\n\n\\\n\n$$\n\\mathrm{Var}(Y \\mid X) = \\phi \\, V(\\mu), \\qquad \\mu = E(Y \\mid X).\n$$\n\n\\\n\nwhere $V(\\mu)$ is the **variance function** (determined by the family) and $\\phi$ is a constant scale factor.\n\n- **Normal**: $V(\\mu) = 1$ (constant variance, $\\phi = \\sigma^2$)\n- **Bernoulli**: $V(\\mu) = \\mu(1-\\mu)$ (typically $\\phi = 1$)\n- **Poisson**: $V(\\mu) = \\mu$ (typically $\\phi = 1$)\n\n\n\n## 2. Systematic Component\n\nThe **systematic component** is *exactly the same* as in *normal* linear regression: we predict a linear combination of predictors.\n\n<br/>\n\n$$\n\\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_k x_{ik}.\n$$\n<br/>\n\nBasically it describes how the expected value (i.e., the mean, the first moment) \nof the chosen distribution (the random component) varies according to the predictors.\n\n\n## 3. Link Function\n\nThe **link function** $g(\\cdot)$ connects the expected value (mean) $\\mu_i$ of the distribution to the linear predictor $\\eta_i$:\n\n\\\n\n$$\ng(\\mu_i) = \\eta_i\n$$\n\n\\\n\n-   The linear predictor $\\eta_i$ can be any real number: $(-\\infty, +\\infty)$\n-   But $\\mu_i$ (the mean) is **constrained** by the distribution's support\n-   The link function **transforms** $\\mu_i$ to be unbounded\n\n## Common link functions\n\n| Distribution | Support of $y$ | Link | Purpose |\n|---|---|-----|---|\n| Normal | $(-\\infty,\\infty)$ | Identity: $g(\\mu)=\\mu$ | No transformation |\n| Binomial | $\\{0,1,\\ldots,n\\}$ | Logit on $p$: $g(p_i)=\\log\\!\\left(\\frac{p}{1-p}\\right)$, where $p=\\mu/n$ | Probability $\\to \\mathbb{R}$ |\n| Poisson | $\\{0,1,2,\\ldots\\}$ | Log: $g(\\mu)=\\log(\\mu)$ | Positive $\\to \\mathbb{R}$ |\n| Gamma | $(0,\\infty)$ | Log: $g(\\mu)=\\log(\\mu)$ | Positive $\\to \\mathbb{R}$ |\n\n\n\n\n## Normal + identity\n\nFor example, a Generalized Linear Model with the **Normal** family and **identity link** can be written as:\n\n<br/>\n\n$$\n\\begin{aligned}\ny_i &\\sim \\mathcal{N}(\\mu_i, \\sigma^2) && \\text{Random component} \\\\\n\\mu_i &= \\eta_i && \\text{Link (identity)} \\\\\n\\eta_i &= \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik} && \\text{Systematic component}\n\\end{aligned}\n$$\n\n## Binomial + logit\n\nFor example, a Generalized Linear Model with the **Binomial** family and **logit link** can be written as:\n\n<br/>\n\n$$\n\\begin{aligned}\ny_i \\mid x_i &\\sim \\text{Binomial}(n_i, p_i) && \\text{Random component} \\\\\n\\text{logit}(p_i) &= \\log\\!\\left(\\frac{p_i}{1-p_i}\\right)=\\eta_i && \\text{Link (logit)} \\\\\n\\eta_i &= \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik} && \\text{Systematic component}\n\\end{aligned}\n$$\n\n\n# Binomial Logistic Regression {.section}\n\n## Binary outcomes and counts\n\nThe ***Bernoulli*** or the ***Binomial*** distributions can be used as **random component** when we have a binary dependent variable or the number of successes over the total number of trials.\n\n\\\n\n-   **Binary** (pass/fail), one trial per person; or\n-   **Counts of successes** out of $n$ trials (e.g., items correct out of $n$).\n\n\n## Bernoulli (one student)\n\nLet $k \\in \\{0,1\\}$ indicate whether a student passes the exam:\n\n\\\n\n$$\nf(k; p) =\n\\begin{cases}\np       & \\text{if } k = 1, \\\\\nq = 1-p & \\text{if } k = 0.\n\\end{cases}\n$$\n\n\\\n\nThe probability mass function ${\\displaystyle f}$ of the Bernoulli distribution, over possible outcomes $k$, is $f(k,p) = p^k(1-p)^{1-k}$\n\n\\\n\nWhere $p$ is the probability of success and $k$ the two possible results \n0 and 1. The mean is $p$ and the variance is $p(1-p)$\n\n\n## One student takes the exam\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrbinom(n = 1, size = 1, prob = 0.7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n. . .\n\nOver 10,000 students?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmany = rbinom(n = 100000, size = 1, prob = 0.7); head(many)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 1 1 0 1 1\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Mean\n(p = mean(many))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.70085\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Variance\n(p*(1-p))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2096593\n```\n\n\n:::\n\n```{.r .cell-code}\n(sd(many)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2096614\n```\n\n\n:::\n:::\n\n\n## Binomial (a class of students)\n\nNow let $k$ be the number of students who pass out of $n$ students who take the same exam.\n\n\\\n\nThe probability of having $k$ success (e.g., 0, 1, 2, etc.), out of $n$ trials with a\nprobability of success $p$ (and failing $q = 1 - p$) is:\n\n\\\n\n$$\nf(n, k, p) \\;=\\; \\binom{n}{k}\\, p^{k}\\,(1-p)^{\\,n-k}\n$$\n\n\\\n\nThe $np$ is the mean of the binomial distribution and $np(1-p)$ is the variance. The\nbinomial distribution is just the repetition of $n$ independent Bernoulli trials.\n\n## Ten students take the exam\n\nHow many pass?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn = 10; p = 0.7\nrbinom(n = 1, size = n, prob = 0.7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7\n```\n\n\n:::\n:::\n\n\n. . .\n\nOver 10,000 repetition ...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmany = rbinom(n = 100000, size = n, prob = p); head(many)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7 8 8 8 8 9\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn*p # Mean count: E(Y) = np \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn*p*(1-p) # Variance count: Var(y) = np(1-p)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.1\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(many/n) # Mean proportion = p\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.700083\n```\n\n\n:::\n:::\n\n\n## Binomial distribution: $n = 20$, $p = 0.9$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-27-1.svg)\n:::\n:::\n\n\n## Binomial distribution: $n = 20$, $p = 0.5$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-28-1.svg)\n:::\n:::\n\n\n## Mean–variance relationship\n\n$$E[Y] = np \\quad \\text{and} \\quad \\mathrm{Var}(Y) = np(1-p)$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-29-1.svg)\n:::\n:::\n\n\n**Variance is not constant!** \n\n# Odds and Logit Link {.section}\n\n## Odds\n\nBecause probabilities must stay between 0 and 1, we work with **odds**.\n\nLet $p = P(\\text{Pass}=1)$, then the *odds* of passing compare “pass” to “fail”:\n\n\\\n\n$$\n\\text{odds} = \\frac{p}{1-p}, \\qquad \\text{p} = \\frac{odds}{odds+1}\n$$\n\n\\\n\n\n::: fragment\nIf $p=0.80$, then $\\text{odds}=\\frac{0.80}{0.20}=4$.\n:::\n\n\\\n\n::: fragment\nSo passing is **4-to-1** relative to failing (4 expected passes for 1 fail, on average).\n:::\n\n## Logit link (log-odds)\n\nThe *logit* is the log-odds:\n\n$$\n\\text{logit}(p)=\\log\\!\\left(\\frac{p}{1-p}\\right)=\\eta.\n$$\n\n\\\n\nThis maps $p \\in (0,1)$ to any real number, which makes it easier to model with a linear predictor.\n\n\n## Why ? \nThe odds have an interesting property when taking the logarithm. We can express a probability $p$ sing a scale ranging $[+\\infty,-\\infty]$.\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-30-1.svg){fig-align='left'}\n:::\n:::\n\n\n\n## Odds\n\nWith $p_i = P(\\text{Pass}_i = 1 \\mid x_i)$ and $x_i$ hours studied, the model assumes:\n\n\\\n\n$$\n\\begin{aligned}\n\\log\\left(\\frac{p_i}{1-p_i}\\right) &= \\eta_i \\\\\n\\eta_i &= \\beta_0 + \\beta_1 x_i\n\\end{aligned}\n$$\n\nThen the odds:\n\n\\\n\n$$\n\\frac{p_i}{1-p_i}=\\exp(\\beta_0+\\beta_1 x_i).\n$$\\\n\n\n\n\n## Odds ratios\n\nThen the odds:\n\\\n$$\n\\frac{p_i}{1-p_i}=\\exp(\\beta_0+\\beta_1 x_i).\n$$\n\n\\\n\n\nWhen $x = 0$ \n\n$$\n\\frac{p_i}{1-p_i} = \\exp(\\beta_0).\n$$\n\n\\\n\nWhen $x = 1$  \n\n$$\n\\frac{p_i}{1-p_i} =  \\exp(\\beta_0 + \\beta_1) = \\exp(\\beta_0)\\exp(\\beta_1).\n$$\n\n\n## Odds Ratios\n\nThen the ratio of these two odds is:\n\n\\\n\n$$\\frac{\\exp(\\beta_0)\\exp(\\beta_1)}{\\exp(\\beta_0)} = \\exp(\\beta_1)$$\n\n\\\n\nThis means that the odds of passing when increasing study hours by 1 is $\\exp(\\beta_1)$ times greater than at the baseline (i.e., when $x = 0$).\n\n\n\n## Linear on log-odds\n\nA +1 increase in $x$ changes $\\eta$ by a constant amount $\\beta_1$. So equal increases in $x$ correspond to equal increases in log-odds.\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-31-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Inverse link (back to probability)\n\nTo go back to probability we apply the inverse-logit:\n\n$$\np_i = \\frac{e^{\\eta_i}}{1+e^{\\eta_i}}.\n$$\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-32-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Not linear in probability\n\nEqual increases in $x$ generally **do not** correspond to equal increases in $p$, because $p=\\text{logit}^{-1}(\\eta)$ is nonlinear.\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](01-intro-glm_files/figure-revealjs/unnamed-chunk-33-1.svg){fig-align='left'}\n:::\n:::\n\n\n\n## Numerical example ($\\beta_1=0.8$)\n\nHere $\\exp(0.8)\\approx 2.23$, so each +1 hour multiplies the odds by \\~2.23.\n\n| Baseline **p** | Baseline odds $p/(1-p)$ | New odds $=2.23\\times$ odds | New **p** | $\\Delta$ **p** |\n|--------------:|--------------:|--------------:|--------------:|--------------:|\n| 0.10 | 0.11 | 0.25 | 0.20 | +0.10 |\n| 0.20 | 0.25 | 0.55 | 0.36 | +0.16 |\n| 0.36 | 0.55 | 1.22 | 0.55 | +0.20 |\n| 0.55 | 1.22 | 2.73 | 0.73 | +0.18 |\n| 0.73 | 2.73 | 6.07 | 0.86 | +0.13 |\n| 0.86 | 6.07 | 13.51 | 0.93 | +0.07 |\n\n## \n\n![](/img/coffe.png){fig-align=\"center\" width=\"500\"}\n\n[Vecteezy](https://www.vecteezy.com/free-png/funny)\n\n## References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "e995c81535c5fcc8018a3200c3bba61c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Introduction to Generalized Linear Models for Psychology\nsubtitle: Generalized Linear Models Workshop \nformat: minimal-revealjs\nexecute: \n  echo: true\ndate: last-modified\ndate-format: \"*[Last modified:] DD-MM-YYYY*\"\ntoc: true\nhtml-math-method: mathml\nfig-dpi: 300\n---\n\n\n\n# Linear Regression $y \\sim x$ {.section}\n\n## Linear Regression $y \\sim x$\n\n-   Estimate the expected (average) outcome given predictors.\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Linear Regression $y \\sim x$\n\n-   A **constant change** in a *x* leads to a constant change in the *y*.\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Normal Linear Regression $y \\sim x$\n\n-   **Variation**: Characterize how outcomes vary around that expectation.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-4-1.svg){fig-align='center'}\n:::\n:::\n\n\n## The Normal distribution\n\nThe Normal distribution has parameters $\\mu$ (mean) and $\\sigma^2$ (variance):\n\n$f(y \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( - \\frac{(y - \\mu)^2}{2\\sigma^2} \\right)$\n\n<br/>\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-5-1.svg){fig-align='center'}\n:::\n:::\n\n\n## The Normal distribution\n\nThe Normal distribution has parameters $\\mu$ (mean) and $\\sigma^2$ (variance):\n\n$f(y \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( - \\frac{(y - \\mu)^2}{2\\sigma^2} \\right)$\n\n<br/>\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-6-1.svg){fig-align='center'}\n:::\n:::\n\n\n## The Normal distribution\n\nThe Normal distribution has parameters $\\mu$ (mean) and $\\sigma^2$ (variance):\n\n$f(y \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( - \\frac{(y - \\mu)^2}{2\\sigma^2} \\right)$\n\n<br/>\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-7-1.svg){fig-align='center'}\n:::\n:::\n\n\n## $$y_i \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_i, \\sigma^2)$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-8-1.svg){fig-align='center'}\n:::\n:::\n\n\nConditional on $x$, the variable $y$ follows a Normal distribution centered at the regression line, with fixed spread ($\\sigma^2$)\n\n## $$y \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-9-1.svg){fig-align='center'}\n:::\n:::\n\n\n-   **Support**: $(-\\infty, +\\infty)$\n-   **Mean** (expected value): $\\mu$\n-   **Variance**: $\\sigma^2$\n-   **Independence:** $\\mu$ and $\\sigma^2$ are separate; changing the mean does *not* change the variance!\n\n## Why do these matter for regression?\n\nWhen building a model, we need to know:\n\n-   **Support**: what range of values is possible\n-   **Mean** (expected value): what value to predict on average\n-   **Variance**: how much variation around the mean\n-   **Mean–variance relationship**: does variance change with the mean?\n\n## Where the Normal model breaks down\n\n### Reaction Times\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-10-1.svg)\n:::\n:::\n\n\n## Where the Normal model breaks down\n\n### Exam pass/fail\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-11-1.svg)\n:::\n:::\n\n\n## Where the Normal model breaks down\n\n### Number of errors in a task\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-12-1.svg)\n:::\n:::\n\n\n## Probability distributions\n\nA probability distribution of a random variable $Y$ describes the **probabilities assigned to each possible value** $y$, given certain parameters values.\n\n<br/>\n\n::: fragment\nFor a normally distributed random varible...\n\n$$f(y \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( - \\frac{(y - \\mu)^2}{2\\sigma^2} \\right)$$\n:::\n\n## Parameters and variables\n\nAny probability function has **parameters** and **variables**:\n\n$$f(y \\mid \\boldsymbol{\\theta})$$\n\n-   $y$ is a specific observed value\n-   $\\boldsymbol{\\theta}$ is a vector of parameters that define the distribution's shape\n-   $f(\\cdot)$ is the function itself\n\n## \n\n| Distribution | Support | $E[Y]$ | $\\mathrm{Var}(Y)$ |\n|------------------|-----------------|----------------:|--------------------:|\n| **Normal**: $f(y \\text{ | } \\mu,\\sigma^2)$ | $y \\in \\mathbb{R}$ | $\\mu$ | $\\sigma^2$ |\n| **Gamma**: $f(y \\text{ | } \\alpha,\\beta)$ | $y \\in (0,\\infty)$ | $\\alpha/\\beta$ | $\\alpha/\\beta^2$ |\n| **Binomial**: $f(y \\text{ | } n,p)$ | $y \\in \\{0,1,\\dots,n\\}$ | $np$ | $np(1-p)$ |\n| **Poisson**: $f(y \\text{ | } \\lambda)$ | $y \\in \\{0,1,2,\\dots\\}$ | $\\lambda$ | $\\lambda$ |\n\n## Generalized Linear Models (GLMs)\n\n::: incremental\n-   **Right distribution**: Match the outcome's support\n\n-   **Mean-dependent variance**: Variance changes with the mean, not constant\n\n-   **Link function**: Transform the mean $\\rightarrow$ Ensures predictions respect the outcome's constraints\n:::\n\n------------------------------------------------------------------------\n\n## Example: Passing the exam\n\nDoes studying more increase the probability of passing?\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-13-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Fitting a Normal Linear Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_lm <- lm(passed ~ study_hours, data = dat_exam)\n```\n:::\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-15-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Fitting a Generalized Linear Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_glm <- glm(passed ~ study_hours, data = dat_exam, \n               family = binomial(link = \"logit\"))\n```\n:::\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-17-1.svg){fig-align='left'}\n:::\n:::\n\n\n# Generalized Linear Models (GLM) {.section}\n\n## The three ingredients of a GLM\n\n::: incremental\n1.  **Random Component**: Choose a distribution  \n\n2.  **Systematic Component**: Linear predictor\n\n    $\\eta_i = \\beta_0 + \\beta_1 x_i + …$  \n\n3.  **Link Function**: Connect mean to predictor\n\n    $g(\\mu_i) = \\eta_i$\n:::\n\n------------------------------------------------------------------------\n\n## 1. Random Component\n\nThe **random component** specifies the probability distribution of the outcome $y$:\n\n$$\ny \\sim \\text{Distribution}(\\text{parameters})\n$$\n\n \n\n::: {.fragment fragment-index=\"1\"}\nWhat **support** does your outcome have?\n\n-   Binary: $\\{0, 1\\}$ → $\\text{Binomial}(n, p)$\n-   Counts: $\\{0, 1, 2, \\ldots\\}$ → $\\text{Poisson}(\\lambda)$\n-   Any real number: $(-\\infty, \\infty)$ → $\\text{Gaussian}(\\mu, \\sigma)$\n:::\n\n## 1. Random Component\n\nChoosing a distribution specifies not only the mean, but also how the variance depends on the mean:\n\n\\\n\n$$\n\\mathrm{Var}(Y \\mid \\mathbf{X}) = V(\\mu),\n\\qquad \\mu = E(Y \\mid \\mathbf{X}).\n$$\n\n \n\n*Examples*:\n\n-   Normal: $V(\\mu)=\\text{constant}$;\n-   Binomial: $V(\\mu)=\\mu(1-\\mu)$;\n-   Poisson: $V(\\mu)=\\mu$.\n\n## 2. Systematic Component\n\nThe **systematic component** is *exactly the same* as in *normal* linear regression: we predict a linear combination of predictors.\n\n<br/>\n\n$$\n\\eta_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\cdots + \\beta_k x_{ki}\n$$\n\n<br/>\n\nwhere $k$ denotes the total number of predictors.\n\n------------------------------------------------------------------------\n\n## 3. Link Function\n\nThe **link function** $g(\\cdot)$ connects the expected value (mean) $\\mu_i$ of the distribution to the linear predictor $\\eta$:\n\n\\\n\n$$\ng(\\mu_i) = \\eta_i\n$$\n\n\\\n\n-   The linear predictor $\\eta_i$ can be any real number: $(-\\infty, +\\infty)$\n-   But $\\mu_i$ (the mean) is **constrained** by the distribution's support\n-   The link function **transforms** $\\mu$ to be unbounded\n\n## Common link functions\n\n| Distribution | Support | Link Function | Purpose |\n|-------------|-------------|-------------|---------------------------------|\n| Normal | $(-\\infty, +\\infty)$ | **Identity**: $\\text{ }$ $g(\\mu) = \\mu$ | No transformation |\n| Binomial | $[0, 1]$ | **Logit**: $g(\\mu) = \\log\\!\\left(\\frac{\\mu}{1-\\mu}\\right)$ | Probability to $\\mathbb{R}$ |\n| Gamma | $(0, \\infty)$ | **Log**: $g(\\mu) = \\log(\\mu)$ | Positive to $\\mathbb{R}$ |\n\n------------------------------------------------------------------------\n\n## Putting it together\n\nFor example, a Generalized Linear Model with the **Gaussian** family and **identity link** could be written as:\n\n<br/>\n\n$$\n\\begin{aligned}\ny_i &\\sim \\text{Gaussian}(\\mu_i, \\sigma) \\quad && \\text{Random component} \\\\ \\\\\ng(\\mu_i) &= \\eta_i \\quad && \\text{Identity link} \\\\ \\\\\n\\eta_i &= \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} \\quad && \\text{Systematic component}\n\\end{aligned}\n$$\n\n## Putting it together\n\nFor example, a Generalized Linear Model with the **Binomial** family and **logit link** could be written as:\n\n<br/>\n\n$$\n\\begin{aligned}\ny_i &\\sim \\text{Binomial}(n, p_i) && \\text{Random component} \\\\ \\\\\n\\log\\left(\\frac{p_i}{1-p_i}\\right) &= \\eta_i && \\text{Logit link} \\\\ \\\\\n\\eta_i &= \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} && \\text{Systematic component}\n\\end{aligned}\n$$\n\n# Binomial Logistic Regression {.section}\n\n## Bernoulli and Binomial distributions\n\n-   Number of items correct on a test (out of $n$)\n-   Number of people who improve after treatment (out of $n$ patients)\n-   Accuracy on a task (successes / total trials)\n\n## Bernoulli distribution\n\nA single binary trial with two outcomes (success/failure):\n\n\\\n\n$$\nP(Y = y \\mid p) = p^y(1-p)^{1-y}, \\quad y \\in \\{0, 1\\}\n$$\n\n\\\n\nwhere $p$ is the probability of success.\n\n\\\n\n**Mean:** $E(Y) = p$\n\n**Variance:** $\\text{Var}(Y) = p(1-p)$\n\n## Flip one coin (heads with probability 0.7)\n\n\n::: {.cell}\n\n```{.r .cell-code}\none = rbinom(n = 1, size = 1, prob = 0.7);one\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\nOver 10,000 flips, what’s the estimated probability of heads?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmany = rbinom(n = 100000, size = 1, prob = 0.7); head(many)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 1 1 0 1 1\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Mean: E(Y) = p \np = mean(many); p\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.70085\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Variance: Var(Y) = p(1-p)\np*(1-p)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2096593\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(many)^2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2096614\n```\n\n\n:::\n:::\n\n\n## Binomial distribution\n\nThe probability of getting exactly **k** successes in **n** independent *Bernoulli* trials:\n\n\\\n\n$$\nP(Y = k \\mid n, p) = \\binom{n}{k} p^k (1-p)^{n-k}, \\quad k \\in \\{0, 1, \\ldots, n\\}.\n$$\n\n\\\n\n**Mean:** $E(Y) = np$\n\n**Variance:** $\\text{Var}(Y) = np(1-p)$\n\n## Flip one coin 10 times\n\nHow many heads you get?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn = 10; p = 0.7\nrbinom(n = 1, size = n, prob = 0.7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7\n```\n\n\n:::\n:::\n\n\nOver 10,000 repetition ...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmany = rbinom(n = 100000, size = n, prob = p); head(many)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7 8 8 8 8 9\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn*p # Mean count: E(Y) = np \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn*p*(1-p) # Variance count: Var(Y) = np(1-p)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.1\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(many/n) # Mean proportion = p\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.700083\n```\n\n\n:::\n\n```{.r .cell-code}\np*(1-p)/n    # Var(Y/n) = p(1-p)/n (variance of proportion)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.021\n```\n\n\n:::\n:::\n\n\n## Binomial distribution: $n = 20$, $p = 0.9$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-27-1.svg)\n:::\n:::\n\n\n## Binomial distribution: $n = 20$, $p = 0.5$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-28-1.svg)\n:::\n:::\n\n\n## Mean–variance relationship\n\n$$E[Y] = np \\quad \\text{and} \\quad \\mathrm{Var}(Y) = np(1-p)$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-29-1.svg)\n:::\n:::\n\n\n## Mean–variance relationship\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-30-1.svg)\n:::\n:::\n\n\n**Variance is not constant!** We model both the mean and the variance!\n\n# Odds, Logit Link, Odds Ratios {.section}\n\n## Odds\n\nLet $\\mu = P(\\text{Pass}=1)$. The *odds* of passing compare “pass” to “fail”:\n\n\\\n\n$$\n\\text{odds} = \\frac{\\mu}{1-\\mu}.\n$$\n\n\\\n\n::: fragment\nIf $\\mu=0.80$, then $\\text{odds}=\\frac{0.80}{0.20}=4$.\n:::\n\n\\\n\n::: fragment\nSo passing is **4-to-1** relative to failing (4 expected passes for 1 fail, on average).\n:::\n\n## Logit link (log-odds)\n\nThe logit link takes probabilities $(0,1)$ to the real line:\n\n\\\n\n$$\n\\eta_i = \\log\\!\\left(\\frac{\\mu_i}{1-\\mu_i}\\right).\n$$\n\n\\\n\nIn a logistic regression we assume the log-odds are linear in predictors:\n\n\\\n\n$$\n\\eta_i = \\beta_0 + \\beta_1 x_i.\n$$\n\n## Linear on log-odds\n\nA +1 increase in $x$ changes $\\eta$ by a constant amount: $\\eta(x+1)-\\eta(x)=\\beta_1$.\n(So equal increases in $x$ correspond to equal increases in log-odds.)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-31-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n\n## Inverse link (back to probability)\n\nTo go back to probability we apply the inverse-logit:\n\n$$\n\\mu_i = \\frac{e^{\\eta_i}}{1+e^{\\eta_i}}.\n$$\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-32-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Not linear in probability\n\nEqual increases in $x$ generally **do not** correspond to equal increases in $\\mu$,\nbecause $\\mu=\\text{logit}^{-1}(\\eta)$ is nonlinear. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-33-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Odds ratios\n\nFrom the model $\\eta_i=\\beta_0+\\beta_1 x_i$, the odds are\n\n\\\n\n$$\n\\frac{\\mu_i}{1-\\mu_i}=\\exp(\\beta_0+\\beta_1 x_i).\n$$\n\\\n\n::: {.incremental}\n\n-   The odds of passing when study hours $x = 0$ is $\\exp(\\beta_0)$\n-   The odds of passing when study hours $x = 1$ is \\\n    $$\\exp(\\beta_0 + \\beta_1) = \\exp(\\beta_0) \\exp(\\beta_1)$$\n:::\n\n## Odds Ratios\n\nThen the ratio of these two odds is:\n\n\\\n\n$$\\frac{\\exp(\\beta_0)\\exp(\\beta_1)}{\\exp(\\beta_0)} = \\exp(\\beta_1)$$\n\n\\\n\nThis means that the odds of passing when increasing study hours by 1 is $\\exp(\\beta_1)$ times greater than at the baseline (i.e., when $x = 0$).\n\n##\n\nIf we increase $x$ by 1 unit, the odds ratio is\n\n\\\n\n$$\n\\text{OR}=\\frac{\\text{odds}(x+1)}{\\text{odds}(x)}=\\exp(\\beta_1).\n$$\n\n\\\n\n\nSo each +1 unit in study hours multiplies the odds of passing by $\\exp(\\beta_1)$.\n\n## Numerical example ($\\beta_1=0.8$)\n\nHere $\\exp(0.8)\\approx 2.23$, so each +1 hour multiplies the odds by ~2.23. \n\n| Baseline **p** | Baseline odds $p/(1-p)$ | New odds $=2.23\\times$ odds | New **p** | $\\Delta$ **p** |\n|---:|---:|---:|---:|---:|\n| 0.10 | 0.11 | 0.25 | 0.20 | +0.10 |\n| 0.20 | 0.25 | 0.55 | 0.36 | +0.16 |\n| 0.36 | 0.55 | 1.22 | 0.55 | +0.20 |\n| 0.55 | 1.22 | 2.73 | 0.73 | +0.18 |\n| 0.73 | 2.73 | 6.07 | 0.86 | +0.13 |\n| 0.86 | 6.07 | 13.51 | 0.93 | +0.07 |\n\n## \n\n![](/img/coffe.png){fig-align=\"center\" width=\"500\"}\n\n[Vecteezy](https://www.vecteezy.com/free-png/funny)\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "9ed684565fa90878be4802f580f9b6b4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Introduction to Generalized Linear Models for Psychology\nsubtitle: Generalized Linear Models Workshop \nformat: minimal-revealjs\nexecute: \n  echo: true\ndate: last-modified\ndate-format: \"*[Last modified:] DD-MM-YYYY*\"\ntoc: true\nhtml-math-method: mathml\nfig-dpi: 300\n---\n\n\n\n# Linear Regression $y \\sim x$ {.section}\n\n## Linear Regression $y \\sim x$\n\n-   Estimate the expected (average) outcome given predictors.\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Linear Regression $y \\sim x$\n\n-   A constant change in **x** leads to a constant change in the **expected outcome**.\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Linear Regression $y \\sim x$\n\n-   **Variation**: Characterize how outcomes vary around that expectation.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-4-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Random variable vs observed value\n\n-   Before we observe $y_i$, the outcome is uncertain: we model it as a **random variable** $Y_i$.\n\n$$\nY_i \\mid x_i \\sim \\text{Distribution}(\\text{parameters}).\n$$\n\n-   After collecting data, we record one realized (observed) value $y_i$.\n\n$$\ny_i \\text{ is the observed value (a realization) of } Y_i.\n$$\n\n<br/>\n\nFollowing @Dunn2018-ww, we write $y_i$ for the response and it is modeled as random before observation.\n\n## Probability distributions\n\nA probability distribution of a random variable $Y$ describes the **probabilities assigned to each possible value** $y$, given certain parameters values.\n\n<br/>\n\n$$f(y \\mid \\boldsymbol{\\theta})$$\n\n<br/>\n\n-   $y$ is a specific observed value\n\n-   $\\boldsymbol{\\theta}$ is a vector of parameters that define the distribution's shape\n\n-   $f(\\cdot)$ is the function itself\n\n<br/>\n\n**Note.** For continuous outcomes $f(y \\mid \\boldsymbol{\\theta})$ is a density, not a probability at a point (probabilities come from integrating the density).\n\n## The Normal distribution\n\nThe Normal distribution has parameters $\\mu$ (mean) and $\\sigma^2$ (variance):\n\n$f(y \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( - \\frac{(y - \\mu)^2}{2\\sigma^2} \\right)$\n\n<br/>\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-5-1.svg){fig-align='center'}\n:::\n:::\n\n\n## The Normal distribution\n\nThe Normal distribution has parameters $\\mu$ (mean) and $\\sigma^2$ (variance):\n\n$f(y \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( - \\frac{(y - \\mu)^2}{2\\sigma^2} \\right)$\n\n<br/>\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-6-1.svg){fig-align='center'}\n:::\n:::\n\n\n## The Normal distribution\n\nThe Normal distribution has parameters $\\mu$ (mean) and $\\sigma^2$ (variance):\n\n$f(y \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( - \\frac{(y - \\mu)^2}{2\\sigma^2} \\right)$\n\n<br/>\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-7-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Normal linear regression\n\n$$\ny_i \\mid x_i \\sim \\mathcal{N}(\\mu_i,\\sigma^2),\n\\qquad\n\\mu_i = \\beta_0 + \\beta_1 x_i.\n$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-8-1.svg){fig-align='center'}\n:::\n:::\n\n\nFor a fixed $x_i$, the model describes the distribution of possible outcomes around the mean $\\mu_i$; the data give one realized outcome $y_i$.\n\n## $$y_i \\mid x_i \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_i,\\ \\sigma^2)$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-9-1.svg){fig-align='center'}\n:::\n:::\n\n\n-   **Support**: $(-\\infty, +\\infty)$\n-   **Mean** (expected value): $\\mu$\n-   **Variance**: $\\sigma^2$\n-   **Independence:** $\\mu$ and $\\sigma^2$ are separate; changing the mean does *not* change the variance!\n\n## Why do these matter for regression?\n\nWhen building a model, we need to know:\n\n-   **Support**: what range of values is possible\n-   **Mean** (expected value): what value to predict on average\n-   **Variance**: how much variation around the mean\n-   **Mean–variance relationship**: does variance change with the mean?\n\n## Where the Normal model breaks down\n\n### Reaction Times\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-10-1.svg)\n:::\n:::\n\n\n## Where the Normal model breaks down\n\n### Exam pass/fail\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-11-1.svg)\n:::\n:::\n\n\n## Where the Normal model breaks down\n\n### Number of errors in a task\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-12-1.svg)\n:::\n:::\n\n\n\n\n## Probability distributions\n\n<br/>\n\n| Distribution | Support | mean: $E[Y]$ | variance: $\\mathrm{Var}(Y)$ |\n|------------------|------------------|:----------------:|:------------------:|\n| **Normal**: $f(y \\text{ | } \\mu,\\sigma^2)$ | $y \\in \\mathbb{R}$ | $\\mu$ | $\\sigma^2$ |\n| **Gamma**: $f(y \\text{ | } \\alpha,\\beta)$ | $y \\in (0,\\infty)$ | $\\alpha/\\beta$ | $\\alpha/\\beta^2$ |\n| **Binomial**: $f(y \\text{ | } n,p)$ | $y \\in \\{0,1,\\dots,n\\}$ | $np$ | $np(1-p)$ |\n| **Poisson**: $f(y \\text{ | } \\lambda)$ | $y \\in \\{0,1,2,\\dots\\}$ | $\\lambda$ | $\\lambda$ |\n\n## Generalized Linear Models (GLMs)\n\n::: incremental\n-   **Right distribution**: Match the outcome's support\n\n-   **Mean-dependent variance**: Variance changes with the mean, not constant\n\n-   **Link function**: Transform the mean $\\rightarrow$ Ensures predictions respect the outcome's constraints\n:::\n\n------------------------------------------------------------------------\n\n## Example: Passing the exam\n\nDoes studying more increase the probability of passing?\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-13-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Fitting a Normal Linear Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_lm <- lm(passed ~ study_hours, data = dat_exam)\n```\n:::\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-15-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Fitting a Generalized Linear Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_glm <- glm(passed ~ study_hours, data = dat_exam, \n               family = binomial(link = \"logit\"))\n```\n:::\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-17-1.svg){fig-align='left'}\n:::\n:::\n\n\n# Generalized Linear Models (GLM) {.section}\n\n## The three ingredients of a GLM\n\n::: incremental\n1.  **Random Component**: Choose a distribution\n\n2.  **Systematic Component**: Linear predictor\n\n    $\\eta_i = \\beta_0 + \\beta_1 x_i + …$\n\n3.  **Link Function**: Connect mean to predictor\n\n    $g(\\mu_i) = \\eta_i$\n:::\n\n## 1. Random component\n\nThe **random component** specifies a probability model for the response $y_i$\n(we treat $y_i$ as random in the model):\n\n<br/>\n\n$$\ny_i \\mid x_i \\sim \\text{Distribution}(\\text{parameters}).\n$$\n\n<br/>\n\n::: {.fragment fragment-index=\"1\"}\nWhat **support**?\n\n- Binary: $\\{0,1\\}$ $\\rightarrow$ $\\text{Bernoulli}(p)$ (or $\\text{Binomial}(1,p)$)\n- Counts: $\\{0,1,2,\\ldots\\}$ $\\rightarrow$ $\\text{Poisson}(\\lambda)$\n- Any real number: $(-\\infty,\\infty)$ $\\rightarrow$ $\\mathcal{N}(\\mu,\\sigma^2)$\n:::\n\n## 1. Random component (mean–variance)\n\nChoosing a distribution specifies not only the mean, but also how the variance depends on the mean:\n\n<br/>\n\n$$\n\\mathrm{Var}(y_i \\mid x_i) = \\phi\\,V(\\mu_i),\n\\qquad \\mu_i = E(y_i \\mid x_i).\n$$\n\n<br/>\n\n- Normal: $V(\\mu_i)=1$ (so $\\phi=\\sigma^2$)\n- Bernoulli: $V(\\mu_i)=\\mu_i(1-\\mu_i)$ (often $\\phi=1$)\n- Poisson: $V(\\mu_i)=\\mu_i$ (often $\\phi=1$)\n\n\n## 2. Systematic Component\n\nThe **systematic component** is *exactly the same* as in *normal* linear regression: we predict a linear combination of predictors.\n\n<br/>\n\n$$\n\\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_k x_{ik}.\n$$\n\n<br/>\n\nwhere $k$ denotes the total number of predictors.\n\n------------------------------------------------------------------------\n\n## 3. Link Function\n\nThe **link function** $g(\\cdot)$ connects the expected value (mean) $\\mu_i$ of the distribution to the linear predictor $\\eta_i$:\n\n\\\n\n$$\ng(\\mu_i) = \\eta_i\n$$\n\n\\\n\n-   The linear predictor $\\eta_i$ can be any real number: $(-\\infty, +\\infty)$\n-   But $\\mu_i$ (the mean) is **constrained** by the distribution's support\n-   The link function **transforms** $\\mu_i$ to be unbounded\n\n## Common link functions\n\n## Common link functions\n\n| Distribution | Support of $y$ | Link | Purpose |\n|---|---|-----|---|\n| Normal | $(-\\infty,\\infty)$ | Identity: $g(\\mu)=\\mu$ | No transformation |\n| Binomial | $\\{0,1,\\ldots,n_i\\}$ | Logit on $p_i$: $g(p_i)=\\log\\!\\left(\\frac{p_i}{1-p_i}\\right)$, where $p_i=\\mu_i/n_i$ | Probability $\\to \\mathbb{R}$ |\n| Poisson | $\\{0,1,2,\\ldots\\}$ | Log: $g(\\mu)=\\log(\\mu)$ | Positive $\\to \\mathbb{R}$ |\n| Gamma | $(0,\\infty)$ | Log: $g(\\mu)=\\log(\\mu)$ | Positive $\\to \\mathbb{R}$ |\n\n\n\n\n## Normal + identity\n\nFor example, a Generalized Linear Model with the **Normal** family and **identity link** can be written as:\n\n$$\n\\begin{aligned}\ny_i &\\sim \\mathcal{N}(\\mu_i, \\sigma^2) && \\text{Random component} \\\\\n\\mu_i &= E(y_i \\mid x_i) && \\text{Mean} \\\\\n\\mu_i &= \\eta_i && \\text{Link (identity)} \\\\\n\\eta_i &= \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik} && \\text{Systematic component}\n\\end{aligned}\n$$\n\n## Binomial + logit\n\nFor example, a Generalized Linear Model with the **Binomial** family and **logit link** can be written as:\n\n$$\n\\begin{aligned}\ny_i \\mid x_i &\\sim \\text{Binomial}(n_i, p_i) && \\text{Random component} \\\\\n\\mu_i &= E(y_i \\mid x_i)=n_i p_i && \\text{Mean (expected count)} \\\\\n\\text{logit}(p_i) &= \\log\\!\\left(\\frac{p_i}{1-p_i}\\right)=\\eta_i && \\text{Link (logit)} \\\\\n\\eta_i &= \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik} && \\text{Systematic component}\n\\end{aligned}\n$$\n\n\n# Binomial Logistic Regression {.section}\n\n## Binary outcomes and counts\n\nWe model outcomes that are either:\n\n-   **Binary** (pass/fail), one trial per person; or\n-   **Counts of successes** out of $n_i$ trials (e.g., items correct out of $n_i$).\n\nThese are naturally described by the Bernoulli and Binomial distributions.\n\n## Bernoulli (one student)\n\nLet $y_i \\in \\{0,1\\}$ indicate whether student $i$ passes the exam:\n\n<br/>\n\n$$\ny_i \\sim \\text{Bernoulli}(p_i),\n\\qquad p_i = P(y_i=1 \\mid x_i).\n$$\n<br/>\n\nHere $p_i$ is the conditional mean: \n$$\n\\mu_i = E(y_i \\mid x_i) = p_i.\n$$\n\n<br/>\n\nFor a Bernoulli random variable, \n$$\n\\mathrm{Var}(y_i \\mid x_i)=p_i(1-p_i).\n$$\n\n## One student takes the exam\n\n\n::: {.cell}\n\n```{.r .cell-code}\none = rbinom(n = 1, size = 1, prob = 0.7);one\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\nOver 10,000 students, what’s the estimated probability of passing that exam?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmany = rbinom(n = 100000, size = 1, prob = 0.7); head(many)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 1 1 0 1 1\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Mean: E(y) = p \np = mean(many); p\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.70085\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Variance: Var(y) = p(1-p)\np*(1-p)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2096593\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(many)^2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2096614\n```\n\n\n:::\n:::\n\n\n## Binomial (a class of students)\n\nNow let $y_i$ be the number of students who pass in class $i$ out of $n_i$ students who take the same exam:\n\n<br/>\n\n$$\ny_i \\sim \\text{Binomial}(n_i, p_i),\n\\qquad p_i = P(\\text{success} \\mid x_i).\n$$\n<br/>\n\nThen,\n$$\nE(y_i \\mid x_i)=n_i p_i,\n\\qquad\n\\mathrm{Var}(y_i \\mid x_i)=n_i p_i(1-p_i).\n$$\n<br/>\n(When $n_i=1$, the Binomial reduces to the Bernoulli case.)\n\n## Ten students take the exam\n\nHow many pass?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn = 10; p = 0.7\nrbinom(n = 1, size = n, prob = 0.7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7\n```\n\n\n:::\n:::\n\n\nOver 10,000 repetition ...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmany = rbinom(n = 100000, size = n, prob = p); head(many)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7 8 8 8 8 9\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn*p # Mean count: E(Y) = np \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn*p*(1-p) # Variance count: Var(y) = np(1-p)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.1\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(many/n) # Mean proportion = p\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.700083\n```\n\n\n:::\n\n```{.r .cell-code}\np*(1-p)/n    # Var(y/n) = p(1-p)/n (variance of proportion)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.021\n```\n\n\n:::\n:::\n\n\n## Binomial distribution: $n = 20$, $p = 0.9$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-27-1.svg)\n:::\n:::\n\n\n## Binomial distribution: $n = 20$, $p = 0.5$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-28-1.svg)\n:::\n:::\n\n\n## Mean–variance relationship\n\n$$E[y] = np \\quad \\text{and} \\quad \\mathrm{Var}(y) = np(1-p)$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-29-1.svg)\n:::\n:::\n\n\n## Mean–variance relationship\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-30-1.svg)\n:::\n:::\n\n\n**Variance is not constant!** We model both the mean and the variance!\n\n# Odds, Logit Link, Odds Ratios {.section}\n\n## Odds\n\nBecause probabilities must stay between 0 and 1, we work with *odds*. Let $p_i = P(\\text{Pass}=1)$, then the *odds* of passing compare “pass” to “fail”:\n\n\n$$\n\\text{odds}_i = \\frac{p_i}{1-p_i}.\n$$\n\n\\\n\n::: fragment\nIf $p_i=0.80$, then $\\text{odds}=\\frac{0.80}{0.20}=4$.\n:::\n\n\\\n\n::: fragment\nSo passing is **4-to-1** relative to failing (4 expected passes for 1 fail, on average).\n:::\n\n## Logit link (log-odds)\n\nThe *logit* is the log-odds:\n\n$$\n\\text{logit}(p_i)=\\log\\!\\left(\\frac{p_i}{1-p_i}\\right)=\\eta_i.\n$$\n\nThis maps $p_i \\in (0,1)$ to any real number, which makes it easier to model with a linear predictor.\n\n$$\n\\eta_i = \\beta_0 + \\beta_1 x_{i}.\n$$\n\n## Linear on log-odds\n\nA +1 increase in $x$ changes $\\eta$ by a constant amount: $\\eta(x+1)-\\eta(x)=\\beta_1$. (So equal increases in $x$ correspond to equal increases in log-odds.)\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-31-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Inverse link (back to probability)\n\nTo go back to probability we apply the inverse-logit:\n\n$$\n\\mu_i = \\frac{e^{\\eta_i}}{1+e^{\\eta_i}}.\n$$\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-32-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Not linear in probability\n\nEqual increases in $x$ generally **do not** correspond to equal increases in $\\mu$, because $\\mu=\\text{logit}^{-1}(\\eta)$ is nonlinear.\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](intro-glm_files/figure-revealjs/unnamed-chunk-33-1.svg){fig-align='left'}\n:::\n:::\n\n\n## Odds ratios\n\nWith $\\mu_i = P(\\text{Pass}_i = 1 \\mid x_i)$ and $x_i$ hours studied, the model assumes:\n\n$$\n\\begin{aligned}\n\\log\\left(\\frac{\\mu_i}{1-\\mu_i}\\right) &= \\eta_i \\\\\n\\eta_i &= \\beta_0 + \\beta_1 x_i\n\\end{aligned}\n$$\n\nThen the odds:\n\n\\\n\n$$\n\\frac{\\mu_i}{1-\\mu_i}=\\exp(\\beta_0+\\beta_1 x_i).\n$$\\\n\n## Odds ratios\n\nThen the odds:\n\\\n$$\n\\frac{\\mu_i}{1-\\mu_i}=\\exp(\\beta_0+\\beta_1 x_i).\n$$\n\n\\\n\n\nThe odds of passing when study hours $x = 0$ are \n\n\\\n\n$$\n\\frac{\\mu}{1-\\mu} = \\exp(\\beta_0).\n$$\n\nThe odds of passing when study hours $x = 1$ are \n\n\\\n\n$$\n\\exp(\\beta_0 + \\beta_1) = \\exp(\\beta_0)\\exp(\\beta_1).\n$$\n\n\n## Odds Ratios\n\nThen the ratio of these two odds is:\n\n\\\n\n$$\\frac{\\exp(\\beta_0)\\exp(\\beta_1)}{\\exp(\\beta_0)} = \\exp(\\beta_1)$$\n\n\\\n\nThis means that the odds of passing when increasing study hours by 1 is $\\exp(\\beta_1)$ times greater than at the baseline (i.e., when $x = 0$).\n\n## \n\nIf we increase $x$ by 1 unit, the odds ratio is\n\n\\\n\n$$\n\\text{OR}=\\frac{\\text{odds}(x+1)}{\\text{odds}(x)}=\\exp(\\beta_1).\n$$\n\n\\\n\nSo each +1 unit in study hours multiplies the odds of passing by $\\exp(\\beta_1)$.\n\n## Numerical example ($\\beta_1=0.8$)\n\nHere $\\exp(0.8)\\approx 2.23$, so each +1 hour multiplies the odds by \\~2.23.\n\n| Baseline **p** | Baseline odds $p/(1-p)$ | New odds $=2.23\\times$ odds | New **p** | $\\Delta$ **p** |\n|--------------:|--------------:|--------------:|--------------:|--------------:|\n| 0.10 | 0.11 | 0.25 | 0.20 | +0.10 |\n| 0.20 | 0.25 | 0.55 | 0.36 | +0.16 |\n| 0.36 | 0.55 | 1.22 | 0.55 | +0.20 |\n| 0.55 | 1.22 | 2.73 | 0.73 | +0.18 |\n| 0.73 | 2.73 | 6.07 | 0.86 | +0.13 |\n| 0.86 | 6.07 | 13.51 | 0.93 | +0.07 |\n\n## \n\n![](/img/coffe.png){fig-align=\"center\" width=\"500\"}\n\n[Vecteezy](https://www.vecteezy.com/free-png/funny)\n\n## References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
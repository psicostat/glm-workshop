---
title: Introduction to Generalized Linear Models for Psychology
subtitle: Generalized Linear Models Workshop 
format: 
  minimal-revealjs:
    slide-number: true
    html-math-method: mathjax
execute: 
  echo: true
date: last-modified
date-format: "*[Last modified:] DD-MM-YYYY*"
toc: true
#html-math-method: mathml
fig-dpi: 300
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| include: false
#| message: false
#| warning: false

set.seed(15051995)

pkg <- c("ggplot2","patchwork","distributional","ggdist","tidyverse")
sapply(pkg, require, character.only = T)

gen_norm <- function(mu, sd){
  x = rnorm(1e6,mu,sd)
  return(data.frame(x=x))
}

my_cols <- c("deeppink4", "deepskyblue4", "#F8A31B", "forestgreen", "#000000")

theme_clean <- function(base_size = 14) {
  list(
    theme_classic(base_size = base_size),
    theme(
      axis.line  = element_line(colour = "black", linewidth = 1),
      axis.ticks = element_line(colour = "black", linewidth = 1),
      axis.ticks.length = grid::unit(10, "pt"),
      axis.text = element_text(colour = "black"),
      axis.text.y = element_text(angle = 90, vjust = 0.5, hjust = 0.5),
      axis.text.x = element_text(vjust = 0.5, hjust = 0.5),
      legend.title = element_text(size = base_size),
      legend.text  = element_text(size = base_size),
      legend.key.size = grid::unit(1.2, "lines")
    ),
    guides(
      x = guide_axis(cap = "both"),
      y = guide_axis(cap = "both")
    )
  )
}


```

# Linear Regression {.section}

## Linear Regression

Estimate the expected (average) outcome given predictors.

```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 12
#| fig-align: left

x  <- seq(0, 1, 0.2)
lp <- 0.1 + 0.2 * x

dd <- data.frame(
  x    = x,
  mean = lp,
  sd   = 0.02
)

dd$dist <- dist_normal(dd$mean, dd$sd)

dd_points <- data.frame(
  x = runif(100, 0, 1) 
)

dd_points$y <- rnorm(100, 0.1 + 0.2 * dd_points$x, 0.05)

p = ggplot(dd, aes(x = x, y = mean)) +
  geom_line(size = 4,color = my_cols[2]) +
  xlab("x") +
  ylab("y") +
  scale_x_continuous(limits = c(0,1), breaks = seq(0,1,0.2))+
  scale_y_continuous(limits = c(0,.4), breaks = seq(0,.4,0.1))+
  geom_point(data = dd_points, aes(x = x, y = y), size = 3, alpha = 0.5)+
  theme_clean(base_size = 24)

p

```

> The [expected value](https://en.wikipedia.org/wiki/Expected_value) of
> a random variable with a finite number of outcomes is a weighted
> average of all possible outcomes.

## Linear Regression

A constant change in **x** leads to a constant change in the **expected outcome**.

```{r }
#| echo: false
#| fig-height: 7
#| fig-width: 12
#| fig-align: left

# Build “rise over run” step segments between successive x grid points
dd_step <- dd[-nrow(dd), ]
dd_step$xend   <- dd$x[-1]
dd_step$y_next <- dd$mean[-1]
dd_step$dy     <- dd_step$y_next - dd_step$mean

p +  geom_segment(data = dd_step,
               aes(x = x, y = mean, xend = xend, yend = mean),
               color = my_cols[5], linewidth = 2) +
  geom_segment(data = dd_step,
               aes(x = xend, y = mean, xend = xend, yend = y_next),
               color = my_cols[5], linewidth = 2) +
  geom_text(data = dd_step,
            aes(x = xend, y = (mean-.02), label = sprintf("+%.2f", dy)),
            color = "red3", size = 10, hjust = 0.6) 

```

## Normal linear regression: `lm(y~x)`

For a fixed $x_i$, the model describes the **distribution of possible outcomes** around the expected value.

```{r}
#| echo: false
#| fig-height: 7
#| fig-width: 14
#| fig-align: left

x  <- seq(0, 1, 0.2)
lp <- 0.1 + 0.2 * x

dd <- data.frame(
  x    = x,
  mean = lp,
  sd   = 0.02
)

dd$dist <- dist_normal(dd$mean, dd$sd)

dd_points <- data.frame(
  x = runif(100, 0, 1) 
)

dd_points$y <- rnorm(100, 0.1 + 0.2 * dd_points$x, 0.05)

pn = ggplot(dd, aes(x = x, y = mean)) +
  stat_slab(aes(ydist = dist), alpha = 0.5) +
  geom_line(lwd = 2) +
  geom_point(aes(y = mean), size = 6, col = my_cols[2]) +
  xlab("x") +
  ylab("y") +
  ylim(0,0.4)+
  geom_point(data = dd_points, aes(x = x, y = y), size = 3, alpha = 0.5)+
  theme_clean(base_size = 24)+
  guides(
    x = guide_axis(cap = "both"),
    y = guide_axis(cap = "both")
  )

pn

```


# Probability Distributions {.section}

## Why probability distributions?

Our models do not fit our data exactly, we predict **the average** value of the outcome given the predictors.


::: fragment
- Probability distributions help us **characterize** the **variation** that remains ***after*** predicting the average.

- Quantify **uncertainty in the estimated parameters** of the model!
:::


## Probability Distributions

A **probability distribution** describes all possible values a random variable $Y$ can take and their associated probabilities, given certain parameters.

\

$$f(y \mid \boldsymbol{\theta})$$

\

Where:

- $y$ is a specific value 
- $\boldsymbol{\theta}$ is a vector of parameters (e.g., $\mu$, $\sigma^2$)
- $f(\cdot)$ is the probability function



## Discrete vs. Continuous Variables

:::: columns
::: column
**Continuous** random variables (time, weight, temperature, etc.)

- **Probability Density Function (PDF)**
- $f(y \mid \boldsymbol{\theta})$
- The **probability density** at $y$ (not a probability!)
- $P(Y = y) = 0$ for any specific value
:::
::: column
```{r, echo=FALSE}
#| fig-height: 11
#| fig-width: 9
# Parameters
mu <- 0; sigma <- 1
shape <- 2; rate <- 1
lambda <- 3
n <- 10; p <- 0.4   # <-- binomial params

# Normal PDF via stat_function()
p1 <- ggplot(data.frame(x = c(mu - 4*sigma, mu + 4*sigma)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma),
                linewidth = 1.2) +
  labs(title = "Normal (PDF)", x = "y", y = "Density") +
  theme_clean(base_size = 24)

# Gamma PDF via stat_function()
x_max <- qgamma(0.995, shape = shape, rate = rate)
p2 <- ggplot(data.frame(x = c(0, x_max)), aes(x)) +
  stat_function(fun = dgamma, 
                args = list(shape = shape, rate = rate),
                linewidth = 1.2) +
  labs(title = "Gamma (PDF)", x = "y", y = "Density") +
  theme_clean(base_size = 28)

# Poisson PMF (discrete -> compute points/bars explicitly)
k_max <- qpois(0.999, lambda = lambda)
k <- 0:k_max
df_pois <- data.frame(k = k, prob = dpois(k, lambda = lambda))

p3 <- ggplot(df_pois, aes(k, prob)) +
  geom_col(width = 0.7, fill = my_cols[1]) +
  labs(title = "Poisson (PMF)", x = "y", y = "Probability") +
  scale_x_continuous(breaks = k)+
  theme_clean(base_size = 28)

# Binomial
k2 <- 0:n
df_binom <- data.frame(k = k2, prob = dbinom(k2, size = n, prob = p))

p4 <- ggplot(df_binom, aes(k, prob)) +
  geom_col(width = 0.7, fill = my_cols[2]) +
  scale_x_continuous(breaks = k2) +
  labs(title = "Binomial (PMF)", x = "y (out of n)", y = "Probability") +
  theme_clean(base_size = 28)
(p1 / p2)

```

:::
:::::

## Discrete vs. Continuous Variables

:::: columns
::: column
**Discrete** random variables (counts, binary outcomes, etc.)

- **Probability Mass Function (PMF)**
- $P(Y = y \mid \boldsymbol{\theta})$ 
- The **probability** of observing exactly $y$
- $P(Y = 3) = 0.25$ means 25% chance of getting 3

:::
::: column
```{r, echo=FALSE}
#| fig-height: 11
#| fig-width: 9
(p3 / p4)

```
:::
:::::

## Distributions and linear regression

Predict the **average value of $Y$** and quantify the remaining variation:

::: incremental
- **Expected value (mean)**: $E[Y]=\mu$.  
  The long-run average of $Y$ under its distribution.

- **Conditional expected value**: $E[Y\mid X]=\mu(X)$.  
  The long-run average outcome **for a fixed predictor value** $X=x$.

- **Variance**: $\mathrm{Var}(Y)$ (or, in regression, $\mathrm{Var}(Y\mid X)$).  
  How much outcomes vary around the mean (overall or at a given $x$).

:::

::: fragment
In normal linear and generalized linear models we generally include **predictors** on the **mean** of the distribution.
:::


## Why is it important?

The **definitions** of expected value and variance are universal,
but how we compute them (and what they equal) depends on the distribution’s parameters. 

::: fragment
For a **Normal** distribution $Y \sim \mathcal{N}(\mu,\sigma^2)$,
the parameters are exactly the mean and variance.

::: incremental
- **Expected value (mean):** $E[Y]=\mu$. 
- **Variance:** $\mathrm{Var}(Y)=\sigma^2$. 
:::
:::

## 

| Distribution | Support | mean: $E[Y]$ | variance: $\mathrm{Var}(Y)$ |
|------------------|------------------|:----------------:|:-----------------:|
| **Normal**: $f(y \text{ | } \mu,\sigma^2)$ | $y \in \mathbb{R}$ | $\mu$ | $\sigma^2$ |
| **Gamma**: $f(y \text{ | } \alpha,\lambda)$ | $y \in (0,\infty)$ | $\alpha/\lambda$ | $\alpha/\lambda^2$ |
| **Binomial**: $f(y \text{ | } n,p)$ | $y \in \{0,1,\dots,n\}$ | $np$ | $np(1-p)$ |
| **Poisson**: $f(y \text{ | } \lambda)$ | $y \in \{0,1,2,\dots\}$ | $\lambda$ | $\lambda$ |


# The Normal distribution {.section}

## Normal distribution

A Normal distribution has parameters $\mu$ (mean) and $\sigma^2$
(variance):

$$f(y \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( - \frac{(y - \mu)^2}{2\sigma^2} \right)$$

<br/>


```{r}
#| echo: false
#| fig-width: 14
#| fig-align: center

dat = data.frame(x = rbind(gen_norm(0,1),gen_norm(0,2),gen_norm(2,1)),
                 parameters = rep(c("Normal(0, 1)",
                              "Normal(0, 2)",
                              "Normal(2, 1)"), each = nrow(gen_norm(0,1))))

ggplot(subset(dat,parameters == "Normal(0, 1)"), aes(x=x, fill = parameters))+
  geom_density(alpha = 0.8)+
  scale_fill_manual(values = my_cols)+
  xlab("")+xlim(-8,8)+
  theme_clean(base_size = 24)+
  theme(legend.position = "right",
        legend.title=element_blank())
```


## Normal distribution

A Normal distribution has parameters $\mu$ (mean) and $\sigma^2$
(variance):

$$f(y \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( - \frac{(y - \mu)^2}{2\sigma^2} \right)$$

<br/>
```{r}
#| echo: false
#| fig-width: 14
#| fig-align: center
ggplot(subset(dat, parameters != "Normal(2, 1)"), aes(x = x, fill = parameters)) +
  geom_density(alpha = 0.8) +
  scale_fill_manual(values = my_cols) +
  xlab("") + xlim(-8, 8) +
  theme_clean(base_size = 24) +
  theme(legend.position = "right", legend.title = element_blank())
```
  


## Normal distribution

A Normal distribution has parameters $\mu$ (mean) and $\sigma^2$
(variance):

$$f(y \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( - \frac{(y - \mu)^2}{2\sigma^2} \right)$$

<br/>

```{r}
#| echo: false
#| fig-width: 14
#| fig-align: center

ggplot(dat, aes(x=x, fill = parameters))+
  geom_density(alpha = 0.8)+
  scale_fill_manual(values = my_cols)+
  xlab("")+xlim(-8,8)+
   ylim(0,0.4)+
  theme_clean(base_size = 24)+
  theme(legend.position = "right",
        legend.title=element_blank())
```



## Normal linear regression

:::: columns
::: column 
$\mu_i = \beta_0 + \beta_1 x_i,$ 

$y_i \sim \mathcal{N}(\mu_i,\sigma^2)$

```{r, echo=FALSE}
#| fig-align: left
#| fig-width: 12
#| fig-height: 7

pn

```

:::

::: column 
::: fragment
$\quad y_i = \beta_0 + \beta_1 x_i + \varepsilon_i,$

$\quad \varepsilon_i \sim \mathcal{N}(0,\sigma^2)$


```{r, echo=FALSE}
#| fig-align: left
#| fig-width: 12
#| fig-height: 7


# 1) Compute fitted values and errors for the simulated points
dd_points <- dd_points %>%
  mutate(
    y_hat = 0.1 + 0.2 * x,
    eps   = y - y_hat
  )

# 2) Build the "error distribution" object over x (mean 0, sd = error sd)
# Choose sd = 0.05 to match how you simulated y (rnorm(..., sd = 0.05))
dd_err <- data.frame(
  x    = x,
  mean = 0,
  sd   = 0.05
)
dd_err$dist <- dist_normal(dd_err$mean, dd_err$sd)

# 3) Residual-style plot with the same grammar as pn
pe <- ggplot(dd_err, aes(x = x, y = mean)) +
  stat_slab(aes(ydist = dist), alpha = 0.5) +
  geom_line(lwd = 2) +
  geom_point(aes(y = mean), size = 6, col = my_cols[2]) +
  geom_point(data = dd_points, aes(x = x, y = eps), size = 3, alpha = 0.5) +
  xlab("x") +
  ylab(expression(epsilon == y - hat(y))) +
  ylim(-0.2, 0.2) +
  theme_clean(base_size = 24) +
  guides(
    x = guide_axis(cap = "both"),
    y = guide_axis(cap = "both")
  )

pe


```
:::
:::

:::::

::: fragment
Two equivalent ways of expressing the same modeling assumption.
:::


## Normal distribution

-   **Support**: $-\infty, +\infty$
-   mean = mode = median
-   **Mean** (expected value): $\mu$
-   **Variance**: $\sigma^2$
-   **Independence:** changing the mean does *not* change the variance!

## Why does it matter?

Real-world measurements are not automatically Normal, **Normality** is a **modeling choice**.

-   **Support**: what range of values is possible
-   **Mean** (expected value): what value to predict on average
-   **Variance**: how much variation around the mean
-   **Mean–variance relationship**: does variance change with the mean?




## Reaction Times

```{r}
#| echo: false

set.seed(42)
dat_rt <- data.frame(
    rt = c(rgamma(1e5, 7, scale = 0.05) * 1000,
           rgamma(1e5, 14, scale = 0.05) * 1000),
    slow = rep(c("fast","slow"), each = 1e5)
) 

dat_rt |> 
    ggplot(aes(x = rt, fill = slow)) +
    geom_density( color = "black", alpha = 0.8, show.legend = FALSE) +
    labs(x = "Reaction Time (ms)", y = "Count", 
         title = " ") +
    theme_clean(base_size = 24)+
  scale_fill_manual(values = my_cols[4:5])+
  scale_x_continuous(limits = c(0,1800))+
  ylab("density")

```

## Exam pass/fail


```{r}
#| echo: false

dat_exam <- data.frame(y = c(70, 30), x = c("Passed", "Failed"))

dat_exam |> 
    ggplot(aes(x = reorder(x, -y), y = y, fill = x)) +
    geom_col(color = "black",  width = 0.5, show.legend = FALSE) +
    labs(y = "Count (out of 100)", x = "", 
         title = " ") +
    ylim(c(0, 100)) +
    theme_clean(base_size = 24)+
  scale_fill_manual(values = my_cols)
```


## Number of errors in a task

```{r}
#| echo: false

set.seed(42)
dat_count <- data.frame(x = rpois(1e4, 12))

dat_count |> 
    ggplot(aes(x = x)) +
    geom_bar(fill = my_cols[3], color = "black") +
    scale_x_continuous(breaks = seq(0, 30, 3)) +
    labs(x = "Number of Errors", y = "Count", 
         title = " ") +
    theme_clean(base_size = 24)+
  guides(
    x = guide_axis(cap = "both"),
    y = guide_axis(cap = "both")
  )

```



## Example: Passing the exam

Probability of passing the exam as a function of how many hours students
study.

::::: columns
::: {.column width="30%"}
```{r, echo=FALSE}
set.seed(123)
n <- 100
x <- round(runif(n, 0, 100))
b0 <- -3
b1 <- 0.1
y <- rbinom(length(x), 1, plogis(b0 + b1*x))

dat_exam <- data.frame(id = 1:n, studyh = x, passed = y)
dat_exam
```
:::

::: {.column width="70%"}
::: fragment
```{r, eval=FALSE}
# number of students that have passed the exam
sum(dat_exam$passed) 
#> [1] 39
# proportion of students that have passed the exam
mean(dat_exam$passed) 
#> [1] 0.39

# study hours and passing the exam
tapply(dat_exam$studyh, dat_exam$passed, mean)
#>        0        1 
#> 37.52459 69.05128 
#> 
table(dat_exam$passed, cut(dat_exam$studyh, breaks = 4))
#>    (1.9,26.2] (26.2,50.5] (50.5,74.8] (74.8,99.1]
#>  0         23          22          10           6
#>  1          2           6          10          21
#>  
tapply(dat_exam$passed, cut(dat_exam$studyh, breaks = 4), mean)
#>   (0,24.8]   (24.8,49.5]   (49.5,74.2]   (74.2,99.1] 
#>   0.0800000     0.2142857     0.5000000     0.7777778
```
::: 
:::
:::::

## Visualize data

```{r}
#| echo: false
#| cache: true
#| fig-width: 16
#| fig-height: 8
#| fig-align: left

exam_plot <- dat_exam |>
  ggplot(aes(x = studyh, y = passed)) +
  geom_hline(yintercept = c(0, 1), linetype = "dashed", size = 3,
             col = "red", alpha = 0.5) +
  geom_point(size = 5, alpha = 0.5, 
             position = position_jitter(height = 0.03, width = 1)) +
  labs(x = "hours", y = "Passed (0 = No, 1 = Yes)",
       title = "") +
  theme_clean(base_size = 24) +
  scale_y_continuous(limits = c(-0.2, 1.2), breaks = c(0,0.2,0.4,0.6,0.8,1))


exam_plot
```

## Fitting a Normal Linear Model

```{r}
#| echo: false
#| fig-width: 16
#| fig-height: 8
#| fig-align: left
exam_plot +
    geom_smooth(method = "lm", se = FALSE, color = my_cols[2], linewidth = 2.5) +
    ggtitle(" ")

```

::: fragment
How does this look to you?
::: 

## Fitting a Generalized Linear Model

The model should consider both the **support** of the $y$ variable and the **non-linear pattern**!

```{r}
#| echo: false
#| fig-width: 16
#| fig-height: 8
#| fig-align: left
exam_plot +
    stat_smooth(method = "glm", se = FALSE, method.args = list(family = binomial),
                color = my_cols[2], linewidth = 2.5) +
    ggtitle(" ")
```

## Example: Reaction times

Reaction times on two-choice questions as a function of hours studied.

::::: columns
::: {.column width="35%"}
```{r, echo=FALSE}
set.seed(123)
n <- 100
x <- round(runif(n, 0, 100))

# Data-generating process (Gamma): mean RT changes with x
# Use log link so mean is always positive: mu = exp(a0 + a1*x)
a0 <- log(80)   # baseline mean RT in seconds
a1 <- -0.03    # effect of x on log-mean RT

mu <- exp(a0 + a1*x)

shape <- 5       # Gamma shape (k); larger => less skew
scale <- mu/shape

rt <- rgamma(n, shape = shape, scale = scale)

dat_rt <- data.frame(id = 1:n, studyh = x, rt = rt)
dat_rt
```
:::

::: {.column width="65%"}
::: fragment
```{r, eval=FALSE}
range(dat_rt$rt)
#>  1.341846 119.428719

# mean RT as a function of studyh
tapply(dat_rt$rt, cut(dat_rt$studyh, breaks = 4), mean)
#>  (0,24.8]   (24.8,49.5]   (49.5,74.2]   (74.2,99.1] 
#>      56.10084      26.73179      11.39825       6.46455 

```
:::
:::
:::::

## Visualize data

```{r}
#| echo: false
#| cache: true
#| fig-width: 16
#| fig-height: 8
#| fig-align: left

rt_plot <- dat_rt |>
  ggplot(aes(x = studyh, y = rt)) +
  geom_point(size = 5, alpha = 0.6) +
  labs(x = "hours", y = "rt", 
       title = "") +
  theme_clean(base_size = 24)+
  scale_y_continuous(limits = c(-10,160))

rt_plot

```

## Fitting a Normal Linear Model

```{r}
#| echo: false
#| fig-width: 16
#| fig-height: 8
#| fig-align: left
rt_plot +
    geom_smooth(method = "lm", se = FALSE, color = my_cols[2], linewidth = 2.5) +
    ggtitle(" ")+
  geom_hline(yintercept = 0)

```

::: fragment
How does this look to you?
::: 

## Fitting a Generalized Linear Model

The model should consider both the **support** of the $y$ variable and the **non-linear pattern**!

```{r}
#| echo: false
#| fig-width: 16
#| fig-height: 8
#| fig-align: left
rt_plot +
    stat_smooth(method = "glm", se = FALSE, method.args = list(family = Gamma),
                color = my_cols[2], linewidth = 2.5) +
    ggtitle(" ")+
  geom_hline(yintercept = 0)
```

# Generalized Linear Models (GLM) {.section}

## General ideas

::: incremental
-   using distributions beyond the Normal
-   modeling non linear functions on the response scale
-   taking into account mean-variance relationships
:::

## The three ingredients of a GLM

::: incremental
1.  **Random Component**: Choose a distribution

2.  **Systematic Component**: Linear predictor
    $\eta_i = \beta_0 + \beta_1 x_i$

3.  **Link Function**: Transform the mean $g(\mu_i) = \eta_i$
:::

## Random component

The **random component** specifies a probability model for $y_i$:

<br/>

$$
y_i \mid x_i \sim \text{Distribution}(\text{parameters}).
$$

<br/>

::: {.fragment fragment-index="1"}
What **support**?

-   Binary: $\{0,1\}$ $\rightarrow$ $\text{Bernoulli}(p)$ (or
    $\text{Binomial}(1,p)$)
-   Counts: $\{0,1,2,\ldots\}$ $\rightarrow$ $\text{Poisson}(\lambda)$
-   Any real number: $(-\infty,\infty)$ $\rightarrow$
    $\text{Normal}(\mu,\sigma^2)$
:::

## Random component 

Choosing a distribution specifies not only the mean, but also how the variance depends on the mean (**variance function** $V(\mu)$):

$$
\mu = E(Y \mid X), \qquad \mathrm{Var}(Y \mid X) = \phi \, V(\mu),
$$

\

Once you model $\mu(x)$, the variance is also determined **at each x**.

Examples of **variance function** $V(\mu)$:

- $V(\mu) = \mu$ for Poisson, 
- $V(\mu) = \mu(1-\mu)$ for Bernoulli


## Systematic Component

The **systematic component** is *exactly the same* as in *normal* linear
regression: 

<br/>

$$
\eta_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_k x_{ik}.
$$ <br/>

Basically it describes how the expected value (i.e., the mean) of the chosen distribution (the random component) varies
according to the predictors.

## Link Function

The **link function** $g(\cdot)$ connects the expected value (mean)
$\mu_i$ of the distribution to the linear predictor $\eta_i$:


$$
g(\mu_i) = \eta_i
$$

Inverse link:

$$
\mu_i = g^{-1}(\eta_i)
$$

-   The linear predictor $\eta_i$ can be any real number:
    $(-\infty, +\infty)$
-   But $\mu_i$ (the mean) is **constrained** by the distribution's
    support
-   The link function **transforms** $\mu_i$ to be unbounded

## Common link functions

| Distribution | Support of $y$ | Link | Purpose |
|----------------|----------------|------------------------|----------------|
| Normal | $(-\infty,\infty)$ | Identity: $g(\mu)=\mu$ | No transformation |
| Binomial | $\{0,1,\ldots,n\}$ | Logit on $p$: $g(p_i)=\log\!\left(\frac{p}{1-p}\right)$, where $p=\mu/n$ | Probability $\to \mathbb{R}$ |
| Poisson | $\{0,1,2,\ldots\}$ | Log: $g(\mu)=\log(\mu)$ | Positive $\to \mathbb{R}$ |
| Gamma | $(0,\infty)$ | Log: $g(\mu)=\log(\mu)$ | Positive $\to \mathbb{R}$ |

## Normal + identity

For example, a Generalized Linear Model with the **Normal** family and
**identity link** can be written as:

<br/>

$$
\begin{aligned}
y_i &\sim \mathcal{N}(\mu_i, \sigma^2) && \text{Random component} \\
\mu_i &= \eta_i && \text{Link (identity)} \\
\eta_i &= \beta_0 + \beta_1 x_{i1} + \cdots + \beta_k x_{ik} && \text{Systematic component}
\end{aligned}
$$

## Bernoulli + logit

For example, a Generalized Linear Model with the **Bernoulli** family
and **logit link** can be written as:

<br/>

$$
\begin{aligned}
y_i &\sim \text{Bernoulli}(p_i) && \text{Random component} \\
p_i &= \frac{\exp(\eta_i)}{1+\exp(\eta_i)} && \text{Inverse link (logit)} \\
\eta_i &= \beta_0 + \beta_1 x_{i1} + \cdots + \beta_k x_{ik} && \text{Systematic component}
\end{aligned}
$$

## Bernoulli + logit

For example, a Generalized Linear Model with the **Bernoulli** family
and **logit link** can be written as:

<br/>

$$
\begin{aligned}
y_i &\sim \text{Bernoulli}(p_i) && \text{Random component} \\
\text{logit}(p_i) &= \log\!\left(\frac{p_i}{1-p_i}\right)=\eta_i && \text{Link (logit)} \\
\eta_i &= \beta_0 + \beta_1 x_{i1} + \cdots + \beta_k x_{ik} && \text{Systematic component}
\end{aligned}
$$




# Binomial Logistic Regression {.section}

## Random component

The ***Bernoulli(p)*** or the ***Binomial(n,p)*** distributions can be used as
**random component** when we have a binary dependent variable or the
number of successes over the total number of trials.

\

-   **Binary** (pass/fail), one trial per person; or
-   **Counts of successes** out of $n$ trials (e.g., items correct out of $n$).


## Bernoulli Distribution 

Suppose you collected data on whether each student passed the exam.  Let $K \in \{0, 1\}$ denote the outcome, where 0 = fail and 1 = pass.

:::: columns
::: {.column width="30%"}
```{r}
#| echo: false
#| fig-height: 7
#| fig-width: 4
# parameters
p <- 0.8
n <- 20

# data
dat_bern <- tibble::tibble(
  k = 0:1,
  prob = dbinom(k, size = 1, prob = p)
)

dat_binom <- tibble::tibble(
  k = 0:n,
  prob = dbinom(k, size = n, prob = p)
)

# bernoulli plot
p_bern <- dat_bern |>
  ggplot(aes(x = k, y = prob)) +
  geom_segment(aes(xend = k, yend = prob), y = 0,
               color = my_cols[2], linewidth = 3) +
  geom_point(size = 4, color = my_cols[2]) +
  labs(title = "Bernoulli", x = "k", y = "Probability") +
  theme_clean(base_size = 24) +
  scale_x_continuous(breaks = 0:1) +
  scale_y_continuous(limits = c(0, 0.8), breaks = round(seq(0, .8, .1), 2)) 


# binomial plot (your style)
p_binom <- dat_binom |>
  ggplot(aes(x = k, y = prob)) +
  geom_segment(aes(xend = k, yend = prob), y = 0,
               color = my_cols[2], linewidth = 3) +
  geom_point(size = 4, color = my_cols[2]) +
  labs(title = "Binomial(n = 20, p = 0.80)",
       x = "k", y = "Probability") +
  theme_clean(base_size = 24) +
  scale_x_continuous(breaks = 0:n) +
  scale_y_continuous(limits = c(0, 0.8), breaks = round(seq(0, .8, .1), 2)) 

p_bern
```

:::

::: {.column width="70%"}

$$
P(K = k) =
\begin{cases}
p       & \text{if } k = 1 \text{ (pass)}, \\
1-p & \text{if } k = 0 \text{ (fail)}.
\end{cases}
$$

:::fragment

Equivalently:

$$
P(K = k) = p^k(1-p)^{1-k}, \quad k \in \{0, 1\}
$$
:::


:::fragment

- Mean: $E[K] = p$
- Variance: $\text{Var}(K) = p(1-p)$

:::
:::
:::::


## Binomial Distribution 

Suppose $n$ students take the same exam and let $k$ be the number who pass.

$$
P(K = k) \;=\; \binom{n}{k}\, p^{k}\,(1-p)^{n-k}, \quad k = 0, 1, \ldots, n
$$

:::: columns
::: {.column width="60%"}
```{r}
#| echo: false
#| fig-height: 5.5
#| fig-width: 9

p_binom 
```

:::
::: {.column width="40%"}


- Mean: $E[K] = np$
- Variance: $\text{Var}(K) = np(1-p)$
- Sum of $n$ independent Bernoulli trials
- Bernuolli special case of Binomial when $n = 1$
:::
:::::


## Bernoulli

```{r}
#| echo: true
#| eval: true
#| cache: true
rbinom(n = 1, size = 1, prob = 0.7)
```

. . .

```{r}
#| echo: true
#| eval: true
#| cache: true
many = rbinom(n = 100000, size = 1, prob = 0.7); head(many)
```

. . .

```{r}
#| echo: true
#| eval: true
#| cache: true

# Mean
(p = mean(many))

```

. . .

```{r}
#| echo: true
#| eval: true
#| cache: true
# Variance
(p*(1-p))
var(many)
```

## Binomial

```{r}
#| echo: true
#| eval: true
n = 10; p = 0.7
rbinom(n = 1, size = n, prob = 0.7)
```

. . .

```{r}
#| echo: true
#| eval: true
many = rbinom(n = 100000, size = n, prob = p); head(many)
```

. . .

```{r}
#| echo: true
#| eval: true
n*p # Mean count: E(Y) = np 
mean(many)
```

. . .

```{r}
#| echo: true
#| eval: true
n*p*(1-p) # Variance count
var(many)
```

. . .

```{r}
#| echo: true
#| eval: true
mean(many/n) # Mean proportion = p
```


## Binomial distribution: $n = 20$, $p = 0.5$

What is the expected number of students passing the exam?

::: fragment

```{r}
#| echo: false
#| fig-height: 8
#| fig-width: 14

n <- 20
p <- 0.5
dat_binom <- data.frame(k = 0:n)
dat_binom$prob <- dbinom(x = dat_binom$k, size = n, prob = p)

p05 = dat_binom |> 
    ggplot(aes(x = k, y = prob)) +
    geom_segment(aes(x = k, xend = k, y = 0, yend = prob), 
                 color = my_cols[2], linewidth = 3) +
   geom_point(size = 4, color = my_cols[2]) +
    labs(x = "Number of Successes (k)", y = "Probability") +
    theme_clean(base_size = 24)+
  scale_x_continuous(breaks = round(seq(0,20,1)))+
  scale_y_continuous(breaks = round(seq(0,.5,.05),2))+
  guides(
    x = guide_axis(cap = "both"),
    y = guide_axis(cap = "both")
  )

p05

```

:::

## Binomial distribution: $n = 20$, $p = 0.9$

What is the expected number of students passing the exam?

::: fragment

```{r}
#| echo: false
#| fig-height: 8
#| fig-width: 14

n <- 20
p <- 0.9
dat_binom <- data.frame(k = 0:n)
dat_binom$prob <- dbinom(x = dat_binom$k, size = n, prob = p)

p09 = dat_binom |> 
    ggplot(aes(x = k, y = prob)) +
    geom_segment(aes(x = k, xend = k, y = 0, yend = prob), 
                 color = my_cols[1], linewidth = 3) +
   geom_point(size = 4, color = my_cols[1]) +
    labs(x = "Number of Successes (k)", y = "Probability") +
    theme_clean(base_size = 24)+
  scale_x_continuous(breaks = round(seq(0,20,1)))+
  scale_y_continuous(breaks = round(seq(0,.5,.05),2))+
  guides(
    x = guide_axis(cap = "both"),
    y = guide_axis(cap = "both")
  )

p09

```

::: 

## Mean–variance relationship

$$E[Y] = np \quad \text{and} \quad \mathrm{Var}(Y) = np(1-p)$$

```{r}
#| echo: false
#| fig-width: 10
#| fig-height: 6
#| fig-align: center

p <- seq(0, 1, 0.01)
df_mv <- data.frame(
  p = p,
  variance = p * (1 - p)
)

mid <- 0.5
w <- 0.25  # highlight range: [0.38, 0.62]

ggplot(df_mv, aes(x = p, y = variance)) +
  geom_line(data = subset(df_mv, p <  mid - w),
            linewidth = 2, color = my_cols[1]) +
  geom_line(data = subset(df_mv, p >= mid - w & p <= mid + w),
            linewidth = 2, color = my_cols[2]) +
  geom_line(data = subset(df_mv, p >  mid + w),
            linewidth = 2, color = my_cols[1]) +
  labs(
    x = "mean",
    y = "variance",
    title = " "
  ) +
  theme_clean(24) 

```



## Why not model probability directly?

\
**Problem for binary outcomes:**  
\
\
\
\
\

**Solution:**  
\
\



## Why not model probability directly?

\
**Problem for binary outcomes:**  
A linear model can predict values outside, but probabilities must stay between 0 and 1.
\
\
\

**Solution:**  
Transform $p$ using a **link function** that maps $[0,1]$ to $(-\infty, +\infty)$, then model *that* with a linear predictor.


# Odds, Odds Ratio and Log-Odds {.section}

## Odds

The **odds** compare the probability of success to the probability of failure:

$$
\text{odds}(p) = \frac{p}{1-p}
$$

\

::: fragment
**Example:** If $p=0.80$ (80% chance of passing), then
\

$$\text{odds}=\frac{0.80}{0.20}=4$$

This means 4 successes per 1 failure, on average.
:::

\

::: fragment
**Range:** Odds go from $0$ to $+\infty$, but cannot be negative.
:::


## Log-odds (logit)

Taking the **logarithm** of the odds gives the **logit**:


$$
\text{logit}(p) = \log\!\left(\frac{p}{1-p}\right)
$$

::: fragment
**Example:** If $p=0.80$, then $\text{odds}=4$ and

\

$$\text{logit}(0.80) = \log(4) \approx 1.39$$

\
:::

::: fragment
**Range:** Log-odds span $-\infty$ to $+\infty$ 

\

This can be modeled linearly: $\text{logit}(p_i) = \beta_0 + \beta_1 x_i$
:::


## Odds and Log-odds

```{r, echo=FALSE}
#| fig-width: 18
#| fig-height: 9
#| fig-align: left

p <- seq(0.0001, 1-0.0001, length.out = 5000)
odds <- p / (1 - p)
log_odds <- log(p / (1 - p))

df <- data.frame(p = p, log_odds = log_odds, odds = odds)

podd = ggplot(df, aes(x = p, y = odds)) +
  geom_line(linewidth = 1.2) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red", linewidth = 0.8) +
  scale_x_continuous(
    name = "p",
    breaks = seq(0, 1, 0.25),
    labels = c("0.00", "0.25", "0.50", "0.75", "1.00"),
    sec.axis = sec_axis(
      ~ .,
      name = "1 − p",
      breaks = seq(0, 1, 0.25),
      labels = c("1.00", "0.75", "0.50", "0.25", "0.00"),
      guide= guide_axis(cap = "both")
    )
  ) +
  scale_y_continuous(
    name = expression(frac(p, 1-p)),
    breaks = seq(0, 100, 25),
    limits = c(0,100)
  )+
  theme_clean(base_size = 24)

plog = ggplot(df, aes(x = p, y = log_odds)) +
  geom_line(linewidth = 1.2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red", linewidth = 0.8) +
  theme_clean(base_size = 24)+
  scale_x_continuous(
    name = "p",
    breaks = seq(0, 1, 0.25),
    labels = c("0.00", "0.25", "0.50", "0.75", "1.00"),
    sec.axis = sec_axis(
      ~ .,
      name = "1 − p",
      breaks = seq(0, 1, 0.25),
      labels = c("1.00", "0.75", "0.50", "0.25", "0.00"),
      guide= guide_axis(cap = "both")
    )
  ) +
  scale_y_continuous(
    name = expression(log(frac(p, 1-p))),
    breaks = seq(-15, 15, 5),
  )
podd|plog
```


Log-Odds: “Equally strong” probabilities on opposite sides of 0.5 become equal in magnitude but opposite in sign.


## Example

Let $y_i \in \{0,1\}$ (Pass), and
$p_i = P(y_i=1 \mid x_i)$ where $x_i$ = hours studied. 

\

::: fragment

::: columns
::: {.column width="52%"}
**Odds**: $o_i = \dfrac{p_i}{1-p_i}$
\
\
\

**Log-odds (logit)**:
$\text{logit}(p_i)=\log\!\left(\dfrac{p_i}{1-p_i}\right)$.
:::

::: {.column width="48%"}
\
\
**Linear predictor**:
$$
\text{logit}(p_i) = \eta_i,\quad
\eta_i = \beta_0 + \beta_1 x_i .
$$
:::
:::::
:::

\

::: fragment
$\beta_1$ describes change in *log-odds*, not probability. What does that mean?
:::



## From Log-Odds to Odds

**Starting point**: $\log\!\left(\dfrac{p(x)}{1-p(x)}\right) =  \beta_0 + \beta_1 x$ 

\

::: fragment
Exponentiating both sides gives the **odds**: 

$$
\text{odds}(x) = \frac{p(x)}{1-p(x)} = \exp(\beta_0 + \beta_1 x)
$$ 
:::

\

::: fragment
Using properties of exponents: $\exp(\beta_0 + \beta_1 x) = \exp(\beta_0) \cdot \exp(\beta_1 x)$
:::

## From $x = 0$ to $x = 1$

$$\text{odds}(x) = \exp(\beta_0 + \beta_1 x)$$

::: fragment
**When $x = 0$**: $\text{odds}(0) = \exp(\beta_0)$
:::


::: fragment
**When $x = 1$**: $\text{odds}(1) = \exp(\beta_0 + \beta_1) = \exp(\beta_0) \cdot \exp(\beta_1)$
:::


::: fragment
**The ratio**:

$$
\frac{\text{odds}(1)}{\text{odds}(0)} = \frac{\exp(\beta_0) \cdot \exp(\beta_1)}{\exp(\beta_0)} = \exp(\beta_1)
$$
:::

## The Odds Ratio

This ratio is constant **regardless of where you start**!

\

**For any 1-unit increase** (from $x$ to $x+1$):

$$
\frac{\text{odds}(x+1)}{\text{odds}(x)}
=
\frac{\exp(\beta_0) \cdot \exp(\beta_1(x+1))}{\exp(\beta_0) \cdot \exp(\beta_1 x)}
$$

::: fragment
The $\exp(\beta_0)$ terms cancel:

$$
\frac{\text{odds}(x+1)}{\text{odds}(x)}
=
\frac{\exp(\beta_1(x+1))}{\exp(\beta_1 x)}
= \exp(\beta_1)
$$
:::


\

::: fragment
This constant multiplier is the **odds ratio (OR)**.

\

$\rightarrow$ $\exp(\beta_1)$ tells us how the odds multiply for each 1-unit increase in $x$.
:::

## Example

With $\beta_1 = 0.8$, **for a 1-unit increase** ($x \to x+1$):

\

$$
\frac{\text{odds}(x+1)}{\text{odds}(x)} = \exp(\beta_1) = \exp(0.8) \approx 2.23
$$

\

::: fragment
**What this means**: 

- Studying 1 more hour multiplies your odds of passing by 2.23 (not the probability!)
- This is true whether you go from 2 to 3 hours, 5 to 6 hours, or 10 to 11 hours
:::


## Linear on log-odds

Equal changes in $x$ $\rightarrow$ equal changes in log-odds  
**BUT** odds multiply by constant factor $\exp(\beta_1)$

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: left
#| fig-height: 8
#| fig-width: 18

b0 <- 0
b1 <- 0.8 

# Generate data for plotting
x_vals <- seq(-10, 10, length.out = 300)
eta_vals <- b0 + b1 * x_vals
p_vals <- plogis(eta_vals)
Y_LogOdds <- log(p_vals/(1 - p_vals))  # equals eta_vals 

data <- data.frame(x = x_vals, Y_LogOdds = Y_LogOdds)

# Define x positions for vertical lines (equal intervals in x)
x_positions <- seq(-4, 4, 1)

# Corresponding log-odds values at those x's
y_LogOdds <- b0 + b1 * x_positions

# PLOT
ggplot(data, aes(x = x, y = Y_LogOdds)) +
  geom_line(color = my_cols[2], linewidth = 3) +
  geom_vline(xintercept = x_positions, linetype = "dashed",
             color = "red", linewidth = 1) +
  geom_hline(yintercept = y_LogOdds, linetype = 1,
             color = "#777777", linewidth = 1) +
  theme_clean(base_size = 26) +
  labs(x = expression(x), y = expression(log(odds))) +
  scale_y_continuous(breaks = y_LogOdds) +
  scale_x_continuous(breaks = -4:4) +
  coord_cartesian(ylim = c(-3, 3), 
                  xlim = c(-4, 4.3))+
  theme(plot.margin = margin(10, 80, 10, 10)) +                    
  geom_text(
    data = data.frame(
      x = 4.35,  # put just to the right of the panel
      y = y_LogOdds,
      label = paste0("odds(x) = ", round(exp(y_LogOdds), 2))
    ),
    aes(x = x, y = y, label = label),
    inherit.aes = FALSE,
    color = "black", hjust = .8, vjust = -0.2, size = 10, 
    fontface = "bold"
  )



```

With $\beta_0=0, \beta_1=0.8$: each +1 step in $x$ adds $0.8$ to log-odds, multiplies odds by $\exp(0.8) \approx 2.23$

## Any two values

**For any difference** $\Delta x = x_2 - x_1$:
\

$$
\frac{\text{odds}(x_2)}{\text{odds}(x_1)}
= \frac{\exp(\beta_0 + \beta_1 x_2)}{\exp(\beta_0 + \beta_1 x_1)}
= \exp\!\bigl(\beta_1(x_2-x_1)\bigr)
$$

\

::: fragment
**Example** ($\beta_1=0.8$): comparing $x=6$ to $x=2$ ($\Delta x = 4$):

\

$$
\frac{\text{odds}(6)}{\text{odds}(2)}
= \exp\!\bigl(0.8 \times 4\bigr)
= \exp(3.2) \approx 24.5
$$

\

Studying 4 more hours multiplies odds by 24.5!
:::


## What About Probability?

We've been working with **odds** and **odds ratios**, but what about $p(x)$?

\

::: fragment
**From odds back to probability**:

$$
p(x) = \frac{\text{odds}(x)}{1 + \text{odds}(x)} = \frac{\exp(\beta_0 + \beta_1 x)}{1 + \exp(\beta_0 + \beta_1 x)}
$$

:::

\

::: fragment
Unlike odds, probability changes are **not constant**!

- If $p = 0.1$, adding 1 hour might increase it to $p = 0.20$ (+0.10)
- If $p = 0.5$, adding 1 hour might increase it to $p = 0.69$ (+0.19)
- If $p = 0.9$, adding 1 hour might increase it to $p = 0.96$ (+0.06)

:::

## Not linear in probability

$\rightarrow$ **Same $\beta_1$, same OR (2.23×), but different probability changes!**

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: left
#| fig-height: 10
#| fig-width: 18

# Choose coefficients so +1 in x gives +b1 in log-odds (not +1)
b0 <- 0
b1 <- 0.8

# Generate data for plotting
x_vals <- seq(-10, 10, length.out = 300)
eta_vals <- b0 + b1 * x_vals
Y_Logit  <- plogis(eta_vals)  # p = inverse-logit(eta) [web:227]

data <- data.frame(x = x_vals, Y_Logit = Y_Logit)

# Define x positions for vertical lines (equal intervals in x)
x_positions <- seq(-4, 4, 1)

# Compute corresponding y values (probabilities) at those x's
y_logis <- plogis(b0 + b1 * x_positions)  # [web:227]

# PLOT
ggplot(data, aes(x = x, y = Y_Logit)) +
  geom_line(color = my_cols[2], linewidth = 3) +
  geom_vline(xintercept = x_positions, linetype = "dashed",
             color = "red", linewidth = 1) +
  geom_hline(yintercept = y_logis, linetype = 1,
             color = "#777777", linewidth = 1) +
  labs(
    x = expression(x),
    y = "p"
  ) +
  scale_x_continuous(breaks = -4:4) +
  coord_cartesian(xlim = c(-5, 4), ylim = c(0, 1)) +
  scale_y_continuous(breaks = seq(0, 1, .1)) +
  theme_clean(base_size = 24) +
  geom_text(
    data = data.frame(x = x_positions, y = 0.05, label = paste0("x = ", x_positions)),
    aes(x = x, y = y, label = label),
    color = "black", vjust = 3, size = 7, fontface = "bold"
  ) +
  geom_text(
    data = data.frame(x = -5, y = y_logis, label = paste0("p = ", round(y_logis, 2))),
    aes(x = x, y = y, label = label),
    color = "black", hjust = 0, vjust = -.2, size = 8, fontface = "bold"
  )
```


## Let's try

```{r, echo=TRUE}
beta0 = 0
# odds?
# p?
```

::: fragment
```{r, echo=TRUE}
(odds = exp(beta0))
```
:::

::: fragment
```{r, echo=TRUE}
(p = odds/(1+odds))
(p = exp(beta0)/(1+exp(beta0)))
```
:::

## Let's try

::: fragment
```{r, echo=TRUE}
beta1 = 1
# what happen moving from x = 0 to x = 1?
```
:::

::: fragment
```{r, echo=TRUE}
(odds1 = odds*exp(beta1))
(odds1 = exp(beta0)*exp(beta1))
```
:::

::: fragment
```{r, echo=TRUE}
(p1 = odds1 /(1+odds1))
(p1 = exp(beta0 + beta1)/(1+exp(beta0+beta1)))
```
:::


## Let's try

```{r, echo=TRUE}
beta0 = 3
# odds?
# p?
```

::: fragment
```{r, echo=TRUE}
(odds = exp(beta0))
```
:::

::: fragment
```{r, echo=TRUE}
(p = odds/(1+odds))
(p = exp(beta0)/(1+exp(beta0)))
```
:::

## Let's try
::: fragment
```{r, echo=TRUE}
beta1 = 1
# what happen moving from x = 0 to x = 1?
```
:::

::: fragment
```{r, echo=TRUE}
(odds1 = odds*exp(beta1))
```
:::

::: fragment
```{r, echo=TRUE}
(p1 = odds1 /(1+odds1))
(p1 = exp(beta0 + beta1)/(1+exp(beta0+beta1)))
```
:::


##


![](/img/coffe.png){fig-align="center" width="500"}


[Vecteezy](https://www.vecteezy.com/free-png/funny)


# Extra slides 

## The GLM variance structure

::: incremental

- In GLMs, variance is **not constant** — it changes with the mean: $\mathrm{Var}(Y\mid X) = \phi V(\mu)$

- The **variance function** $V(\mu)$ defines how variance depends on the mean (e.g., $V(\mu) = \mu$ for Poisson, $V(\mu) = \mu(1-\mu)$ for binomial)

- Once you model $\mu(x)$, the variance is also determined **at each x** (up to the scale $\phi$).

- **Heteroscedasticity** is implicit in GLMs

:::

## What is dispersion?

::: incremental

- **Dispersion** quantifies how spread out the data are around the mean

- In the **Normal distribution**: $Y \sim N(\mu, \sigma^2)$, the dispersion parameter is $\phi = \sigma^2$ (the variance)

- $\sigma^2$ controls spread **independently** of $\mu$ — you can have any mean with any variance

- This **separation** between location ($\mu$) and scale ($\sigma^2$) is a defining feature of the Normal family

:::

## Dispersion ($\phi$) is fixed

::: incremental

- Different families handle dispersion **differently**

- **Normal**: $\text{Var}(Y) = \sigma^2$ (constant, independent of mean)

- **Poisson**: $\text{Var}(Y) = \mu$ (variance **equals** the mean, $\phi = 1$ fixed)

- **Binomial**: $\text{Var}(Y) = n\mu(1-\mu)$ (variance is a **function** of the mean, $\phi = 1$ fixed)

- Fixing $\phi = 1$ means the **entire variance is determined by** $V(\mu)$ — no additional free parameter for "extra variability".


:::

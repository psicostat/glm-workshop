---
title: Binomial GLM example
subtitle: Generalized Linear Models Workshop
format: 
  minimal-revealjs:
    slide-number: true
    html-math-method: mathjax
execute: 
  echo: true
date: last-modified
date-format: "*[Last modified:] DD-MM-YYYY*"
toc: true
---

```{r}
#| label: setup
#| include: false
#| message: false
#| warning: false

library(here)
library(lme4)      # for mixed-models
library(tidyverse) # for data manipulation
library(ggplot2)   # plotting

dat <- readRDS(here("data/shimizu2024.rds"))

knitr::opts_chunk$set(
  fig.align = "center",
  dev = "svg"
)
```

```{r}
#| label: functions
#| include: false

compare <- function(x) {
  if(is.null(attributes(x)$dimnames)){
    names(x) <- 1:length(x)
  }
  all_comp <- combn(x, 2, simplify = FALSE)
  out <- vector(mode = "numeric", length = length(all_comp))

  for(i in 1:length(all_comp)){
    comp <- all_comp[[i]]
    ors <- or(comp[1], comp[2])
    out[i] <- ors
  }

  names(out) <- sapply(all_comp, function(x) paste(names(x), collapse = " / "))
  out
}

style_output <- function(x, lines, class = "hg"){
  x <- capture.output(x)
  x <- htmltools::htmlEscape(x)
  x[lines] <- sprintf("<span class='%s'>%s</span>", class, x[lines])
  cat("<pre><code>")
  cat(x, sep = "\n")
  cat("</code></pre>")
}
```

```{r}
#| label: ggplot2
#| include: false

mtheme <- function(){
    theme_minimal(20) +
        theme(legend.position = "bottom",
              legend.title = element_blank())
}

theme_set(mtheme())
```

# Practical example

## Example: @Shimizu2024-xl

```{=html}
<style>
    .hg {
        background-color: yellow;
    }
</style>
```

@Shimizu2024-xl investigated the processing of emotional faces.

- 6 basic emotions: anger, disgust, fear, happiness, sadness and surprise
- intensity in % (from 10% to 100% in steps of 10%)
- 71 participants
- 377 faces (males and females of different identities)
- forced-choice procedure with 7 options (6 emotions + neutral). Chance level at $1/7 = 0.14$.

## @Shimizu2024-xl dataset

We did some pre-processing for the purpose of this example. The original dataset can be found at {{< ai osf >}} [osf.io/zhtbj/](https://osf.io/zhtbj/).

You can download the dataset for this example at this [link](../data/shimizu2024.rds). It is a `rds` file, and you can open it using:

```{r}
#| eval: false
#| echo: true

dat <- readRDS("shimizu.rds")
```

Then we can load some packages:

```{r}
#| eval: false

library(tidyverse) # for data manipulation
library(ggplot2)   # plotting
```

## Exploring

For the purpose if this workshop, we will focus on a single subject (otherwise we should use a mixed-effects model). We also select only the relevant columns.

```{r}
dat <- subset(dat, id == 22)
dat <- dat[, c("id", "age", "intensity", "emotion_lbl", "response_lbl", "acc")]
dat
```

## Exploring

- We have `r nrow(dat)` trials and `r ncol(dat)` columns. 
- The `intensity` is the intensity (from 10% to 100%) of the facial expression. `emotion_lbl` is the emotion and `response_lbl` is the response. 
- When `emotion_lbl = response_lbl` the `acc = 1` namely a correct response.

## Exploring

We can calculate the average accuracy for each emotion. Clearly there is a big difference with `fear` being the hardest one and `surprise` the easiest. We remove `neutral` because we have no associated intensity

```{r}
dat |> 
  group_by(emotion_lbl) |> 
  summarise(p = mean(acc),
            n = n()) |> 
  arrange(desc(p))

dat <- filter(dat, emotion_lbl != "neutral")
```

```{r}
#| include: false

dat_shimizu <- dat
```

## Exploring

Also for intensity, there is a clear pattern. An increasing intensity is associated with increased accuracy.

```{r}
dat |> 
  group_by(intensity) |> 
  summarise(p = mean(acc),
            n = n()) |> 
  arrange(desc(p))
```

## Exploring, plots

```{r}
#| output-location: slide

dat |> 
  group_by(emotion_lbl) |> 
  summarise(p = mean(acc),
            n = n()) |> 
  ggplot(aes(x = fct_reorder(emotion_lbl, p), y = p)) + 
  geom_point(size = 4) +
  geom_line(group = 1) +
  ylim(c(0, 1)) +
  xlab("Emotion") +
  ylab("Accuracy") +
  geom_hline(yintercept = 1/7, lty = "dotted")
```

## Exploring, plots

```{r}
#| output-location: slide
#| out-width: 100%
#| out-heigth: 100%

dat |> 
  group_by(intensity) |> 
  summarise(p = mean(acc),
            n = n()) |> 
  arrange(desc(p)) |> 
  ggplot(aes(x = intensity, y = p)) + 
  geom_point(size = 4) +
  geom_line() +
  ylim(c(0, 1)) +
  xlab("Intensity (%)") +
  ylab("Accuracy") +
  geom_hline(yintercept = 1/7, lty = "dotted")
```

## Exploring, plots

We have few trials, but we can also explore the interaction between emotion and intensity. There are some emotions where the rate of increase in accuracy as a function of the emotion is faster compared to others.

```{r}
#| output-location: slide
#| out-width: 100%
#| out-heigth: 100%

dat |> 
  group_by(intensity, emotion_lbl) |> 
  summarise(p = mean(acc)) |> 
  ggplot(aes(x = intensity, y = p, color = emotion_lbl)) +
  geom_point(size = 4) +
  geom_line()
```

## Exploring, odds and odds ratios

We can start exploring the effects calculating odds and odds ratios.

```{r}
odds <- function(p) p / (1 - p)
or <- function(pn, pd) odds(pn) / odds(pd)
```

```{r}
#| collapse: true

(p_anger <- mean(dat$acc[dat$emotion_lbl == "anger"]))
(p_surprise <- mean(dat$acc[dat$emotion_lbl == "surprise"]))

odds(p_anger)
odds(p_surprise)

or(p_surprise, p_anger)
```

## Exploring, odds and odds ratios

We can also create a contingency table:

```{r}
table(dat$acc, dat$emotion_lbl)
(all_p <- tapply(dat$acc, dat$emotion_lbl, FUN = mean))
odds(all_p)
```

When the odds are lower than 1, the probability of success is lower than the probability of failure. When the odds are greater than 1 the probability of success is higher.

## Exploring, odds and odds ratios

We can also calculate all the possible comparisons. Note that depending on the numerator/denominator the odds ratio is different, but we can simply take the inverse to switch numerator and numerator. Interpreting odds ratio > 1 is usually more intuitive.

```{r}
#| echo: false

compare(
    all_p
) |> round(3)
```

For example, `happiness / sadness ~ 2.04` means that the odds (not the probability) of a correct response is 2 times higher for happy faces compared to sad faces. 

# Model

## The `glm` function

In R we can fit a GLM with the `glm` function. The syntax is the same as the `lm` (for standard linear models). We only need to specify the **random component** and the **link function**.

```{r}
#| eval: false

glm(y ~ x1 + x2 + x3 * x4, # systematic component (linear predictor)
    data = data,
    family = binomial(link = "logit")) # random component and link function
```

Clearly, the `y` in this example need to be consistent with the chosen family. In this case the model is expecting a 0-1 vector. If we provide labels (characters) or number > 1 or < 0 the function will fail.

::: {.callout-note}
A `glm` with `family = gaussian(link = "identity")` is the same as running a `lm`. Internally `glm` calls `lm` in this case.
:::

## The null model

We can start with the easiest model that is a model without the systematic component (with no predictors).

```{r}
fit0 <- glm(acc ~ 1, data = dat, family = binomial(link = "logit"))
summary(fit0)
```

## The null model

![](img/fit0_summary.png)

## The null model, formally

$$
\eta_i = \beta_0
$$

$$
p_i = g^{-1}(\eta_i) = g^{-1}(\beta_0)
$$

$g^{-1}(\cdot)$ is the inverse-logit link:

$$
p_i = \frac{e^{\beta_0}}{1 + e^{\beta_0}}
$$

In other terms, the probability can be calculated inverting the logit link function evaluated on the linear predictor $\eta$. In this case $\eta$ only contains $\beta_0$.

## The null model, interpretation

In this case, the intercept is `r round(coef(fit0)[1], 3)`. The intercept is the expected value (i.e., the mean) of `y` (accuracy here) when everything is zero. In this case $\beta_0$ is just the (logit transformed) overall accuracy.

```{r}
#| collapse: true

b0 <- coef(fit0)["(Intercept)"]
exp(b0) / (1 + exp(b0)) # inverse logit
plogis(b0)              # directly with the dedicated function
mean(dat$acc)           # average accuracy
log(odds(mean(dat$acc))) # probability to logit
qlogis(mean(dat$acc)) # probability to logit with the dedicated function
```

## Categorical predictor, emotion

Then we can include `emotion_lbl` as predictor. Let's see what happens:

```{r}
fit_em <- glm(acc ~ emotion_lbl, data = dat, family = binomial(link = "logit"))
summary(fit_em)
```

## Categorical predictor, emotion

Now we have 6 coefficients. As in standard linear models, by default, categorical predictors are transformed into dummy variables:

```{r}
unique(model.matrix(~ emotion_lbl, data = dat))
```

The intercept is the reference level (in this case `anger`) and the other 5 coefficients represent the comparison between all emotions vs anger.

## Categorical predictor, emotion {.smaller}

Remember that we are working on the link-function space where comparisons are expressed in logit. $\beta_1$ (`emotion_lbldisgust`) is the comparison between `disgust` and `anger`. Formally:

$$
\beta_1 = \mbox{logit}(p(y = 1 | \mbox{disgust})) - \mbox{logit}(p(y = 1 | \mbox{anger}))
$$

But the logit is the logarithm of the odds (let's call $p_a$ and $p_d$ anger and disgust respectively)

$$
\log{\frac{p_d}{1 - p_d}} - \log{\frac{p_a}{1 - p_a}}
$$

A difference of logs can be expressed as the log of the ratio. We can take the exponential to remove the log.

$$
\log{\frac{\frac{p_d}{1 - p_d}}{\frac{p_a}{1 - p_a}}} \qquad e^{\log{\frac{\frac{p_d}{1 - p_d}}{\frac{p_a}{1 - p_a}}}} = \frac{\frac{p_d}{1 - p_d}}{\frac{p_a}{1 - p_a}}
$$

This is exactly the odds ratio! This means that taking the exponential of $\beta_1$ returns the estimated odds ratio for that comparison.

## Categorical predictor

```{r}
coef(fit_em)
exp(coef(fit_em))
```

Comparing with the manual calculation:

```{r}
p_d <- mean(dat$acc[dat$emotion_lbl == "disgust"])
p_a <- mean(dat$acc[dat$emotion_lbl == "anger"])

or(p_d, p_a)
```

## Main effect of `emotion`

We can also assess the effect of `emotion_lbl` using a Likelihood Ratio Test (LRT). Basically we can compare the model with or without the `emotion_lbl` predictor. Using the LRT we are setting the effect of `emotion_lbl` to zero. This means that the null hypothesis is that all possible contrasts among emotions are zero. 

```{r}
anova(fit0, fit_em) # comparing two models
```

## Main effect of `emotion`

```{r}
car::Anova(fit_em)  # using the car::Anova
```

## Confidence intervals

The confidence intervals for model parameters can be extracted with the `confint.default()` function. These are called Wald confidence intervals. They are computed as:

$$
\beta \pm q_{\alpha/2} \mbox{SE}_\beta
$$

Where $q$ is the critical test statistics ($z$ in this case) at $\alpha$ level and $\mbox{SE}_\beta$ is the standard error.

```{r}
fits <- summary(fit_em)$coefficients
fits
```

## Confidence intervals

```{r}
(z <- abs(qnorm(0.05/2))) # critical test statistics at alpha/2 (two tails)
data.frame(
  lower = fits[, "Estimate"] - z * fits[, "Std. Error"],
  upper = fits[, "Estimate"] + z * fits[, "Std. Error"]
)
```


## Confidence intervals

Or directly using the `confint.default()`

```{r}
confint.default(fit_em)
```

## Confidence intervals

But these are confidence intervals of log odds ratios (differences in logit). To obtain confidence intervals of odds ratios we can just take the exponential of the upper and lower bound:

```{r}
exp(confint.default(fit_em))
```

Notice that, for differences in logit the *null* value is zero. Taking $e^0 = 1$ thus the *null* value of an odds ratio is 1 (numerator is the same as the denominator).

## Confidence intervals

The real default for confidence intervals using just `confint()` (and not `confint.default()`) are the so-called profile likelihood confidence intervals. The main difference is that Wald confidence intervals are symmetric by definition while the profile likelihood not necessary.

```{r}
confint(fit_em)
# you can also do exp(confint(fit_em))
```

In this case they are very similar to the Wald but is not always like this.

::: aside
See this [post](https://thestatsgeek.com/2014/02/08/wald-vs-likelihood-ratio-test/) for a nice overview.
:::

## Specific contrasts of `emotion` levels

We can also test some specific contrasts using the `emmeans` or the `multcomp` package. For example:

```{r}
library(emmeans)
mm <- emmeans(fit_em, ~ emotion_lbl)
mm
```

These are called **e**stimated **m**rginal **means**. Importantly, the `emmeans` package uses the model and not the data. Marginal means will depend on the fitted model.

## Specific contrasts of `emotion` levels

We would expect estimated probabilities but we have values in logit (this is why we have negative values). We can also transform the logit into probabilities:

```{r}
#| eval: false

emmeans(fit_em, ~ emotion_lbl, type = "response")
```

```{r}
#| echo: false
#| output: asis

emmeans(fit_em, ~ emotion_lbl, type = "response") |> 
    style_output(10, "hg")
```

## What `emmeans` is doing?

To understand what `emmeans` is doing we need to introduce the term prediction. Given the predictors we ask the model the predicted logit or probability.

```{r}
# prediction for the first 5 trials. 
# The best prediction of the model is the (logit) mean

head(predict(fit0, type = "link"))
```

## What `emmeans` is doing?

```{r}
# prediction for the first 5 trials. 
# the prediction depend on the emotion

head(predict(fit_em, type = "link")) 
head(predict(fit_em, type = "response")) 
head(plogis(predict(fit_em, type = "link"))) # same
```

## What `emmeans` is doing?

For example, to know what is the predicted accuracy for `anger` and `disgust` we can do:

```{r}
predict(fit_em, newdata = data.frame(emotion_lbl = c("anger", "disgust")), type = "response")
```

Or manually:

```{r}
B <- coef(fit_em)
c(plogis(B["(Intercept)"]), # anger
  plogis(B["(Intercept)"] + B["emotion_lbldisgust"])) # disgust
```

On the logit scale, we can do linear combinations of coefficients. This is not valid on the probability scale, this is the reason why we need the link function.

## What `emmeans` is doing?

For reproducing the entire `emmeans` output we just need to provide all emotions into `newdata = `

```{r}
nd <- data.frame(emotion_lbl = unique(dat$emotion_lbl))
data.frame(predict(fit_em, newdata = nd, se = TRUE, type = "response"))
```

## Contrasts with `emmeans`

We can also compute all contrasts across emotions:

```{r}
# or emmeans(fit_em, pairwise ~ emotion_lbl)
pairs(mm, p.adjust = "bonferroni")
```

Be careful to the multiple comparison approach! You can use the `p.adjust =` argument and choose an appropriate method.

## Contrasts with `emmeans`

Some of these contrasts are also the model parameters:

```{r}
#| echo: false
#| output: asis

pp <- pairs(mm, p.adjust = "bonferroni")
style_output(pp, 2:6, "hg")
```

## Contrasts with `emmeans`

You can also express the contrasts into the probability space. We are just taking the exponential thus transforming differences of logit into odds ratios.

```{r}
pairs(mm, type = "response")
```

Notice that: *Tests are performed on the log odds ratio scale*

## Custom contrasts

Clearly you can also provide custom contrasts like `contr.sum()` or `MASS::contr.sdiff()` (for comparing the next level with the previous level). For an overview about contrasts coding see @Granziol2025-sy and @Schad2020-ht.

```{r}
contrast(mm, "consec") # consec ~ MASS::contr.sdif()
# see ?emmeans::contrast
```

Here we are comparing 2 vs 1, 2 vs 3, 3 vs 4, etc.

## Plotting

There are several options to plot the model results. In this case we could plot the predicted probability for each emotion and the confidence intervals. You can use the `effects` package or `ggeffects` (that internally uses `effects`) to create `ggplot2` objects.

```{r}
library(ggeffects)
# grid of prediction and CI (as we did with emmeans or predict())
ggeffect(fit_em, "emotion_lbl")
```

## Plotting

```{r}
# this return a ggplot2 object, you can add layers with +
plot(ggeffect(fit_em, "emotion_lbl"))
```

## Numerical predictor, effect of `intensity`

Now let's fit a model with only the effect of intensity.

```{r}
fit_int <- glm(acc ~ intensity, data = dat, family = binomial(link = "logit"))
summary(fit_int)
```

## Numerical predictor, effect of `intensity`

Now the intercept is the logit accuracy when `intensity` is zero (that is not really a meaningful value). $\beta_1$ here is the expected increase in logit for a unit increase in intensity. In other terms, moving from intensity e.g., 10 to 11 increase the logit of 0.03.

As for the emotion, we can take the exponential of $\beta_1$ obtaining the odds ratio of increasing 1 unit in intensity:

```{r}
exp(coef(fit_int))
```

We have an odds ratio of 1.03 that is quite low (1 is the null value). But this is a scale problem, 1 point in the intensity scale is meaningless.

## Numerical predictor, effect of `intensity`

Before improving the model, notice that the story is the same regardless having categorical or numerical predictor. For categorical predictors the coefficients are odds ratios (or difference in logit) comparing levels (`anger` vs `fear`). For numerical predictor the coefficients are odds ratios (or difference in logit) comparing values separated by 1 unit (in the scale of the predictor).

```{r}
(pp <- predict(fit_int, newdata = data.frame(intensity = c(15, 16))))
diff(pp) # same as b1
exp(diff(pp))  # same as exp(b1)
```

## Numerical predictor, effect of `intensity`

In fact, we can clearly see that on the logit scale the effect is linear while on the probability scale is not linear.

```{r}
# pairs of unit differences in different positions of x
diffs <- list(c(10, 11), c(50, 51), c(80, 81))
sapply(diffs, function(d) diff(predict(fit_int, data.frame(intensity = d))))
```

But is not the same when we take differences in probabilities

```{r}
# pairs of unit differences in different positions of x
diffs <- list(c(10, 11), c(50, 51), c(80, 81))
sapply(diffs, function(d) diff(predict(fit_int, data.frame(intensity = d), type = "response")))
```

## Numerical predictor, effect of `intensity`

Same increase in `intensity` produces a different increase on the probability scale but not on the logit scale. 

```{r}
#| echo: false

pp <- data.frame(intensity = seq(10, 100, 1))
pp$Logit <- qlogis(0.01) + pp$intensity * 0.1
pp$Accuracy <- plogis(pp$Logit)

hg <- data.frame(
  intensity = c(20, 50, 80)
)

hg$Logit <- qlogis(0.01) + hg$intensity * 0.1
hg$Accuracy <- plogis(hg$Logit)
hg <- pivot_longer(hg, c(2, 3))

pp |> 
  pivot_longer(c(Logit, Accuracy)) |>
  ggplot(aes(x = intensity, y = value)) +
  facet_wrap(~name, scales = "free_y") +
  geom_line() +
  theme(axis.title.y = element_blank()) +
  xlab("Intensity") +
  geom_segment(data = hg,
  aes(x = intensity, xend = intensity, y = -Inf, yend = value), lty = "dotted") +
  geom_segment(data = hg,
  aes(x = intensity, xend = -Inf, y = value, yend = value), lty = "dotted")
```

## Numerical predictor, marginal effects

This means that for interpreting results in the probability scale (what we actually want) we cannot think in linear terms (as in standard linear regression). For each value of `intensity` we have a different slope (i.e., derivative).

```{r}
#| echo: false

x <- seq(0, 100, 10)
b0 <- qlogis(0.01)
b1 <- 0.1

# sigmoid values
y <- plogis(b0 + b1 * x)

# derivative
dy_dx <- b1 * y * (1 - y)

# slope segment half-length in x
dx <- 2

df <- data.frame(
  x = x,
  y = y,
  slope = dy_dx,
  x1 = x - dx,
  x2 = x + dx,
  y1 = y - dy_dx * dx,
  y2 = y + dy_dx * dx
)

ggplot(df, aes(x, y)) +
  geom_line(lwd = 1.2) +
  geom_point(size = 2) +
  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),
               color = "red", linewidth = 2) +
  scale_y_continuous(limits = c(0, 1)) +
  labs(x = "Intensity", y = "Probability")
```

## Marginal effects

Now let's plot all the red slopes and see what we can learn:

```{r}
#| echo: false

x <- seq(0, 100, 10)
b0 <- qlogis(0.01)
b1 <- 0.1

# sigmoid values
y <- plogis(b0 + b1 * x)

# derivative
dy_dx <- b1 * y * (1 - y)

data.frame(x, dy_dx) |> 
    ggplot(aes(x = x, y = dy_dx)) +
    geom_line() +
    geom_hline(yintercept = mean(dy_dx), col = "dodgerblue", lwd = 1) +
    geom_vline(xintercept = x[which.max(dy_dx)], col = "firebrick", lty = "dotted") +
    ylab("Slope") +
    geom_point(x = x[which.max(dy_dx)], y = dy_dx[which.max(dy_dx)], size = 4, col = "firebrick") +
    xlab("Intensity")
```

## Marginal effects: a really comprehensive framework

The `marginaleffects` [package](https://marginaleffects.com/) provide a very complete and comprehensive framework to compute marginal effects for several models. You can have a very detailed overview of the theory and the functions reading:

- The main paper by @Arel-Bundock2024-zl
- [https://www.youtube.com/watch?v=ANDC_kkAjeM](https://www.youtube.com/watch?v=ANDC_kkAjeM)

## Marginal effects of `intensity`

```{r}
library(marginaleffects)
sl <- slopes(fit_int, variables = "intensity", by = "intensity")
sl
```

## Marginal effects of `intensity`

```{r}
# average marginal effect
avg_slopes(fit_int)
```

## Marginal effects of `intensity`

```{r}
# max marginal effect
filter(sl, estimate == max(estimate))
```

## Marginal effects of `intensity`

```{r}
plot_slopes(fit_int, variables = "intensity", by = "intensity", vcov = FALSE)
```

## Plotting

The pattern for intensity is almost linear, this is why we have more than one maximum.

```{r}
plot(ggeffect(fit_int, "intensity"))
```

## Inverse estimation

We can also do what is called inverse estimation (common in Psychophysics). We can ask the model what is the level of `intensity` required to achieve a certain accuracy.

```{r}
MASS::dose.p(fit_int, p = 0.75)
```

Furthermore, we need roughly 90% of `intensity` to have an accuracy of 75%.

## Improving the model

We have two problems in terms of interpretability in this model:

- The intercept is meaningless because 0% intensity is not a plausible value
- Intensity from 0% to 100% in steps of 1% is too granular

We can center the variable on the minimum (0 become 10%) and rescale the variable from 0 (10%) to 10 (100%) where the unit increase is 10%.

```{r}
dat$intensity10 <- (dat$intensity - 10) / 10
```

## Additive model, `intensity` and `emotion`

We can now fit a model with the additive effect of emotion and intensity. To simplify the pattern let's keep only two emotions.

```{r}
fit_int_emo <- glm(acc ~ intensity10 + emotion_lbl, data = dat, family = binomial(link = "logit"), subset = emotion_lbl %in% c("anger", "surprise"))

summary(fit_int_emo)
```

## Additive model, `intensity` and `emotion`

The interpretation is the same as before. The `intercept` is the expected logit when everything is zero (intensity = 10 and emotion = anger).

`intensity` is the increase in logit accuracy for a unit (10%) increase in intensity controlling for `emotion_lbl`.

`emotion_lblsurprise` is the logit difference between anger and surprise controlling for `emotion_lbl`.

## Additive model, `intensity` and `emotion`

We can have also the two main effects (not really useful with a factor with two levels):

```{r}
car::Anova(fit_int_emo)
```

## Additive model, `intensity` and `emotion`

```{r}
library(patchwork) # composing plots

plot(ggeffect(fit_int_emo, "intensity10")) + 
  plot(ggeffect(fit_int_emo, "emotion_lbl")) 
```

## Additive model, `intensity` and `emotion`

This means that we have an odds ratio of `r round(exp(coef(fit_int_emo)[3]), 2)` in favor of anger and an odds ratio of `r round(exp(coef(fit_int_emo)[2]), 2)` for a unit increase in intensity.

We can also compute again the marginal effects for `intensity` for each `emotion`:

```{r}
avg_slopes(fit_int_emo, variables = "intensity10", by = "emotion_lbl")
```

This means that we have on average an 8% and 9% of increase in accuracy.

## Interaction model, `intensity` and `emotion`

The final model that we can try is including the interaction between `intensity` and `emotion`.

```{r}
fit_emo_x_int <- glm(acc ~ intensity10 * emotion_lbl, 
                     data = dat, family = binomial(link = "logit"), 
                     subset = emotion_lbl %in% c("anger", "surprise"))

summary(fit_emo_x_int)
```

## Interaction model, `intensity` and `emotion`

With interactions, always visualize first:

```{r}
plot(ggeffect(fit_emo_x_int, terms = c("intensity10", "emotion_lbl")))
```

## Interaction model, `intensity` and `emotion`

Clearly the effect of intensity is not the same for `anger` and `surprise`. The participant reaches high accuracies faster for `surprised` faces compared to `angry` faces.

- `intercept`: is the expected logit for `anger` and `intensity` 0 (10%). Can be considered as the accuracy for the hardest angry face.
- `intensity10`: is the increase in logit accuracy for a unit increase (10%) in intensity for `angry` faces (the red slope in the previous plot)
- `emotion_lblsurprise`: is the logit difference (log odds ratio) between `anger` and `surprise` when `intensity` is 0 (10%). Is a conditional log odds ratio for a fixed value of intensity.
- `intensity10:emotion_lblsurprise`: this is the actual interaction. Is the logit difference of the two slopes (in logit). Is the red slope vs the blue slope.

## Interaction model, `intensity` and `emotion`

Let's improve a little bit the interpretation. We can *center* the emotion applying not the dummy (or treatment) coding but the sum to zero coding.

```{r}
datsub <- filter(dat, emotion_lbl %in% c("anger", "surprise"))
datsub$emotion_lbl <- factor(datsub$emotion_lbl)
contrasts(datsub$emotion_lbl) <- -contr.sum(2)/2
contrasts(datsub$emotion_lbl)
```

## Interaction model, `intensity` and `emotion`

```{r}
fit_emo_x_int2 <- glm(acc ~ intensity10 * emotion_lbl, 
                      data = datsub, 
                      family = binomial(link = "logit"))

summary(fit_emo_x_int2)
```

## Interaction model, `intensity` and `emotion`

The parameters are interpreted in the same way, but now we have a different meaning of 0:

- The intercept is the average logit accuracy when intensity is 10%
- `intensity10`: is the slope when `emotion_lbl` is 0 but 0 now is in the middle of `anger` and `surprise`. This means that `intensity10` is the main effect of intensity controlling for emotion.
- The interaction is the same as before as well as the other `emotion_lbl1` parameter.

## Interaction model, `intensity` and `emotion`

```{r}
names(fit_emo_x_int$coefficients) <- names(fit_emo_x_int2$coefficients) # just for a better output, dangerous otherwise
car::compareCoefs(fit_emo_x_int, fit_emo_x_int2)
```

# Diagnostics

## Deviance

When using the `summary()` function we can see that there is as section about *Deviance*:

```{r}
#| eval: false
summary(fit_int)
```

```{r}
#| echo: false
#| output: asis

summary(fit_int) |> 
  style_output(15:17, class = "hg")
```

This information can be used to assess the goodness of fit of the model and also to compute pseudo-$R^2$ values (see later).

## Deviance

We need to define three types of models:

- **Null Model**: a model without predictors (only the intercept)
- **Actual Model**: the model we fitted with predictors of interest
- **Saturated Model**: a model fitting the data perfectly (no error)

## Deviance and likelihood

This is a visual representation of the three models. The current model should be always between the null and the saturated.

```{r}
#| echo: false

d <- data.frame(
    x = seq(0, 1, 0.01)
)

d$y <- plogis(qlogis(0.2) + d$x*5)
d$p <- rbinom(nrow(d), 1, d$y)


d <- slice_sample(d, n = 30)
d$id <- 1:nrow(d)

d$Null <- mean(d$p)
d$Saturated <- d$p
d$Current <- d$y

pp <- d |> 
  pivot_longer(c(Null, Saturated, Current)) |>
  mutate(name = factor(name, levels = c("Null", "Current", "Saturated"))) |> 
  ggplot(aes(x = x, y = value)) +
  facet_wrap(~name) +
  geom_line(lwd = 1, color = "dodgerblue") +
  geom_point(aes(x = x, y = p), size = 4)
pp
```

## Deviance and likelihood

We can simplify the idea of likelihood and deviance thinking about the distance between the fitted line and the points. As the distance decreases, the likelihood of the model increases.

```{r}
#| echo: false
pp + geom_segment(aes(x = x, xend = x, y = value, yend = p), lty = "dotted")
```

## Deviance

The null and residual deviance that we see in the model output can be calculated as:

$$
D_{\mbox{null}} = 2 -(log(\mathcal{L}_{\mbox{null}}) - log(\mathcal{L}_{\mbox{sat}}))
$$
$$
D_{\mbox{resid}} = 2 -(log(\mathcal{L}_{\mbox{current}}) - log(\mathcal{L}_{\mbox{sat}}))
$$

## Deviance

```{r}
#| warning: false

dat$id <- factor(1:nrow(dat))
fit_cur <- fit_int # current model
fit_sat <- glm(acc ~ 0 + id, data = dat, family = binomial(link = "logit"))
fit_null <- glm(acc ~ 1, data = dat, family = binomial(link = "logit"))
```

```{r}
# residual
2 * -(logLik(fit_cur) - logLik(fit_sat))

# null
2 * -(logLik(fit_null) - logLik(fit_sat))
```

## Deviance, LRT

![](img/lrt.svg)

## $R^2$

The $R^2$ cannot be computed as in standard linear regression. There are different types of pseudo-$R^2$ for example:

- Likelihood ratio $R^2_L$
- Cox and Snell $R^2_{CS}$
- Nagelkerke $R^2_N$
- McFadden $R^2_{McF}$
- Tjur $R^2_T$

All these methods are based on the deviance and/or the likelihood of current/null/saturated models.

::: aside
See [https://en.wikipedia.org/wiki/Pseudo-R-squared](https://en.wikipedia.org/wiki/Pseudo-R-squared) for a nice overview.
:::

## $R^2$

The `performance` R package (used to plot diagnostics and other modelling-related metrics) implements most of the pseudo-$R^2$ values. The default for binomial models is the method proposed by @Tjur2009-ml.

```{r}
#| collapse: true

# performance::r2_tjur()
performance::r2(fit_emo_x_int2) 
performance::r2_coxsnell(fit_emo_x_int2)
performance::r2_mcfadden(fit_emo_x_int2)
performance::r2_nagelkerke(fit_emo_x_int2)
```

## Residuals

Diagnostics in GLMs is more complex than in standard linear models. The main reason is that residuals are more complex due to the link function. For example these are the residuals of the last model we fitted.

```{r}
#| echo: false

rr <- data.frame(
  residuals = residuals(fit_emo_x_int, type = "response"),
  fitted = fitted(fit_emo_x_int)
)

ggplot(rr, aes(x = fitted, y = residuals)) +
  geom_point() +
  xlab("Fitted values (probabilities)") +
  ylab("(raw) Residuals")
```

## Residuals

There are few problems with residuals in GLM:

- **Mean and variance are linked**. This means that as the mean increase also the variance increase violating the standard homoschedasticity assumption. This mainly happens with standard *raw* residuals and in GLM we need to use other residuals (e.g., Pearson, Deviance, etc.)
- **Residuals (even the Pearson or Deviance) are problematic for *discrete* GLM** (such as Binomial or Poisson), see the plot in the previous slide.
- **Residuals for non-normal distributions are not expected to be normally distributed** even when the model is well specified.
- **There are no standard and universal way** to assess the residuals pattern.

## Residuals, a proposal

> The `DHARMa` package uses a simulation-based approach to create readily interpretable scaled (quantile) residuals for fitted generalized linear (mixed) models

These residuals seems quite promising as an unified framework but I haven't systematically explored this possibility.

The `DHARMa` package has a nice [documentation](https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html) explaining the idea and how to use the quantile residuals [see also @Dunn1996-yd; @Dunn2018-ww].

## Residuals, a proposal

> The resulting residuals are standardized to values between 0 and 1 and can be interpreted as intuitively as residuals from a linear regression.

```{r}
#| output-location: slide 
#| out-width: 100%
#| out-heigth: 100%

library(DHARMa)
plot(simulateResiduals(fit_emo_x_int2))
```

# Monte Carlo simulations (MCS)

## Why simulating data?

If you plan to use a GLM and you want to compute some inferential properties (statistical power, type-1 error, etc.) there are usually no analytical (i.e., formula-based) methods to solve the problem.

The only way to do a power analysis with a logistic regression is to simulate data and re-fit the model multiple times to see the long-run behaviour.

MCS are also useful to understand more deeply a certain statistical model or procedure.

## General MCS workflow

1. Define the Data Generation Process (DGP)
2. Define the sample size, number of trials, conditions, etc.
3. Simulate data using random number generations and the DGP
4. Fit the target model
5. Repeat 3-4 a large number of times maybe with different features (e.g., different sample size) defined in 2
6. Summarise the simulation results. For example, counting the number of times the p value is significant (i.e., estimating the statistical power)

::: aside
We have also a longer [document](https://filippogambarota.github.io/notes/mcs-workflow/) about implementing a MCS in R.
:::

## 1. Data Generation Process (DGP)

Let's try estimating the statistical power for the `intensity` effect in our example.

We will simulate data according to a Binomial GLM with a logit link function.

```{r}
#| eval: false

binomial(link = "logit")
qlogis() # link function
plogis() # inverse link function
```

## 2. Experiment features

We will simulate a single subject doing $n$ trials. The main predictor is `intensity` ranging from 10% to 100% in steps of 10%.

```{r}
(intensity <- seq(10, 100, 10))
```

## 3. Random number generation

In R, to simulate data you can use the `r*` function. Each implemented distribution in R has the associated `r*` function (as the `p*` and `q*` function we used before).

```{r}
# simulate 10 numbers from a gaussian distribution
# with mu = 10 and sigma = 5
rnorm(n = 10, mean = 10, sd = 5)
```

## 3. Random number generation

For the Binomial we have `rbinom`:

```{r}
# n = number of rows in our case
# size = number of trials
# p = probability of success in each trial
rbinom(n = 1, size = 1, prob = 0.5)
rbinom(n = 5, size = 1, prob = 0.5)
rbinom(n = 5, size = 10, prob = 0.5)
```

## 3. Random number generation

Notice that `rbinom` is not really intutive because it works both on the binary and the binomial form.

This means to generate $k$ bernoulli trials with results 0 or 1
```{r}
rbinom(10, 1, 0.5)
```

This is the same but aggregating:

```{r}
# the result is the number of successes over 10 
# bernoulli trials
rbinom(1, 10, 0.5)
```

Depending if you want to work with 0-1 values or number of successes/failures (aggregated) you should use one of the two strategies.

## 3. Random number generation

The crucial part of `rbinom` is that the `prob =` argument is vectorized. This means that if you simulate $n$ trials you can provide $n$ probabilities.

```{r}
# 20 different probabilities
(p <- seq(0.1, 0.9, length.out = 20))
```

This means that for the last trials, the probability of success (1) is larger.

```{r}
rbinom(n = 20, size = 1, prob = p)
```

## Dataset

We can start with the deterministic part of the simulation, the experiment.

We simulate an experiment with `nt` trials with random `intensity` values from 10 to 100. We could also create a balanced version.

```{r}
nt <- 100 # number of trials

dat <- data.frame(
  intensity = sample(intensity, nt, replace = TRUE)
)

head(dat) # first 6 rows
```

## Systematic component and random component

The model can be formalized as:

$$
p_i = g^{-1}(\beta_0 + \beta_1 \times \mbox{intensity}_i)
$$

$$
y_i \sim \mathrm{Bernoulli}(p_i) \qquad y_i \in \{0,1\}
$$

Where $g^{-1}$ is the inverse logit function (`qlogis`). This can be reas as, for each trial $i$ the true probability of success is a function of the `intensity`-$i$. The 0-1 outcome comes from a Bernoulli distribution with probability of success $p_i$. This means that according to $\beta_1$ different intensity values will have a different probability of success.
 
Finally $\beta_0 + \beta_1 \times \mbox{intensity}_i$ (i.e., the linear predictor $\eta_i$) need to be simulated in logit scale, then converted back into probabilities.

## Systematic component and random component

$\beta_0$ is the (logit) probability of success for intensity 10%, centering the `intensity` on the minimum. $\beta_1$ is the increase in logit for a unit increase in intensity. Let's rescale intensity to be between 0 (10%) and 10 (100%).

```{r}
#| output-location: slide
#| code-fold: true
#| out-width: 100%
#| out-heigth: 100%

b0 <- qlogis(0.1) # logit of success when intensity = 10%
b1 <- 1
intensity10 <- (intensity - 10) / 10
eta <- b0 + b1 * intensity10

data.frame(
  intensity,
  eta
) |> 
  ggplot(aes(x = intensity, y = plogis(eta))) +
  geom_point(size = 5) +
  geom_line() +
  xlab("Intensity") +
  ylab("Accuracy") +
  ggtitle(latex2exp::TeX("$\\beta_1 = 1$"))
```

## Systematic component and random component

You can choose different $\beta_1$ values according to your hypothesis.

```{r}
#| echo: false

b1v <- c(0.5, 1, 2)
etal <- lapply(b1v, function(b1) b0 + b1 * intensity10)
dd <- data.frame(etal)
names(dd) <- b1v
dd$intensity <- intensity
pivot_longer(dd, 1:3, names_to = "b1", values_to = "eta") |> 
  mutate(b1 = latex2exp::TeX(sprintf("$\\beta_{\\;1} = %s$", b1), output = "character")) |> 
  ggplot(aes(x = intensity, y = plogis(eta))) +
  geom_point() +
  geom_line() +
  facet_wrap(~b1, nrow = 1, labeller = label_parsed) +
  xlab("Intensity") +
  ylab("Accuracy")
```

## Systematic component and random component

Now we can simply apply the same functions to the full dataset. Let's stick with $\beta_1 = 0.5$ for the moment.

```{r}
b0 <- qlogis(0.1)
b1 <- 1
dat$intensity10 <- with(dat, (intensity - 10)/10)
dat$eta <- with(dat, b0 + b1 * intensity10) # link function space
dat$p <- plogis(dat$eta) # inverse link (probability)
head(dat)
```

## Simulate the 0-1 response

Finally we need to simulate the 0-1 response for each trial/observation:

```{r}
dat$acc <- rbinom(nrow(dat), 1, dat$p)
head(dat)
```

For each row/trial, `p` is the true accuracy according to `intensity` and `acc` is the actual simulated response.

## Simulate the 0-1 response

Always plot your simulated data to check the results:

```{r}
#| echo: false

ggplot(dat, aes(x = intensity, y = acc)) +
  geom_point(position = position_jitter(height = 0.03), alpha = 0.5) +
  xlab("Intensity") +
  ylab("Accuracy")
```

## 4. Fit the target model

Now we can fit the desired model:

```{r}
fit <- glm(acc ~ intensity10, data = dat, family = binomial(link = "logit"))
summary(fit)
```

## 5. Repeat the process

Now the core of MCS. By repeating the data generation and model fitting we are simulating the randomness that could happen in real data collection. Better wrapping everything into a function:

```{r}
sim_data <- function(nt, b0, b1){
  dat <- data.frame(
    intensity = sample(seq(10, 100, 10), nt, replace = TRUE)
  )
  dat$intensity10 <- with(dat, (intensity - 10)/10)
  dat$eta <- with(dat, b0 + b1 * intensity10)
  dat$p <- plogis(dat$eta)
  dat$acc <- rbinom(nrow(dat), 1, dat$p)
  return(dat)
}

fit_model <- function(data){
  glm(acc ~ intensity10, data = data, family = binomial(link = "logit"))
}

summary_model <- function(fit){
  # keep only intensity coefficients
  data.frame(summary(fit)$coefficients)[2, ]
}
```

## 5. Repeat the process

And a overall simulation function:

```{r}
do_sim <- function(nsim, nt, b0, b1){
  res <- replicate(nsim, {
    dat <- sim_data(nt, b0, b1)
    fit <- fit_model(dat)
    summary_model(fit)
  }, simplify = FALSE)
  res <- do.call(rbind, res)
  rownames(res) <- NULL
  names(res) <- c("b", "se", "z", "p")
  return(res)
}

do_sim(5, 100, b0, b1)
```

## 5. Repeat the process

`nsim` is the number of simulations. Usually larger is better considering computational constraints. If feasible, 5000 or 10000 is usually more than enough. For complex models where 5000 or 10000 is not an option, try never go below 1000. See @Burton2006-hl and @Koehler2009-do for discussion about the number of simulations.

```{r}
tictoc::tic()
sim <- do_sim(1000, nt = 50, b0 = qlogis(0.1), b1 = 0.5)
tictoc::toc()
```

In this case we are lucky, running 1000 simulations only takes ~ 1.5 seconds. Sometimes can also takes hours, days or weeks.

## 6. Results

Each row is the parameter estimated from a simulated dataset. The statistical power is the % of p values lower than $\alpha$ of the total number of simulations:

```{r}
head(sim)

sum(sim$p <= 0.05) / 1000
# or mean(sum(sim$p <= 0.05))
```

The power is pretty high with 50 trials. Let's try a lower effect with different number of trials.

## More conditions

We can try different number of trials. For each `n` we simulate 1000 datasets, fit the models, extract the p values and estimate the statistical power.

```{r}
#| cache: true
#| warning: false

n <- c(10, 50, 80, 100, 150, 200)
power <- rep(NA, length(n))

for(i in 1:length(power)){
  res <- do_sim(1000, n[i], b0 = qlogis(0.1), b1 = 0.2)
  power[i] <- mean(res$p <= 0.05)
}

power
```

## More conditions

```{r}
#| echo: false

data.frame(power, n) |> 
  ggplot(aes(x = n, y = power)) +
  geom_point(size = 8) +
  geom_line() +
  xlab("Number of trials") +
  ylab("Power") +
  ggtitle("b1 = 0.2, exp(b1) = 1.22")
```

# Latent formulation of the binomial model

## Latent formulation of the binomial model

To clarify a little bit the terminology the *logit* function (link function $g(\cdot)$) is:

$$
q = \log{\frac{p}{1 - p}}
$$

The inverse of the logit is called *logistic* (inverse link function $g^{-1}(\cdot)$) function:

$$
p = \frac{e^p}{1 + e^p}
$$

## Latent formulation of the binomial model

```{r}
#| echo: false

library(ggplot2)
ggplot() +
    stat_function(fun = qlogis, n = 1e3, lwd = 1) +
    ylim(c(-5, 5)) +
    xlab("Probability") +
    ylab("Logit") +
    geom_vline(xintercept = 0.5, lty = "dotted") +
    geom_hline(yintercept = 0, lty = "dotted")
```

## Latent formulation of the binomial model

- Instead of thinking about a binary variable, you can think about e.g. accuracy as a continous measure from $-\infty$ to $+\infty$. 
- Then you can imagine to cut this continous measure using a threshold. Everthing above the threshold takes the value 1 otherwise 0.
- Using the *logit* function we are assuming that this underlying *latent* variable is a standard **logistic** distribution.
- The standard logistic distribution is a continous variable (similar to the Gaussian) with mean $\mu = 0$ and $\sigma^2 = \frac{s^2 \pi^2}{3}$. Given that $s^2 = 1$ in the standard logistic distribution the variance is $\frac{\pi^2}{3}$.

::: aside
[https://en.wikipedia.org/wiki/Logistic_distribution](https://en.wikipedia.org/wiki/Logistic_distribution)
:::

## Latent formulation of the binomial model

You can draw the logistic distribution using `dlogis()`. The comulative distribution function `plogis()` is the *logistic* function and the quantile function `qlogis()` is the *logit* function.

## Latent formulation of the binomial model

In this framework, saying that the logit in one condition is 0 means that the probability is 0.5 thus 50% of the observations are expected to have a value of 1 and 50% a value of 0.

A shift in the latent distribution represents also a shift in the proportions of 1 and 0. Regression coefficients (in logit) represent the shift in the latent distribution and odds ratios are the shift in the odds/proportions.

## Latent formulation of the binomial model

```{r}
#| echo: false
#| fig-height: 8
#| fig-width: 10

library(distributional)
library(ggdist)
library(tidyverse)

data.frame(
    id = c("a", "b"), 
    location = c(0, 2),
    scale = c(1, 1)
) |> mutate(
    dist = dist_logistic(location, scale)
) |> 
    ggplot(aes(x = factor(id), dist = dist)) +
    stat_halfeye(aes(fill = after_stat(y < 0)),
                 point_interval = NULL) +
    geom_hline(yintercept = 0, lty = "dotted") +
    geom_line(group = 0,
              aes(x = id, y = location)) +
    geom_point(aes(x = id, y = location)) +
    theme(legend.position = "none") +
    annotate("label",
             x = 1,
             y = 1.5, 
             label = "P(Y = 1|x) = 0.5") +
    annotate("label",
             x = 1,
             y = -1.5, 
             label = "P(Y = 1|x) = 0.5") +
    annotate("label",
             x = 2,
             y = 4, 
             label = "P(Y = 1|x) = 0.9") +
    annotate("label",
             x = 2,
             y = -1.5, 
             label = "P(Y = 1|x) = 0.1") +
    xlab("Condition") +
    ylab("Logit") +
    ggtitle("logit(b) - logit(a) = 2\nOR = exp(2) ~ 7.38")
```


## Latent formulation of the binomial model

In other terms, the logistic regression can be expressed as a linear model predicting shifts in the mean of the logistic distribution as a functions of predictors as in standard linear models assuming normality.

The parameters $\beta$ are shifts in the underlying latent distribution. $\beta = 1$ is like saying that the difference between the groups is 1 standard deviation on the latent scale.

## Latent formulation of the binomial model

Practically this means that we can write the same model in the latent form:

$$
z_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$

$$
\epsilon_i \sim \mbox{Logistic}(0, 1)
$$

Where $z_i$ is the latent value (e.g., logit). Errors are distributed according to a standard logistic distribution.

Then the observed 0-1 values:

$$
\begin{cases}
1 & \text{if } z_i > 0, \\
0 & \text{if } z_i \le 0 .
\end{cases}
$$

## Latent formulation of the binomial model

In R we can show this very easily. We can simulate a latent shift and the corresponding pattern in probabilities using the standard and latent form of the logistic regression.

```{r}
#| echo: true

p1 <- 0.5 # probability of group 1
p2 <- 0.8 # probability of group 2
(d <- qlogis(p2) - qlogis(p1)) # latent shift, log odds ratio
(OR <- exp(d)) # odds ratio
```

## Latent formulation of the binomial model

```{r}
#| echo: true

# number of observations, large to show the pattern
n <- 10000

# using rbinom
x <- rep(0:1, each = n/2) # condition 1 and condition 2
p <- plogis(qlogis(p1) + d * x)
yb <- rbinom(n, 1, p)

# latent form
z <- qlogis(p1) + x * d + rlogis(n, 0, 1)
yl <- ifelse(z > 0, 1, 0)

tapply(yb, x, mean)
tapply(yl, x, mean)
```

## Latent formulation of the binomial model

The beauty of the latent intepretation is that if you change the error part from a logistic to a gaussian distribution, you obtain the **probit** model.

In the probit the idea is the same but shifts are in units of a standard normal distribution that can be intepreted as Cohen's $d$.

# Probit link function

## Probit link function

Sometimes, the *probit* link function can be used. The main difference is that when using the *logit* link function the underlying distribution is called logistic. When using the *probit* link function the underlying distribution is Gaussian.

```{r, echo=FALSE}
ggplot() +
    stat_function(aes(color = "Probit"),
                      fun = pnorm) +
    stat_function(aes(color = "Logit"),
                  fun = plogis) +
    xlim(-4, 4) +
    theme(legend.title = element_blank(),
          legend.position = c(0.9,0.2),
          axis.title.x = element_blank()) +
    ylab("Probability")
```

## Probit link function

When using the **probit link** the parameters are interpreted as difference in *z-scores* associated with a unit increase in the predictors. In fact probabilities are mapped into *z-scores* using the cumulative normal distribution.

```{r}
p1 <- 0.8
p2 <- 0.6

qlogis(c(p1, p2)) # log(odds(p1)), logit link
qnorm(c(p1, p2)) # probit link

log(or(p1, p2)) # ~ beta1, logit link
qnorm(p1) - qnorm(p2) # ~beta1, probit link
```

## Probit link function

```{r, echo = FALSE}
probit_cum <- ggplot() +
    stat_function(fun = pnorm) +
    xlim(-4, 4) +
    theme(legend.title = element_blank(),
          legend.position = c(0.9,0.2)) +
    ylab(latex2exp::TeX("$p = \\Phi^{-1}(z)$")) +
    xlab("z") +
    geom_segment(aes(x = qnorm(c(p1,p2)),
                     y = c(0, 0),
                     xend = qnorm(c(p1,p2)),
                     yend = c(p1, p2))) +
    geom_segment(aes(x = qnorm(c(p1,p2)),
                     y = c(p1,p2),
                     xend = c(-4, -4),
                     yend = c(p1,p2))) +
    geom_point(aes(x = qnorm(c(p1,p2)),
                   y = c(p1, p2)),
               color = c("red", "blue"),
               size = 5) +
    geom_point(aes(x = qnorm(c(p1,p2)),
                   y = c(0, 0)),
               color = c("red", "blue"),
               size = 5) +
    geom_point(aes(x = c(-4, -4)),
               y = c(p1, p2),
               color = c("red", "blue"),
               size = 5) +
    theme(aspect.ratio = 1)

probit_denp1 <- ggplot() +
    stat_function(fun = dnorm) +
    xlim(-4, 4) +
    ylab("Density") +
    geom_point(aes(x = qnorm(p1),
                   y = c(0)),
               color = c("red"),
               size = 5) +
    geom_point(aes(x = qnorm(p1),
                   y = dnorm(qnorm(p1))),
               color = c("red"),
               size = 5) +
    geom_area(aes(x = c(-4, 4)),
              stat = "function", 
              fun = dnorm,
              fill = "red",
              xlim = c(-4, qnorm(p1)),
              alpha = 0.5) +
    xlab(latex2exp::TeX("$\\Phi(p)$"))

probit_denp2 <- ggplot() +
    stat_function(fun = dnorm) +
    xlim(-4, 4) +
    ylab("Density") +
    geom_point(aes(x = qnorm(p2),
                   y = c(0)),
               color = c("blue"),
               size = 5) +
    geom_point(aes(x = qnorm(p2),
                   y = dnorm(qnorm(p2))),
               color = c("blue"),
               size = 5) +
    geom_area(aes(x = c(-4, 4)),
              stat = "function", 
              fun = dnorm,
              fill = "blue",
              xlim = c(-4, qnorm(p2)),
              alpha = 0.5) +
    xlab(latex2exp::TeX("$\\Phi(p)$"))

right_plot <- cowplot::plot_grid(
    probit_denp1 + theme_minimal(base_size = 15), 
    probit_denp2 + theme_minimal(base_size = 15),
    nrow = 2
)

left_plot <- probit_cum

cowplot::plot_grid(
    left_plot + theme_minimal(base_size = 15), 
    right_plot,
    rel_widths = c(2,1)
)
```

## Why using the probit link function?

The probit can be a really useful choice:

- A binomial GLM with a probit link can be used to estimate Signal Detection Theory ($d$' and criterion) parameters [see @DeCarlo1998-ay; @DeCarlo2010-lj]. 
- The parameters are intepreted as differences in standard deviations similar to Cohen's $d$. This is also very useful in simulations.

# Odds Ratio (OR) to Cohen's $d$

## Odds Ratio (OR) to Cohen's $d$

The Odds Ratio can be considered an effect size measure. We can transform the OR into other effect size measure [@Sanchez-Meca2003-ji; @Borenstein2009-mo].

$$
d = \log(OR) \frac{\sqrt{3}}{\pi}
$$

```{r}
# in R with the effectsize package
or <- 1.5
effectsize::logoddsratio_to_d(log(or))
# or 
effectsize::oddsratio_to_d(or)
```

## Odds Ratio (OR) to Cohen's $d$

```{r}
#| echo: false

pl <- ggplot() +
  stat_function(fun = effectsize::d_to_oddsratio, n = 1e3) +
  xlim(c(-1, 1)) +
  xlab("Cohen's d") +
  ylab("Odds Ratio") +
  geom_vline(xintercept = 0, lty = "dotted") +
  geom_hline(yintercept = 1, lty = "dotted")

pr <- ggplot() +
  stat_function(fun = effectsize::d_to_logoddsratio, n = 1e3) +
  xlim(c(-1, 1)) +
  xlab("Cohen's d") +
  ylab("Log(Odds Ratio)") +
  geom_vline(xintercept = 0, lty = "dotted") +
  geom_hline(yintercept = 0, lty = "dotted")

pl + pr
```

# Binary vs binomial data structure

## Binary vs binomial data structure

In our facial expression example we used the binary data structure. This means that the vector of the response variable contains 0 and 1.

```{r}
#| include: false

dat <- dat_shimizu
```

```{r}
head(dat)
```

## Binary vs binomial data structure

The same dataset can be also used in the so-called binomial format. In this case we can aggregate `emotion_lbl` and `intensity` counting the number of correct responses.

```{r}
dat_binomial <- dat |> 
  group_by(intensity) |> 
  summarise(nt = n(),      # total number of trials
            nc = sum(acc), # number of correct responses
            nf = nt - nc,  # number of fails
            acc = nc / nt) # proportion of correct response
```

## Binary vs binomial data structure

```{r}
head(dat_binomial)
```

## Binary vs binomial data structure

The dataset contains **exactly** the same information, we are not losing anything by aggregating. We can check it by fitting two models. Notice the way of writing the model in the binomial way:

```{r}
fit_binary   <- glm(acc ~ intensity, data = dat, family = binomial(link = "logit"))
fit_binomial <- glm(cbind(nc, nf) ~ intensity, data = dat_binomial, family = binomial(link = "logit"))
```

## Binary vs binomial data structure

```{r}
car::compareCoefs(fit_binary, fit_binomial)
```

## Binary vs binomial data structure

A third option is also modelling directly the proportions providing the `weigths = ` argument as the number of trials.

```{r}
fit_prop <- glm(acc ~ intensity, data = dat_binomial, weights = nt, family = binomial(link = "logit")) 
car::compareCoefs(fit_binary, fit_binomial, fit_prop)
```

Again, exactly the same.

## Binary vs binomial data structure

Why using the binary vs the binomial format? Depends on the type of predictors! 

In our (and similar) case, "aggregating" with categorical predictor is possible without losing information. The advantage is that models in the binomial format are very fast (especially for complex models such as mixed-effects models). Also the diagnostics in terms of residuals are better [see @Gelman2020-tg, p. 253]

If you have predictors at the 0-1 level you cannot aggregate without losing information. In our case, imagine to have for each trial the 0-1 accuracy and the reaction time. You need to use the binary format otherwise you have to bin the reaction time variable (losing information).

## References